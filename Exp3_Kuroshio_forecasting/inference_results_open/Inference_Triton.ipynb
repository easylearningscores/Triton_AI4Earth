{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105d1dd-49b8-4e91-9b41-23406f05992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "from model.Triton_model import Triton\n",
    "\n",
    "\n",
    "# ============================== Initialization Configuration ==============================\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "def load_single_model(model_path):\n",
    "    \"\"\"Load the best inference model\"\"\"\n",
    "    model = Triton(\n",
    "        shape_in=(10, 2, 128, 128),\n",
    "        spatial_hidden_dim=256,\n",
    "        output_channels=2,\n",
    "        temporal_hidden_dim=512,\n",
    "        num_spatial_layers=4,\n",
    "        num_temporal_layers=8).to(device)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        if any(k.startswith('module.') for k in checkpoint.keys()):\n",
    "            checkpoint = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n",
    "        model.load_state_dict(checkpoint)\n",
    "        logging.info(f\"Model loaded successfully.: {os.path.basename(model_path)}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found.: {model_path}\")\n",
    "    return model\n",
    "\n",
    "# ============================== Dataloader ==============================\n",
    "class OceanDataLoader:\n",
    "    def __init__(self, nc_path):\n",
    "        self.ds = xr.open_dataset(nc_path)\n",
    "        self.time_stamps = self.ds.time.values.astype('datetime64[s]').astype(datetime)\n",
    "    \n",
    "    def generate_target_dates(self, start_date, end_date, interval_days=1):\n",
    "        \"\"\"Generate a continuous initial date sequence.\"\"\"\n",
    "        all_dates = []\n",
    "        current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        while current_date <= end_date:\n",
    "            if current_date in self.time_stamps:\n",
    "                all_dates.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "            current_date += timedelta(days=interval_days)\n",
    "        \n",
    "        if not all_dates:\n",
    "            raise ValueError(\"No valid dates found. Please check the date range and data files.\")\n",
    "        return all_dates\n",
    "    \n",
    "    def load_single_case(self, target_date, pred_days):\n",
    "        \"\"\"\n",
    "        Load a single initial condition \n",
    "        param target_date: The last date of the input sequence (format: 'YYYY-MM-%d') \n",
    "        param pred_days: The number of days to predict \n",
    "        return: (initial data, true labels, initial timestamp, label timestamp) \n",
    "        \"\"\"\n",
    "        try:\n",
    "            target_dt = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "            end_idx = np.where(self.time_stamps == target_dt)[0][0]\n",
    "        except IndexError:\n",
    "            available_dates = [d.strftime(\"%Y-%m-%d\") for d in self.time_stamps[-10:]]\n",
    "            raise ValueError(f\"Invalid date {target_date}, the last 10 available dates: {available_dates}\")\n",
    "\n",
    "        if end_idx < 9:\n",
    "            raise ValueError(f\"At least 9 days of data are required, the earliest available date: {self.time_stamps[0].strftime('%Y-%m-%d')}\")\n",
    "        if end_idx + pred_days >= len(self.time_stamps):\n",
    "            raise ValueError(f\"Prediction exceeds data range, the data cutoff date is: {self.time_stamps[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        initial_dates = self.time_stamps[end_idx-9 : end_idx+1]  \n",
    "        label_dates = self.time_stamps[end_idx+1 : end_idx+1+pred_days]\n",
    "\n",
    "        def load_var(var_name, start, end):\n",
    "            data = self.ds[var_name].isel(time=slice(start, end)).values\n",
    "            return torch.FloatTensor(np.nan_to_num(data, nan=0.0))\n",
    "\n",
    "        ugos_init = load_var('ugos', end_idx-9, end_idx+1)\n",
    "        vgos_init = load_var('vgos', end_idx-9, end_idx+1)\n",
    "        initial = torch.stack([ugos_init, vgos_init], dim=1).unsqueeze(0).to(device)\n",
    "\n",
    "        ugos_label = load_var('ugos', end_idx+1, end_idx+1+pred_days)\n",
    "        vgos_label = load_var('vgos', end_idx+1, end_idx+1+pred_days)\n",
    "        label = torch.stack([ugos_label, vgos_label], dim=1).unsqueeze(0).to(device)\n",
    "\n",
    "        return initial, label, initial_dates, label_dates\n",
    "\n",
    "# ============================== Inference engine ==============================\n",
    "def predict_single(model, initial_input, pred_days):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    current_input = initial_input.clone()\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        total_steps = (pred_days + 9) // 10\n",
    "        for _ in tqdm(range(total_steps), desc=f\"Prediction progress.\", leave=False):\n",
    "            output = model(current_input)\n",
    "            predictions.append(output.cpu())\n",
    "            current_input = output[:, -10:]\n",
    "    \n",
    "    return torch.cat(predictions, dim=1)[:, :pred_days].to(device)\n",
    "\n",
    "# ============================== Batch processing. ==============================\n",
    "def process_batch(model, data_loader, target_dates, pred_days, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    success_count = 0\n",
    "    \n",
    "    for date_str in tqdm(target_dates, desc=\"Process initial conditions.\"):\n",
    "        try:\n",
    "            initial, label, init_dates, label_dates = data_loader.load_single_case(date_str, pred_days)\n",
    "            initial = initial[..., ::2, ::2] \n",
    "            label = label[..., ::2, ::2]\n",
    "            \n",
    "            prediction = predict_single(model, initial, pred_days)\n",
    "            \n",
    "            save_path = os.path.join(save_dir, f\"forecast_{date_str.replace('-','')}.h5\")\n",
    "            save_results(\n",
    "                initial.cpu(), \n",
    "                label.cpu(), \n",
    "                prediction.cpu(),\n",
    "                init_dates, \n",
    "                label_dates,\n",
    "                save_path\n",
    "            )\n",
    "            success_count += 1\n",
    "            \n",
    "            del initial, label, prediction\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Process {date_str} failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logging.info(f\"Processing complete, successfully processed {success_count}/{len(target_dates)} initial conditions\")\n",
    "\n",
    "# ============================== Results saved ==============================\n",
    "def save_results(initial, label, prediction, init_dates, label_dates, save_path):\n",
    "    with h5py.File(save_path, 'w') as f:\n",
    "        f.create_dataset('initial', data=initial.numpy())\n",
    "        f.create_dataset('label', data=label.numpy())\n",
    "        f.create_dataset('prediction', data=prediction.numpy())\n",
    "        \n",
    "        def save_dates(dataset_name, dates):\n",
    "            str_dates = [d.strftime(\"%Y-%m-%d\") for d in dates]\n",
    "            dt = h5py.string_dtype(encoding='utf-8')\n",
    "            f.create_dataset(dataset_name, data=np.array(str_dates, dtype=dt))\n",
    "        \n",
    "        save_dates('initial_dates', init_dates)\n",
    "        save_dates('label_dates', label_dates)\n",
    "        \n",
    "        f.attrs['input_end_date'] = init_dates[-1].strftime(\"%Y-%m-%d\")\n",
    "        f.attrs['pred_start_date'] = label_dates[0].strftime(\"%Y-%m-%d\")\n",
    "        f.attrs['pred_end_date'] = label_dates[-1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def visualize_enhanced(h5_path, step=0, save_fig=True):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        initial = f['initial'][0]\n",
    "        label = f['label'][0]\n",
    "        prediction = f['prediction'][0]\n",
    "        init_dates = [d.decode() for d in f['initial_dates'][:]]\n",
    "        label_dates = [d.decode() for d in f['label_dates'][:]]\n",
    "    \n",
    "    input_end_date = init_dates[-1]\n",
    "    pred_date = label_dates[min(step, len(label_dates)-1)]\n",
    "    \n",
    "    def get_speed(data, step):\n",
    "        return np.sqrt(data[step,0]**2 + data[step,1]**2)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "    fig.suptitle(f\"Comparison of Ocean Surface Current Speed\\nInput End Date: {input_end_date} â†’ Prediction Date: {pred_date}\", \n",
    "                y=1.05, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plot_kwargs = {\n",
    "        'cmap': 'jet',\n",
    "        'extent': [123.1, 154.9, 10.06, 41.94],  \n",
    "        'origin': 'lower',\n",
    "        'vmin': 0,\n",
    "        'vmax': max(np.nanmax(label), np.nanmax(prediction))\n",
    "    }\n",
    "    \n",
    "    speed_initial = get_speed(initial, -1)\n",
    "    im0 = axes[0].imshow(speed_initial, **plot_kwargs)\n",
    "    axes[0].set_title(f\"Initial Field Last Day\\n{init_dates[-1]}\", fontsize=12)\n",
    "    axes[0].set_xlabel('Longitude', fontsize=10)\n",
    "    axes[0].set_ylabel('Latitude', fontsize=10)\n",
    "\n",
    "    \n",
    "    speed_label = get_speed(label, step)\n",
    "    im1 = axes[1].imshow(speed_label, **plot_kwargs)\n",
    "    axes[1].set_title(f\"True Values\\n{pred_date}\", fontsize=12)\n",
    "    axes[1].set_xlabel('Longitude', fontsize=10)\n",
    "\n",
    "    speed_pred = get_speed(prediction, step)\n",
    "    im2 = axes[2].imshow(speed_pred, **plot_kwargs)\n",
    "    axes[2].set_title(f\"Predicted Values\\n{pred_date}\", fontsize=12)\n",
    "    axes[2].set_xlabel('Longitude', fontsize=10)\n",
    "\n",
    "    cbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8, pad=0.03)\n",
    "    cbar.set_label('Current Speed (m/s)', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        fig_name = f\"forecast_{input_end_date}_day{step+1}.png\"\n",
    "        plt.savefig(fig_name, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============================== Main ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    backbone = 'Kuro_Triton_exp1_128_20250322'\n",
    "    config = {\n",
    "        'model_path': f'/jizhicfs/easyluwu/ocean_project/NPJ_baselines/Exp_2_Kuroshio/checkpoints/{backbone}_best_model.pth',\n",
    "        'data_path': '/jizhicfs/easyluwu/ocean_project/kuro/KURO.nc',\n",
    "        'date_range': {  \n",
    "            'start': '2021-01-01',\n",
    "            'end': '2021-12-31',\n",
    "            'interval': 5  \n",
    "        },\n",
    "        'pred_days': 120,\n",
    "        'save_dir':f'./{backbone}_forecast_results'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        model = load_single_model(config['model_path'])\n",
    "        data_loader = OceanDataLoader(config['data_path'])\n",
    "        \n",
    "        target_dates = data_loader.generate_target_dates(\n",
    "            start_date=config['date_range']['start'],\n",
    "            end_date=config['date_range']['end'],\n",
    "            interval_days=config['date_range']['interval']\n",
    "        )\n",
    "        logging.info(f\"Generated {len(target_dates)} initial dates, example: {target_dates[:5]}...\")\n",
    "        \n",
    "        process_batch(\n",
    "            model, \n",
    "            data_loader,\n",
    "            target_dates,\n",
    "            config['pred_days'],\n",
    "            config['save_dir']\n",
    "        )\n",
    "        \n",
    "        sample_dates = [target_dates[0], target_dates[-1]]\n",
    "        for date in sample_dates:\n",
    "            h5_file = os.path.join(config['save_dir'], f\"forecast_{date.replace('-','')}.h5\")\n",
    "            for step in [0, 60, 119]: \n",
    "                visualize_enhanced(h5_file, step=step)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Main process error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eaa58f-6e63-4876-a558-dbe5d085ed6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a4033-f4fa-495c-b095-f39099d18811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8346e2d-5eb7-4a04-bb6f-4a0831016f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

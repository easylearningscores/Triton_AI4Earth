2024-11-11 13:57:17,741 Epoch 1/2000
2024-11-11 13:57:33,467 Current Learning Rate: 0.0099993832
2024-11-11 13:57:34,075 Train Loss: 1.2661774, Val Loss: 0.0945691
2024-11-11 13:57:34,075 Epoch 2/2000
2024-11-11 13:57:48,763 Current Learning Rate: 0.0099975328
2024-11-11 13:57:49,623 Train Loss: 0.0537641, Val Loss: 0.0279424
2024-11-11 13:57:49,623 Epoch 3/2000
2024-11-11 13:58:04,524 Current Learning Rate: 0.0099944494
2024-11-11 13:58:05,625 Train Loss: 0.0225878, Val Loss: 0.0174877
2024-11-11 13:58:05,626 Epoch 4/2000
2024-11-11 13:58:20,987 Current Learning Rate: 0.0099901336
2024-11-11 13:58:21,766 Train Loss: 0.0182771, Val Loss: 0.0150993
2024-11-11 13:58:21,766 Epoch 5/2000
2024-11-11 13:58:37,352 Current Learning Rate: 0.0099845867
2024-11-11 13:58:38,354 Train Loss: 0.0156225, Val Loss: 0.0133836
2024-11-11 13:58:38,355 Epoch 6/2000
2024-11-11 13:58:54,652 Current Learning Rate: 0.0099778098
2024-11-11 13:58:55,447 Train Loss: 0.0145146, Val Loss: 0.0130899
2024-11-11 13:58:55,447 Epoch 7/2000
2024-11-11 13:59:10,803 Current Learning Rate: 0.0099698048
2024-11-11 13:59:11,606 Train Loss: 0.0142418, Val Loss: 0.0129172
2024-11-11 13:59:11,606 Epoch 8/2000
2024-11-11 13:59:26,594 Current Learning Rate: 0.0099605735
2024-11-11 13:59:27,429 Train Loss: 0.0140829, Val Loss: 0.0128121
2024-11-11 13:59:27,429 Epoch 9/2000
2024-11-11 13:59:42,661 Current Learning Rate: 0.0099501183
2024-11-11 13:59:43,689 Train Loss: 0.0139972, Val Loss: 0.0127396
2024-11-11 13:59:43,689 Epoch 10/2000
2024-11-11 13:59:59,735 Current Learning Rate: 0.0099384417
2024-11-11 14:00:00,530 Train Loss: 0.0139306, Val Loss: 0.0126877
2024-11-11 14:00:00,531 Epoch 11/2000
2024-11-11 14:00:15,580 Current Learning Rate: 0.0099255466
2024-11-11 14:00:16,603 Train Loss: 0.0138500, Val Loss: 0.0126377
2024-11-11 14:00:16,604 Epoch 12/2000
2024-11-11 14:00:32,449 Current Learning Rate: 0.0099114363
2024-11-11 14:00:33,472 Train Loss: 0.0137952, Val Loss: 0.0126092
2024-11-11 14:00:33,473 Epoch 13/2000
2024-11-11 14:00:49,904 Current Learning Rate: 0.0098961141
2024-11-11 14:00:50,822 Train Loss: 0.0137525, Val Loss: 0.0125482
2024-11-11 14:00:50,822 Epoch 14/2000
2024-11-11 14:01:06,299 Current Learning Rate: 0.0098795838
2024-11-11 14:01:07,340 Train Loss: 0.0136998, Val Loss: 0.0125104
2024-11-11 14:01:07,341 Epoch 15/2000
2024-11-11 14:01:23,530 Current Learning Rate: 0.0098618496
2024-11-11 14:01:24,335 Train Loss: 0.0136709, Val Loss: 0.0124729
2024-11-11 14:01:24,336 Epoch 16/2000
2024-11-11 14:01:39,180 Current Learning Rate: 0.0098429158
2024-11-11 14:01:39,977 Train Loss: 0.0136169, Val Loss: 0.0124519
2024-11-11 14:01:39,978 Epoch 17/2000
2024-11-11 14:01:55,280 Current Learning Rate: 0.0098227871
2024-11-11 14:01:56,264 Train Loss: 0.0135782, Val Loss: 0.0124041
2024-11-11 14:01:56,264 Epoch 18/2000
2024-11-11 14:02:12,422 Current Learning Rate: 0.0098014684
2024-11-11 14:02:13,438 Train Loss: 0.0135803, Val Loss: 0.0123766
2024-11-11 14:02:13,438 Epoch 19/2000
2024-11-11 14:02:29,142 Current Learning Rate: 0.0097789651
2024-11-11 14:02:30,239 Train Loss: 0.0135045, Val Loss: 0.0123399
2024-11-11 14:02:30,239 Epoch 20/2000
2024-11-11 14:02:45,969 Current Learning Rate: 0.0097552826
2024-11-11 14:02:46,963 Train Loss: 0.0134864, Val Loss: 0.0123273
2024-11-11 14:02:46,963 Epoch 21/2000
2024-11-11 14:03:03,191 Current Learning Rate: 0.0097304268
2024-11-11 14:03:04,677 Train Loss: 0.0135118, Val Loss: 0.0122758
2024-11-11 14:03:04,677 Epoch 22/2000
2024-11-11 14:03:20,274 Current Learning Rate: 0.0097044038
2024-11-11 14:03:21,242 Train Loss: 0.0133713, Val Loss: 0.0122122
2024-11-11 14:03:21,242 Epoch 23/2000
2024-11-11 14:03:36,192 Current Learning Rate: 0.0096772202
2024-11-11 14:03:36,969 Train Loss: 0.0133401, Val Loss: 0.0121755
2024-11-11 14:03:36,969 Epoch 24/2000
2024-11-11 14:03:52,247 Current Learning Rate: 0.0096488824
2024-11-11 14:03:53,264 Train Loss: 0.0132870, Val Loss: 0.0121372
2024-11-11 14:03:53,264 Epoch 25/2000
2024-11-11 14:04:08,343 Current Learning Rate: 0.0096193977
2024-11-11 14:04:09,135 Train Loss: 0.0132457, Val Loss: 0.0121040
2024-11-11 14:04:09,135 Epoch 26/2000
2024-11-11 14:04:23,673 Current Learning Rate: 0.0095887731
2024-11-11 14:04:24,427 Train Loss: 0.0131983, Val Loss: 0.0120748
2024-11-11 14:04:24,427 Epoch 27/2000
2024-11-11 14:04:39,002 Current Learning Rate: 0.0095570164
2024-11-11 14:04:39,773 Train Loss: 0.0131626, Val Loss: 0.0120470
2024-11-11 14:04:39,773 Epoch 28/2000
2024-11-11 14:04:54,297 Current Learning Rate: 0.0095241353
2024-11-11 14:04:55,000 Train Loss: 0.0131318, Val Loss: 0.0120103
2024-11-11 14:04:55,000 Epoch 29/2000
2024-11-11 14:05:09,700 Current Learning Rate: 0.0094901379
2024-11-11 14:05:10,421 Train Loss: 0.0130881, Val Loss: 0.0119711
2024-11-11 14:05:10,421 Epoch 30/2000
2024-11-11 14:05:25,041 Current Learning Rate: 0.0094550326
2024-11-11 14:05:25,922 Train Loss: 0.0130381, Val Loss: 0.0119285
2024-11-11 14:05:25,922 Epoch 31/2000
2024-11-11 14:05:41,262 Current Learning Rate: 0.0094188282
2024-11-11 14:05:42,297 Train Loss: 0.0129911, Val Loss: 0.0118835
2024-11-11 14:05:42,297 Epoch 32/2000
2024-11-11 14:05:57,406 Current Learning Rate: 0.0093815334
2024-11-11 14:05:58,205 Train Loss: 0.0129408, Val Loss: 0.0118445
2024-11-11 14:05:58,205 Epoch 33/2000
2024-11-11 14:06:12,583 Current Learning Rate: 0.0093431576
2024-11-11 14:06:13,444 Train Loss: 0.0128994, Val Loss: 0.0118045
2024-11-11 14:06:13,444 Epoch 34/2000
2024-11-11 14:06:27,706 Current Learning Rate: 0.0093037101
2024-11-11 14:06:28,491 Train Loss: 0.0128525, Val Loss: 0.0117625
2024-11-11 14:06:28,492 Epoch 35/2000
2024-11-11 14:06:43,050 Current Learning Rate: 0.0092632008
2024-11-11 14:06:43,806 Train Loss: 0.0128143, Val Loss: 0.0117353
2024-11-11 14:06:43,807 Epoch 36/2000
2024-11-11 14:06:58,363 Current Learning Rate: 0.0092216396
2024-11-11 14:06:59,142 Train Loss: 0.0127623, Val Loss: 0.0116933
2024-11-11 14:06:59,142 Epoch 37/2000
2024-11-11 14:07:14,647 Current Learning Rate: 0.0091790368
2024-11-11 14:07:15,449 Train Loss: 0.0127313, Val Loss: 0.0116767
2024-11-11 14:07:15,449 Epoch 38/2000
2024-11-11 14:07:31,353 Current Learning Rate: 0.0091354029
2024-11-11 14:07:32,129 Train Loss: 0.0126899, Val Loss: 0.0116313
2024-11-11 14:07:32,130 Epoch 39/2000
2024-11-11 14:07:47,116 Current Learning Rate: 0.0090907486
2024-11-11 14:07:47,900 Train Loss: 0.0126563, Val Loss: 0.0115961
2024-11-11 14:07:47,900 Epoch 40/2000
2024-11-11 14:08:04,085 Current Learning Rate: 0.0090450850
2024-11-11 14:08:04,878 Train Loss: 0.0126185, Val Loss: 0.0115656
2024-11-11 14:08:04,878 Epoch 41/2000
2024-11-11 14:08:19,508 Current Learning Rate: 0.0089984233
2024-11-11 14:08:20,279 Train Loss: 0.0125796, Val Loss: 0.0115548
2024-11-11 14:08:20,279 Epoch 42/2000
2024-11-11 14:08:35,722 Current Learning Rate: 0.0089507751
2024-11-11 14:08:36,552 Train Loss: 0.0125514, Val Loss: 0.0115522
2024-11-11 14:08:36,552 Epoch 43/2000
2024-11-11 14:08:52,783 Current Learning Rate: 0.0089021520
2024-11-11 14:08:53,590 Train Loss: 0.0125182, Val Loss: 0.0115146
2024-11-11 14:08:53,591 Epoch 44/2000
2024-11-11 14:09:09,053 Current Learning Rate: 0.0088525662
2024-11-11 14:09:09,853 Train Loss: 0.0124835, Val Loss: 0.0114822
2024-11-11 14:09:09,853 Epoch 45/2000
2024-11-11 14:09:24,865 Current Learning Rate: 0.0088020298
2024-11-11 14:09:25,694 Train Loss: 0.0124573, Val Loss: 0.0114429
2024-11-11 14:09:25,694 Epoch 46/2000
2024-11-11 14:09:39,949 Current Learning Rate: 0.0087505553
2024-11-11 14:09:40,802 Train Loss: 0.0124457, Val Loss: 0.0114221
2024-11-11 14:09:40,803 Epoch 47/2000
2024-11-11 14:09:55,217 Current Learning Rate: 0.0086981555
2024-11-11 14:09:56,011 Train Loss: 0.0124224, Val Loss: 0.0114076
2024-11-11 14:09:56,012 Epoch 48/2000
2024-11-11 14:10:10,537 Current Learning Rate: 0.0086448431
2024-11-11 14:10:11,397 Train Loss: 0.0124149, Val Loss: 0.0113851
2024-11-11 14:10:11,397 Epoch 49/2000
2024-11-11 14:10:26,503 Current Learning Rate: 0.0085906315
2024-11-11 14:10:26,504 Train Loss: 0.0124183, Val Loss: 0.0114720
2024-11-11 14:10:26,504 Epoch 50/2000
2024-11-11 14:10:41,894 Current Learning Rate: 0.0085355339
2024-11-11 14:10:41,895 Train Loss: 0.0123957, Val Loss: 0.0113915
2024-11-11 14:10:41,895 Epoch 51/2000
2024-11-11 14:10:57,760 Current Learning Rate: 0.0084795640
2024-11-11 14:10:58,530 Train Loss: 0.0123648, Val Loss: 0.0113490
2024-11-11 14:10:58,531 Epoch 52/2000
2024-11-11 14:11:13,324 Current Learning Rate: 0.0084227355
2024-11-11 14:11:13,325 Train Loss: 0.0123474, Val Loss: 0.0113556
2024-11-11 14:11:13,326 Epoch 53/2000
2024-11-11 14:11:28,539 Current Learning Rate: 0.0083650626
2024-11-11 14:11:29,242 Train Loss: 0.0123528, Val Loss: 0.0113295
2024-11-11 14:11:29,242 Epoch 54/2000
2024-11-11 14:11:43,858 Current Learning Rate: 0.0083065593
2024-11-11 14:11:43,859 Train Loss: 0.0123243, Val Loss: 0.0113394
2024-11-11 14:11:43,859 Epoch 55/2000
2024-11-11 14:11:59,784 Current Learning Rate: 0.0082472402
2024-11-11 14:12:00,507 Train Loss: 0.0122991, Val Loss: 0.0112948
2024-11-11 14:12:00,507 Epoch 56/2000
2024-11-11 14:12:15,124 Current Learning Rate: 0.0081871199
2024-11-11 14:12:15,125 Train Loss: 0.0122957, Val Loss: 0.0113359
2024-11-11 14:12:15,125 Epoch 57/2000
2024-11-11 14:12:30,952 Current Learning Rate: 0.0081262133
2024-11-11 14:12:31,721 Train Loss: 0.0122802, Val Loss: 0.0112878
2024-11-11 14:12:31,721 Epoch 58/2000
2024-11-11 14:12:46,994 Current Learning Rate: 0.0080645353
2024-11-11 14:12:46,995 Train Loss: 0.0122734, Val Loss: 0.0112881
2024-11-11 14:12:46,995 Epoch 59/2000
2024-11-11 14:13:02,764 Current Learning Rate: 0.0080021011
2024-11-11 14:13:02,764 Train Loss: 0.0122800, Val Loss: 0.0112881
2024-11-11 14:13:02,765 Epoch 60/2000
2024-11-11 14:13:18,621 Current Learning Rate: 0.0079389263
2024-11-11 14:13:19,638 Train Loss: 0.0122700, Val Loss: 0.0112763
2024-11-11 14:13:19,638 Epoch 61/2000
2024-11-11 14:13:35,644 Current Learning Rate: 0.0078750263
2024-11-11 14:13:36,634 Train Loss: 0.0122608, Val Loss: 0.0112667
2024-11-11 14:13:36,634 Epoch 62/2000
2024-11-11 14:13:52,697 Current Learning Rate: 0.0078104169
2024-11-11 14:13:53,451 Train Loss: 0.0122487, Val Loss: 0.0112643
2024-11-11 14:13:53,451 Epoch 63/2000
2024-11-11 14:14:08,402 Current Learning Rate: 0.0077451141
2024-11-11 14:14:09,416 Train Loss: 0.0122393, Val Loss: 0.0112629
2024-11-11 14:14:09,417 Epoch 64/2000
2024-11-11 14:14:25,244 Current Learning Rate: 0.0076791340
2024-11-11 14:14:26,298 Train Loss: 0.0122336, Val Loss: 0.0112508
2024-11-11 14:14:26,299 Epoch 65/2000
2024-11-11 14:14:41,517 Current Learning Rate: 0.0076124928
2024-11-11 14:14:42,293 Train Loss: 0.0122102, Val Loss: 0.0112134
2024-11-11 14:14:42,293 Epoch 66/2000
2024-11-11 14:14:58,575 Current Learning Rate: 0.0075452071
2024-11-11 14:14:59,539 Train Loss: 0.0122081, Val Loss: 0.0112017
2024-11-11 14:14:59,540 Epoch 67/2000
2024-11-11 14:15:15,566 Current Learning Rate: 0.0074772933
2024-11-11 14:15:16,590 Train Loss: 0.0121974, Val Loss: 0.0111912
2024-11-11 14:15:16,591 Epoch 68/2000
2024-11-11 14:15:32,850 Current Learning Rate: 0.0074087684
2024-11-11 14:15:32,850 Train Loss: 0.0121820, Val Loss: 0.0111961
2024-11-11 14:15:32,851 Epoch 69/2000
2024-11-11 14:15:48,748 Current Learning Rate: 0.0073396491
2024-11-11 14:15:49,551 Train Loss: 0.0121818, Val Loss: 0.0111584
2024-11-11 14:15:49,552 Epoch 70/2000
2024-11-11 14:16:04,629 Current Learning Rate: 0.0072699525
2024-11-11 14:16:05,721 Train Loss: 0.0121623, Val Loss: 0.0111324
2024-11-11 14:16:05,721 Epoch 71/2000
2024-11-11 14:16:22,081 Current Learning Rate: 0.0071996958
2024-11-11 14:16:22,831 Train Loss: 0.0121434, Val Loss: 0.0111188
2024-11-11 14:16:22,832 Epoch 72/2000
2024-11-11 14:16:37,019 Current Learning Rate: 0.0071288965
2024-11-11 14:16:37,901 Train Loss: 0.0121036, Val Loss: 0.0111138
2024-11-11 14:16:37,902 Epoch 73/2000
2024-11-11 14:16:53,012 Current Learning Rate: 0.0070575718
2024-11-11 14:16:53,013 Train Loss: 0.0120830, Val Loss: 0.0111231
2024-11-11 14:16:53,014 Epoch 74/2000
2024-11-11 14:17:08,410 Current Learning Rate: 0.0069857395
2024-11-11 14:17:09,336 Train Loss: 0.0120593, Val Loss: 0.0110455
2024-11-11 14:17:09,336 Epoch 75/2000
2024-11-11 14:17:25,214 Current Learning Rate: 0.0069134172
2024-11-11 14:17:26,247 Train Loss: 0.0120199, Val Loss: 0.0110349
2024-11-11 14:17:26,248 Epoch 76/2000
2024-11-11 14:17:42,284 Current Learning Rate: 0.0068406228
2024-11-11 14:17:43,352 Train Loss: 0.0120269, Val Loss: 0.0109805
2024-11-11 14:17:43,353 Epoch 77/2000
2024-11-11 14:17:58,616 Current Learning Rate: 0.0067673742
2024-11-11 14:17:59,448 Train Loss: 0.0118804, Val Loss: 0.0108571
2024-11-11 14:17:59,448 Epoch 78/2000
2024-11-11 14:18:14,848 Current Learning Rate: 0.0066936896
2024-11-11 14:18:14,849 Train Loss: 0.0118254, Val Loss: 0.0109454
2024-11-11 14:18:14,849 Epoch 79/2000
2024-11-11 14:18:31,465 Current Learning Rate: 0.0066195871
2024-11-11 14:18:32,467 Train Loss: 0.0116624, Val Loss: 0.0105468
2024-11-11 14:18:32,467 Epoch 80/2000
2024-11-11 14:18:48,726 Current Learning Rate: 0.0065450850
2024-11-11 14:18:49,776 Train Loss: 0.0116413, Val Loss: 0.0104802
2024-11-11 14:18:49,776 Epoch 81/2000
2024-11-11 14:19:05,544 Current Learning Rate: 0.0064702016
2024-11-11 14:19:06,615 Train Loss: 0.0113220, Val Loss: 0.0103361
2024-11-11 14:19:06,617 Epoch 82/2000
2024-11-11 14:19:22,882 Current Learning Rate: 0.0063949555
2024-11-11 14:19:23,886 Train Loss: 0.0111988, Val Loss: 0.0101918
2024-11-11 14:19:23,887 Epoch 83/2000
2024-11-11 14:19:40,105 Current Learning Rate: 0.0063193652
2024-11-11 14:19:41,141 Train Loss: 0.0110398, Val Loss: 0.0101036
2024-11-11 14:19:41,142 Epoch 84/2000
2024-11-11 14:19:57,519 Current Learning Rate: 0.0062434494
2024-11-11 14:19:58,597 Train Loss: 0.0109725, Val Loss: 0.0100524
2024-11-11 14:19:58,597 Epoch 85/2000
2024-11-11 14:20:14,789 Current Learning Rate: 0.0061672268
2024-11-11 14:20:15,557 Train Loss: 0.0109243, Val Loss: 0.0099134
2024-11-11 14:20:15,557 Epoch 86/2000
2024-11-11 14:20:30,737 Current Learning Rate: 0.0060907162
2024-11-11 14:20:31,628 Train Loss: 0.0107117, Val Loss: 0.0098298
2024-11-11 14:20:31,628 Epoch 87/2000
2024-11-11 14:20:46,658 Current Learning Rate: 0.0060139365
2024-11-11 14:20:46,659 Train Loss: 0.0107305, Val Loss: 0.0098862
2024-11-11 14:20:46,659 Epoch 88/2000
2024-11-11 14:21:02,896 Current Learning Rate: 0.0059369066
2024-11-11 14:21:04,661 Train Loss: 0.0105596, Val Loss: 0.0096487
2024-11-11 14:21:04,661 Epoch 89/2000
2024-11-11 14:21:20,799 Current Learning Rate: 0.0058596455
2024-11-11 14:21:21,613 Train Loss: 0.0103846, Val Loss: 0.0095496
2024-11-11 14:21:21,613 Epoch 90/2000
2024-11-11 14:21:36,590 Current Learning Rate: 0.0057821723
2024-11-11 14:21:37,478 Train Loss: 0.0102785, Val Loss: 0.0094894
2024-11-11 14:21:37,479 Epoch 91/2000
2024-11-11 14:21:52,472 Current Learning Rate: 0.0057045062
2024-11-11 14:21:53,255 Train Loss: 0.0101886, Val Loss: 0.0093482
2024-11-11 14:21:53,255 Epoch 92/2000
2024-11-11 14:22:08,255 Current Learning Rate: 0.0056266662
2024-11-11 14:22:08,256 Train Loss: 0.0100830, Val Loss: 0.0093750
2024-11-11 14:22:08,256 Epoch 93/2000
2024-11-11 14:22:24,411 Current Learning Rate: 0.0055486716
2024-11-11 14:22:25,170 Train Loss: 0.0100160, Val Loss: 0.0091938
2024-11-11 14:22:25,170 Epoch 94/2000
2024-11-11 14:22:39,379 Current Learning Rate: 0.0054705416
2024-11-11 14:22:40,156 Train Loss: 0.0099185, Val Loss: 0.0091012
2024-11-11 14:22:40,157 Epoch 95/2000
2024-11-11 14:22:55,352 Current Learning Rate: 0.0053922955
2024-11-11 14:22:56,117 Train Loss: 0.0097893, Val Loss: 0.0090778
2024-11-11 14:22:56,118 Epoch 96/2000
2024-11-11 14:23:11,657 Current Learning Rate: 0.0053139526
2024-11-11 14:23:12,400 Train Loss: 0.0097754, Val Loss: 0.0089586
2024-11-11 14:23:12,401 Epoch 97/2000
2024-11-11 14:23:28,012 Current Learning Rate: 0.0052355323
2024-11-11 14:23:28,750 Train Loss: 0.0096276, Val Loss: 0.0089473
2024-11-11 14:23:28,750 Epoch 98/2000
2024-11-11 14:23:43,761 Current Learning Rate: 0.0051570538
2024-11-11 14:23:43,762 Train Loss: 0.0096329, Val Loss: 0.0091781
2024-11-11 14:23:43,762 Epoch 99/2000
2024-11-11 14:23:59,405 Current Learning Rate: 0.0050785366
2024-11-11 14:24:00,170 Train Loss: 0.0095311, Val Loss: 0.0087892
2024-11-11 14:24:00,170 Epoch 100/2000
2024-11-11 14:24:15,090 Current Learning Rate: 0.0050000000
2024-11-11 14:24:15,921 Train Loss: 0.0094008, Val Loss: 0.0086809
2024-11-11 14:24:15,922 Epoch 101/2000
2024-11-11 14:24:32,953 Current Learning Rate: 0.0049214634
2024-11-11 14:24:32,953 Train Loss: 0.0094966, Val Loss: 0.0090536
2024-11-11 14:24:32,953 Epoch 102/2000
2024-11-11 14:24:48,689 Current Learning Rate: 0.0048429462
2024-11-11 14:24:48,690 Train Loss: 0.0098488, Val Loss: 0.0090301
2024-11-11 14:24:48,691 Epoch 103/2000
2024-11-11 14:25:03,879 Current Learning Rate: 0.0047644677
2024-11-11 14:25:03,880 Train Loss: 0.0094386, Val Loss: 0.0102315
2024-11-11 14:25:03,881 Epoch 104/2000
2024-11-11 14:25:19,963 Current Learning Rate: 0.0046860474
2024-11-11 14:25:19,964 Train Loss: 0.0097765, Val Loss: 0.0087173
2024-11-11 14:25:19,964 Epoch 105/2000
2024-11-11 14:25:35,667 Current Learning Rate: 0.0046077045
2024-11-11 14:25:35,667 Train Loss: 0.0093596, Val Loss: 0.0088081
2024-11-11 14:25:35,667 Epoch 106/2000
2024-11-11 14:25:51,694 Current Learning Rate: 0.0045294584
2024-11-11 14:25:52,495 Train Loss: 0.0093826, Val Loss: 0.0085501
2024-11-11 14:25:52,495 Epoch 107/2000
2024-11-11 14:26:07,785 Current Learning Rate: 0.0044513284
2024-11-11 14:26:07,786 Train Loss: 0.0092222, Val Loss: 0.0085908
2024-11-11 14:26:07,786 Epoch 108/2000
2024-11-11 14:26:23,748 Current Learning Rate: 0.0043733338
2024-11-11 14:26:24,524 Train Loss: 0.0091933, Val Loss: 0.0084863
2024-11-11 14:26:24,525 Epoch 109/2000
2024-11-11 14:26:39,085 Current Learning Rate: 0.0042954938
2024-11-11 14:26:39,086 Train Loss: 0.0093985, Val Loss: 0.0086324
2024-11-11 14:26:39,086 Epoch 110/2000
2024-11-11 14:26:55,275 Current Learning Rate: 0.0042178277
2024-11-11 14:26:55,275 Train Loss: 0.0093343, Val Loss: 0.0085354
2024-11-11 14:26:55,276 Epoch 111/2000
2024-11-11 14:27:11,247 Current Learning Rate: 0.0041403545
2024-11-11 14:27:12,026 Train Loss: 0.0090339, Val Loss: 0.0084420
2024-11-11 14:27:12,026 Epoch 112/2000
2024-11-11 14:27:26,618 Current Learning Rate: 0.0040630934
2024-11-11 14:27:27,462 Train Loss: 0.0089710, Val Loss: 0.0083739
2024-11-11 14:27:27,462 Epoch 113/2000
2024-11-11 14:27:42,084 Current Learning Rate: 0.0039860635
2024-11-11 14:27:42,084 Train Loss: 0.0090989, Val Loss: 0.0084235
2024-11-11 14:27:42,085 Epoch 114/2000
2024-11-11 14:27:58,247 Current Learning Rate: 0.0039092838
2024-11-11 14:27:58,248 Train Loss: 0.0089871, Val Loss: 0.0084957
2024-11-11 14:27:58,248 Epoch 115/2000
2024-11-11 14:28:14,890 Current Learning Rate: 0.0038327732
2024-11-11 14:28:14,891 Train Loss: 0.0095247, Val Loss: 0.0091416
2024-11-11 14:28:14,891 Epoch 116/2000
2024-11-11 14:28:31,490 Current Learning Rate: 0.0037565506
2024-11-11 14:28:32,222 Train Loss: 0.0090694, Val Loss: 0.0082615
2024-11-11 14:28:32,222 Epoch 117/2000
2024-11-11 14:28:46,636 Current Learning Rate: 0.0036806348
2024-11-11 14:28:46,637 Train Loss: 0.0089576, Val Loss: 0.0082832
2024-11-11 14:28:46,637 Epoch 118/2000
2024-11-11 14:29:01,774 Current Learning Rate: 0.0036050445
2024-11-11 14:29:03,468 Train Loss: 0.0088039, Val Loss: 0.0080960
2024-11-11 14:29:03,468 Epoch 119/2000
2024-11-11 14:29:17,745 Current Learning Rate: 0.0035297984
2024-11-11 14:29:17,746 Train Loss: 0.0088573, Val Loss: 0.0081461
2024-11-11 14:29:17,746 Epoch 120/2000
2024-11-11 14:29:33,047 Current Learning Rate: 0.0034549150
2024-11-11 14:29:33,837 Train Loss: 0.0085194, Val Loss: 0.0079936
2024-11-11 14:29:33,837 Epoch 121/2000
2024-11-11 14:29:48,437 Current Learning Rate: 0.0033804129
2024-11-11 14:29:49,164 Train Loss: 0.0084638, Val Loss: 0.0079802
2024-11-11 14:29:49,165 Epoch 122/2000
2024-11-11 14:30:04,560 Current Learning Rate: 0.0033063104
2024-11-11 14:30:05,343 Train Loss: 0.0083644, Val Loss: 0.0077298
2024-11-11 14:30:05,344 Epoch 123/2000
2024-11-11 14:30:19,769 Current Learning Rate: 0.0032326258
2024-11-11 14:30:20,557 Train Loss: 0.0081522, Val Loss: 0.0076117
2024-11-11 14:30:20,557 Epoch 124/2000
2024-11-11 14:30:35,764 Current Learning Rate: 0.0031593772
2024-11-11 14:30:35,765 Train Loss: 0.0081840, Val Loss: 0.0076353
2024-11-11 14:30:35,765 Epoch 125/2000
2024-11-11 14:30:51,376 Current Learning Rate: 0.0030865828
2024-11-11 14:30:52,227 Train Loss: 0.0081295, Val Loss: 0.0076054
2024-11-11 14:30:52,227 Epoch 126/2000
2024-11-11 14:31:06,569 Current Learning Rate: 0.0030142605
2024-11-11 14:31:07,344 Train Loss: 0.0079633, Val Loss: 0.0075449
2024-11-11 14:31:07,344 Epoch 127/2000
2024-11-11 14:31:22,482 Current Learning Rate: 0.0029424282
2024-11-11 14:31:23,522 Train Loss: 0.0079932, Val Loss: 0.0074044
2024-11-11 14:31:23,522 Epoch 128/2000
2024-11-11 14:31:39,151 Current Learning Rate: 0.0028711035
2024-11-11 14:31:39,152 Train Loss: 0.0079451, Val Loss: 0.0074685
2024-11-11 14:31:39,152 Epoch 129/2000
2024-11-11 14:31:53,855 Current Learning Rate: 0.0028003042
2024-11-11 14:31:54,643 Train Loss: 0.0080458, Val Loss: 0.0073241
2024-11-11 14:31:54,643 Epoch 130/2000
2024-11-11 14:32:09,356 Current Learning Rate: 0.0027300475
2024-11-11 14:32:09,357 Train Loss: 0.0077950, Val Loss: 0.0073365
2024-11-11 14:32:09,357 Epoch 131/2000
2024-11-11 14:32:24,716 Current Learning Rate: 0.0026603509
2024-11-11 14:32:24,717 Train Loss: 0.0078814, Val Loss: 0.0075878
2024-11-11 14:32:24,717 Epoch 132/2000
2024-11-11 14:32:39,992 Current Learning Rate: 0.0025912316
2024-11-11 14:32:39,993 Train Loss: 0.0077670, Val Loss: 0.0075323
2024-11-11 14:32:39,993 Epoch 133/2000
2024-11-11 14:32:55,396 Current Learning Rate: 0.0025227067
2024-11-11 14:32:56,191 Train Loss: 0.0079082, Val Loss: 0.0073139
2024-11-11 14:32:56,191 Epoch 134/2000
2024-11-11 14:33:11,173 Current Learning Rate: 0.0024547929
2024-11-11 14:33:11,982 Train Loss: 0.0078441, Val Loss: 0.0071975
2024-11-11 14:33:11,982 Epoch 135/2000
2024-11-11 14:33:26,885 Current Learning Rate: 0.0023875072
2024-11-11 14:33:27,631 Train Loss: 0.0076429, Val Loss: 0.0071971
2024-11-11 14:33:27,632 Epoch 136/2000
2024-11-11 14:33:42,905 Current Learning Rate: 0.0023208660
2024-11-11 14:33:42,906 Train Loss: 0.0077887, Val Loss: 0.0072025
2024-11-11 14:33:42,906 Epoch 137/2000
2024-11-11 14:33:59,633 Current Learning Rate: 0.0022548859
2024-11-11 14:33:59,634 Train Loss: 0.0075687, Val Loss: 0.0072376
2024-11-11 14:33:59,634 Epoch 138/2000
2024-11-11 14:34:15,815 Current Learning Rate: 0.0021895831
2024-11-11 14:34:16,588 Train Loss: 0.0076570, Val Loss: 0.0071839
2024-11-11 14:34:16,588 Epoch 139/2000
2024-11-11 14:34:31,610 Current Learning Rate: 0.0021249737
2024-11-11 14:34:32,318 Train Loss: 0.0076112, Val Loss: 0.0070536
2024-11-11 14:34:32,319 Epoch 140/2000
2024-11-11 14:34:48,361 Current Learning Rate: 0.0020610737
2024-11-11 14:34:48,362 Train Loss: 0.0074787, Val Loss: 0.0071142
2024-11-11 14:34:48,362 Epoch 141/2000
2024-11-11 14:35:04,661 Current Learning Rate: 0.0019978989
2024-11-11 14:35:05,481 Train Loss: 0.0075355, Val Loss: 0.0070531
2024-11-11 14:35:05,481 Epoch 142/2000
2024-11-11 14:35:20,933 Current Learning Rate: 0.0019354647
2024-11-11 14:35:21,667 Train Loss: 0.0074945, Val Loss: 0.0070093
2024-11-11 14:35:21,667 Epoch 143/2000
2024-11-11 14:35:37,283 Current Learning Rate: 0.0018737867
2024-11-11 14:35:38,050 Train Loss: 0.0075831, Val Loss: 0.0069973
2024-11-11 14:35:38,050 Epoch 144/2000
2024-11-11 14:35:52,508 Current Learning Rate: 0.0018128801
2024-11-11 14:35:52,509 Train Loss: 0.0074640, Val Loss: 0.0070031
2024-11-11 14:35:52,509 Epoch 145/2000
2024-11-11 14:36:07,813 Current Learning Rate: 0.0017527598
2024-11-11 14:36:07,814 Train Loss: 0.0075974, Val Loss: 0.0070424
2024-11-11 14:36:07,814 Epoch 146/2000
2024-11-11 14:36:23,208 Current Learning Rate: 0.0016934407
2024-11-11 14:36:24,031 Train Loss: 0.0074360, Val Loss: 0.0069656
2024-11-11 14:36:24,032 Epoch 147/2000
2024-11-11 14:36:38,977 Current Learning Rate: 0.0016349374
2024-11-11 14:36:40,023 Train Loss: 0.0074785, Val Loss: 0.0069548
2024-11-11 14:36:40,023 Epoch 148/2000
2024-11-11 14:36:55,988 Current Learning Rate: 0.0015772645
2024-11-11 14:36:55,989 Train Loss: 0.0074445, Val Loss: 0.0069636
2024-11-11 14:36:55,989 Epoch 149/2000
2024-11-11 14:37:12,244 Current Learning Rate: 0.0015204360
2024-11-11 14:37:13,285 Train Loss: 0.0073779, Val Loss: 0.0069269
2024-11-11 14:37:13,286 Epoch 150/2000
2024-11-11 14:37:29,307 Current Learning Rate: 0.0014644661
2024-11-11 14:37:30,181 Train Loss: 0.0074302, Val Loss: 0.0069077
2024-11-11 14:37:30,182 Epoch 151/2000
2024-11-11 14:37:45,669 Current Learning Rate: 0.0014093685
2024-11-11 14:37:45,670 Train Loss: 0.0075630, Val Loss: 0.0069273
2024-11-11 14:37:45,670 Epoch 152/2000
2024-11-11 14:38:01,698 Current Learning Rate: 0.0013551569
2024-11-11 14:38:01,699 Train Loss: 0.0074667, Val Loss: 0.0069243
2024-11-11 14:38:01,699 Epoch 153/2000
2024-11-11 14:38:17,812 Current Learning Rate: 0.0013018445
2024-11-11 14:38:17,812 Train Loss: 0.0074766, Val Loss: 0.0069102
2024-11-11 14:38:17,813 Epoch 154/2000
2024-11-11 14:38:33,435 Current Learning Rate: 0.0012494447
2024-11-11 14:38:33,436 Train Loss: 0.0073373, Val Loss: 0.0070224
2024-11-11 14:38:33,436 Epoch 155/2000
2024-11-11 14:38:48,699 Current Learning Rate: 0.0011979702
2024-11-11 14:38:49,458 Train Loss: 0.0072681, Val Loss: 0.0068770
2024-11-11 14:38:49,458 Epoch 156/2000
2024-11-11 14:39:03,830 Current Learning Rate: 0.0011474338
2024-11-11 14:39:03,831 Train Loss: 0.0073033, Val Loss: 0.0068844
2024-11-11 14:39:03,831 Epoch 157/2000
2024-11-11 14:39:19,716 Current Learning Rate: 0.0010978480
2024-11-11 14:39:19,716 Train Loss: 0.0072875, Val Loss: 0.0068927
2024-11-11 14:39:19,717 Epoch 158/2000
2024-11-11 14:39:35,281 Current Learning Rate: 0.0010492249
2024-11-11 14:39:36,151 Train Loss: 0.0073433, Val Loss: 0.0068395
2024-11-11 14:39:36,152 Epoch 159/2000
2024-11-11 14:39:51,357 Current Learning Rate: 0.0010015767
2024-11-11 14:39:52,371 Train Loss: 0.0073250, Val Loss: 0.0068180
2024-11-11 14:39:52,371 Epoch 160/2000
2024-11-11 14:40:08,373 Current Learning Rate: 0.0009549150
2024-11-11 14:40:09,421 Train Loss: 0.0073589, Val Loss: 0.0068086
2024-11-11 14:40:09,422 Epoch 161/2000
2024-11-11 14:40:25,682 Current Learning Rate: 0.0009092514
2024-11-11 14:40:25,683 Train Loss: 0.0073513, Val Loss: 0.0068151
2024-11-11 14:40:25,683 Epoch 162/2000
2024-11-11 14:40:42,024 Current Learning Rate: 0.0008645971
2024-11-11 14:40:42,024 Train Loss: 0.0073903, Val Loss: 0.0068148
2024-11-11 14:40:42,025 Epoch 163/2000
2024-11-11 14:40:57,038 Current Learning Rate: 0.0008209632
2024-11-11 14:40:57,782 Train Loss: 0.0073884, Val Loss: 0.0068027
2024-11-11 14:40:57,782 Epoch 164/2000
2024-11-11 14:41:12,557 Current Learning Rate: 0.0007783604
2024-11-11 14:41:13,381 Train Loss: 0.0073094, Val Loss: 0.0067945
2024-11-11 14:41:13,381 Epoch 165/2000
2024-11-11 14:41:27,996 Current Learning Rate: 0.0007367992
2024-11-11 14:41:28,698 Train Loss: 0.0073065, Val Loss: 0.0067943
2024-11-11 14:41:28,698 Epoch 166/2000
2024-11-11 14:41:43,623 Current Learning Rate: 0.0006962899
2024-11-11 14:41:44,318 Train Loss: 0.0073264, Val Loss: 0.0067916
2024-11-11 14:41:44,319 Epoch 167/2000
2024-11-11 14:41:59,347 Current Learning Rate: 0.0006568424
2024-11-11 14:42:00,015 Train Loss: 0.0072393, Val Loss: 0.0067788
2024-11-11 14:42:00,015 Epoch 168/2000
2024-11-11 14:42:15,282 Current Learning Rate: 0.0006184666
2024-11-11 14:42:16,149 Train Loss: 0.0072001, Val Loss: 0.0067757
2024-11-11 14:42:16,149 Epoch 169/2000
2024-11-11 14:42:31,300 Current Learning Rate: 0.0005811718
2024-11-11 14:42:32,027 Train Loss: 0.0072500, Val Loss: 0.0067664
2024-11-11 14:42:32,027 Epoch 170/2000
2024-11-11 14:42:46,442 Current Learning Rate: 0.0005449674
2024-11-11 14:42:47,150 Train Loss: 0.0071376, Val Loss: 0.0067461
2024-11-11 14:42:47,151 Epoch 171/2000
2024-11-11 14:43:02,596 Current Learning Rate: 0.0005098621
2024-11-11 14:43:03,505 Train Loss: 0.0072696, Val Loss: 0.0067427
2024-11-11 14:43:03,506 Epoch 172/2000
2024-11-11 14:43:19,383 Current Learning Rate: 0.0004758647
2024-11-11 14:43:19,384 Train Loss: 0.0072316, Val Loss: 0.0067536
2024-11-11 14:43:19,385 Epoch 173/2000
2024-11-11 14:43:36,058 Current Learning Rate: 0.0004429836
2024-11-11 14:43:36,791 Train Loss: 0.0071943, Val Loss: 0.0067361
2024-11-11 14:43:36,791 Epoch 174/2000
2024-11-11 14:43:51,310 Current Learning Rate: 0.0004112269
2024-11-11 14:43:52,133 Train Loss: 0.0071758, Val Loss: 0.0067280
2024-11-11 14:43:52,133 Epoch 175/2000
2024-11-11 14:44:07,358 Current Learning Rate: 0.0003806023
2024-11-11 14:44:07,359 Train Loss: 0.0072222, Val Loss: 0.0067455
2024-11-11 14:44:07,359 Epoch 176/2000
2024-11-11 14:44:22,137 Current Learning Rate: 0.0003511176
2024-11-11 14:44:23,001 Train Loss: 0.0071256, Val Loss: 0.0067152
2024-11-11 14:44:23,001 Epoch 177/2000
2024-11-11 14:44:37,471 Current Learning Rate: 0.0003227798
2024-11-11 14:44:37,471 Train Loss: 0.0071485, Val Loss: 0.0067262
2024-11-11 14:44:37,472 Epoch 178/2000
2024-11-11 14:44:52,901 Current Learning Rate: 0.0002955962
2024-11-11 14:44:53,647 Train Loss: 0.0071857, Val Loss: 0.0067099
2024-11-11 14:44:53,647 Epoch 179/2000
2024-11-11 14:45:08,428 Current Learning Rate: 0.0002695732
2024-11-11 14:45:08,429 Train Loss: 0.0072040, Val Loss: 0.0067122
2024-11-11 14:45:08,430 Epoch 180/2000
2024-11-11 14:45:24,485 Current Learning Rate: 0.0002447174
2024-11-11 14:45:24,486 Train Loss: 0.0072393, Val Loss: 0.0067132
2024-11-11 14:45:24,486 Epoch 181/2000
2024-11-11 14:45:40,812 Current Learning Rate: 0.0002210349
2024-11-11 14:45:41,560 Train Loss: 0.0071451, Val Loss: 0.0066998
2024-11-11 14:45:41,561 Epoch 182/2000
2024-11-11 14:45:56,967 Current Learning Rate: 0.0001985316
2024-11-11 14:45:56,969 Train Loss: 0.0071755, Val Loss: 0.0067005
2024-11-11 14:45:56,969 Epoch 183/2000
2024-11-11 14:46:12,765 Current Learning Rate: 0.0001772129
2024-11-11 14:46:12,766 Train Loss: 0.0073248, Val Loss: 0.0067023
2024-11-11 14:46:12,766 Epoch 184/2000
2024-11-11 14:46:28,837 Current Learning Rate: 0.0001570842
2024-11-11 14:46:29,586 Train Loss: 0.0071232, Val Loss: 0.0066986
2024-11-11 14:46:29,587 Epoch 185/2000
2024-11-11 14:46:44,200 Current Learning Rate: 0.0001381504
2024-11-11 14:46:44,200 Train Loss: 0.0071744, Val Loss: 0.0067003
2024-11-11 14:46:44,200 Epoch 186/2000
2024-11-11 14:47:00,589 Current Learning Rate: 0.0001204162
2024-11-11 14:47:00,590 Train Loss: 0.0070693, Val Loss: 0.0067045
2024-11-11 14:47:00,590 Epoch 187/2000
2024-11-11 14:47:16,675 Current Learning Rate: 0.0001038859
2024-11-11 14:47:17,469 Train Loss: 0.0072742, Val Loss: 0.0066943
2024-11-11 14:47:17,469 Epoch 188/2000
2024-11-11 14:47:33,364 Current Learning Rate: 0.0000885637
2024-11-11 14:47:34,337 Train Loss: 0.0071269, Val Loss: 0.0066895
2024-11-11 14:47:34,338 Epoch 189/2000
2024-11-11 14:47:50,191 Current Learning Rate: 0.0000744534
2024-11-11 14:47:51,126 Train Loss: 0.0071737, Val Loss: 0.0066882
2024-11-11 14:47:51,127 Epoch 190/2000
2024-11-11 14:48:06,026 Current Learning Rate: 0.0000615583
2024-11-11 14:48:06,820 Train Loss: 0.0070884, Val Loss: 0.0066866
2024-11-11 14:48:06,820 Epoch 191/2000
2024-11-11 14:48:21,312 Current Learning Rate: 0.0000498817
2024-11-11 14:48:21,313 Train Loss: 0.0071572, Val Loss: 0.0066868
2024-11-11 14:48:21,313 Epoch 192/2000
2024-11-11 14:48:37,373 Current Learning Rate: 0.0000394265
2024-11-11 14:48:38,177 Train Loss: 0.0071594, Val Loss: 0.0066853
2024-11-11 14:48:38,178 Epoch 193/2000
2024-11-11 14:48:52,743 Current Learning Rate: 0.0000301952
2024-11-11 14:48:52,744 Train Loss: 0.0071081, Val Loss: 0.0066857
2024-11-11 14:48:52,744 Epoch 194/2000
2024-11-11 14:49:08,362 Current Learning Rate: 0.0000221902
2024-11-11 14:49:09,116 Train Loss: 0.0071270, Val Loss: 0.0066851
2024-11-11 14:49:09,117 Epoch 195/2000
2024-11-11 14:49:23,562 Current Learning Rate: 0.0000154133
2024-11-11 14:49:24,293 Train Loss: 0.0071012, Val Loss: 0.0066851
2024-11-11 14:49:24,293 Epoch 196/2000
2024-11-11 14:49:38,864 Current Learning Rate: 0.0000098664
2024-11-11 14:49:38,865 Train Loss: 0.0071697, Val Loss: 0.0066872
2024-11-11 14:49:38,865 Epoch 197/2000
2024-11-11 14:49:54,603 Current Learning Rate: 0.0000055506
2024-11-11 14:49:54,604 Train Loss: 0.0071689, Val Loss: 0.0066890
2024-11-11 14:49:54,604 Epoch 198/2000
2024-11-11 14:50:11,226 Current Learning Rate: 0.0000024672
2024-11-11 14:50:11,227 Train Loss: 0.0072255, Val Loss: 0.0066871
2024-11-11 14:50:11,227 Epoch 199/2000
2024-11-11 14:50:27,207 Current Learning Rate: 0.0000006168
2024-11-11 14:50:27,208 Train Loss: 0.0071076, Val Loss: 0.0066866
2024-11-11 14:50:27,209 Epoch 200/2000
2024-11-11 14:50:42,650 Current Learning Rate: 0.0000000000
2024-11-11 14:50:42,651 Train Loss: 0.0071233, Val Loss: 0.0066863
2024-11-11 14:50:42,651 Epoch 201/2000
2024-11-11 14:50:59,131 Current Learning Rate: 0.0000006168
2024-11-11 14:50:59,132 Train Loss: 0.0071115, Val Loss: 0.0066865
2024-11-11 14:50:59,132 Epoch 202/2000
2024-11-11 14:51:15,454 Current Learning Rate: 0.0000024672
2024-11-11 14:51:15,455 Train Loss: 0.0071187, Val Loss: 0.0066864
2024-11-11 14:51:15,456 Epoch 203/2000
2024-11-11 14:51:30,824 Current Learning Rate: 0.0000055506
2024-11-11 14:51:30,825 Train Loss: 0.0071984, Val Loss: 0.0066861
2024-11-11 14:51:30,825 Epoch 204/2000
2024-11-11 14:51:46,130 Current Learning Rate: 0.0000098664
2024-11-11 14:51:46,131 Train Loss: 0.0071159, Val Loss: 0.0066870
2024-11-11 14:51:46,131 Epoch 205/2000
2024-11-11 14:52:01,915 Current Learning Rate: 0.0000154133
2024-11-11 14:52:01,916 Train Loss: 0.0071094, Val Loss: 0.0066877
2024-11-11 14:52:01,916 Epoch 206/2000
2024-11-11 14:52:18,069 Current Learning Rate: 0.0000221902
2024-11-11 14:52:18,069 Train Loss: 0.0071672, Val Loss: 0.0066869
2024-11-11 14:52:18,070 Epoch 207/2000
2024-11-11 14:52:34,326 Current Learning Rate: 0.0000301952
2024-11-11 14:52:35,349 Train Loss: 0.0071063, Val Loss: 0.0066836
2024-11-11 14:52:35,349 Epoch 208/2000
2024-11-11 14:52:50,950 Current Learning Rate: 0.0000394265
2024-11-11 14:52:50,951 Train Loss: 0.0072819, Val Loss: 0.0066849
2024-11-11 14:52:50,951 Epoch 209/2000
2024-11-11 14:53:06,823 Current Learning Rate: 0.0000498817
2024-11-11 14:53:06,824 Train Loss: 0.0071576, Val Loss: 0.0066842
2024-11-11 14:53:06,824 Epoch 210/2000
2024-11-11 14:53:22,350 Current Learning Rate: 0.0000615583
2024-11-11 14:53:23,380 Train Loss: 0.0071084, Val Loss: 0.0066829
2024-11-11 14:53:23,380 Epoch 211/2000
2024-11-11 14:53:38,583 Current Learning Rate: 0.0000744534
2024-11-11 14:53:39,588 Train Loss: 0.0071460, Val Loss: 0.0066817
2024-11-11 14:53:39,589 Epoch 212/2000
2024-11-11 14:53:56,036 Current Learning Rate: 0.0000885637
2024-11-11 14:53:57,106 Train Loss: 0.0070951, Val Loss: 0.0066803
2024-11-11 14:53:57,106 Epoch 213/2000
2024-11-11 14:54:13,344 Current Learning Rate: 0.0001038859
2024-11-11 14:54:13,345 Train Loss: 0.0071032, Val Loss: 0.0066816
2024-11-11 14:54:13,345 Epoch 214/2000
2024-11-11 14:54:29,228 Current Learning Rate: 0.0001204162
2024-11-11 14:54:29,229 Train Loss: 0.0071267, Val Loss: 0.0066882
2024-11-11 14:54:29,229 Epoch 215/2000
2024-11-11 14:54:44,483 Current Learning Rate: 0.0001381504
2024-11-11 14:54:44,484 Train Loss: 0.0073151, Val Loss: 0.0066887
2024-11-11 14:54:44,484 Epoch 216/2000
2024-11-11 14:55:00,812 Current Learning Rate: 0.0001570842
2024-11-11 14:55:00,812 Train Loss: 0.0072554, Val Loss: 0.0066824
2024-11-11 14:55:00,812 Epoch 217/2000
2024-11-11 14:55:16,058 Current Learning Rate: 0.0001772129
2024-11-11 14:55:16,059 Train Loss: 0.0071964, Val Loss: 0.0066823
2024-11-11 14:55:16,059 Epoch 218/2000
2024-11-11 14:55:33,319 Current Learning Rate: 0.0001985316
2024-11-11 14:55:34,352 Train Loss: 0.0072060, Val Loss: 0.0066800
2024-11-11 14:55:34,352 Epoch 219/2000
2024-11-11 14:55:50,242 Current Learning Rate: 0.0002210349
2024-11-11 14:55:50,243 Train Loss: 0.0074376, Val Loss: 0.0066992
2024-11-11 14:55:50,243 Epoch 220/2000
2024-11-11 14:56:06,937 Current Learning Rate: 0.0002447174
2024-11-11 14:56:06,937 Train Loss: 0.0073687, Val Loss: 0.0066983
2024-11-11 14:56:06,937 Epoch 221/2000
2024-11-11 14:56:21,670 Current Learning Rate: 0.0002695732
2024-11-11 14:56:21,671 Train Loss: 0.0071589, Val Loss: 0.0066803
2024-11-11 14:56:21,671 Epoch 222/2000
2024-11-11 14:56:37,122 Current Learning Rate: 0.0002955962
2024-11-11 14:56:37,929 Train Loss: 0.0070669, Val Loss: 0.0066722
2024-11-11 14:56:37,930 Epoch 223/2000
2024-11-11 14:56:52,638 Current Learning Rate: 0.0003227798
2024-11-11 14:56:52,639 Train Loss: 0.0072259, Val Loss: 0.0066722
2024-11-11 14:56:52,639 Epoch 224/2000
2024-11-11 14:57:08,152 Current Learning Rate: 0.0003511176
2024-11-11 14:57:08,153 Train Loss: 0.0071376, Val Loss: 0.0066735
2024-11-11 14:57:08,153 Epoch 225/2000
2024-11-11 14:57:24,526 Current Learning Rate: 0.0003806023
2024-11-11 14:57:25,578 Train Loss: 0.0070889, Val Loss: 0.0066610
2024-11-11 14:57:25,579 Epoch 226/2000
2024-11-11 14:57:41,353 Current Learning Rate: 0.0004112269
2024-11-11 14:57:42,354 Train Loss: 0.0070849, Val Loss: 0.0066532
2024-11-11 14:57:42,354 Epoch 227/2000
2024-11-11 14:57:57,520 Current Learning Rate: 0.0004429836
2024-11-11 14:57:57,521 Train Loss: 0.0072774, Val Loss: 0.0066535
2024-11-11 14:57:57,521 Epoch 228/2000
2024-11-11 14:58:13,445 Current Learning Rate: 0.0004758647
2024-11-11 14:58:13,446 Train Loss: 0.0071172, Val Loss: 0.0068272
2024-11-11 14:58:13,446 Epoch 229/2000
2024-11-11 14:58:30,203 Current Learning Rate: 0.0005098621
2024-11-11 14:58:30,203 Train Loss: 0.0070752, Val Loss: 0.0066571
2024-11-11 14:58:30,204 Epoch 230/2000
2024-11-11 14:58:46,594 Current Learning Rate: 0.0005449674
2024-11-11 14:58:46,595 Train Loss: 0.0072198, Val Loss: 0.0067566
2024-11-11 14:58:46,595 Epoch 231/2000
2024-11-11 14:59:03,017 Current Learning Rate: 0.0005811718
2024-11-11 14:59:03,018 Train Loss: 0.0071745, Val Loss: 0.0068617
2024-11-11 14:59:03,018 Epoch 232/2000
2024-11-11 14:59:18,849 Current Learning Rate: 0.0006184666
2024-11-11 14:59:18,850 Train Loss: 0.0071195, Val Loss: 0.0066977
2024-11-11 14:59:18,850 Epoch 233/2000
2024-11-11 14:59:35,116 Current Learning Rate: 0.0006568424
2024-11-11 14:59:35,117 Train Loss: 0.0071271, Val Loss: 0.0066595
2024-11-11 14:59:35,117 Epoch 234/2000
2024-11-11 14:59:50,317 Current Learning Rate: 0.0006962899
2024-11-11 14:59:51,113 Train Loss: 0.0071020, Val Loss: 0.0066361
2024-11-11 14:59:51,113 Epoch 235/2000
2024-11-11 15:00:07,488 Current Learning Rate: 0.0007367992
2024-11-11 15:00:07,489 Train Loss: 0.0070847, Val Loss: 0.0066725
2024-11-11 15:00:07,489 Epoch 236/2000
2024-11-11 15:00:23,002 Current Learning Rate: 0.0007783604
2024-11-11 15:00:23,791 Train Loss: 0.0070665, Val Loss: 0.0065985
2024-11-11 15:00:23,792 Epoch 237/2000
2024-11-11 15:00:38,908 Current Learning Rate: 0.0008209632
2024-11-11 15:00:38,908 Train Loss: 0.0072264, Val Loss: 0.0066265
2024-11-11 15:00:38,909 Epoch 238/2000
2024-11-11 15:00:54,586 Current Learning Rate: 0.0008645971
2024-11-11 15:00:54,587 Train Loss: 0.0072261, Val Loss: 0.0066852
2024-11-11 15:00:54,587 Epoch 239/2000
2024-11-11 15:01:10,337 Current Learning Rate: 0.0009092514
2024-11-11 15:01:11,348 Train Loss: 0.0070137, Val Loss: 0.0065943
2024-11-11 15:01:11,349 Epoch 240/2000
2024-11-11 15:01:27,526 Current Learning Rate: 0.0009549150
2024-11-11 15:01:27,527 Train Loss: 0.0071517, Val Loss: 0.0066673
2024-11-11 15:01:27,528 Epoch 241/2000
2024-11-11 15:01:43,314 Current Learning Rate: 0.0010015767
2024-11-11 15:01:43,315 Train Loss: 0.0071173, Val Loss: 0.0066433
2024-11-11 15:01:43,315 Epoch 242/2000
2024-11-11 15:01:58,615 Current Learning Rate: 0.0010492249
2024-11-11 15:01:59,421 Train Loss: 0.0070518, Val Loss: 0.0065926
2024-11-11 15:01:59,421 Epoch 243/2000
2024-11-11 15:02:14,143 Current Learning Rate: 0.0010978480
2024-11-11 15:02:14,143 Train Loss: 0.0070750, Val Loss: 0.0068506
2024-11-11 15:02:14,144 Epoch 244/2000
2024-11-11 15:02:31,550 Current Learning Rate: 0.0011474338
2024-11-11 15:02:31,551 Train Loss: 0.0070213, Val Loss: 0.0067021
2024-11-11 15:02:31,551 Epoch 245/2000
2024-11-11 15:02:48,857 Current Learning Rate: 0.0011979702
2024-11-11 15:02:49,666 Train Loss: 0.0070417, Val Loss: 0.0065648
2024-11-11 15:02:49,666 Epoch 246/2000
2024-11-11 15:03:04,849 Current Learning Rate: 0.0012494447
2024-11-11 15:03:04,850 Train Loss: 0.0071130, Val Loss: 0.0067087
2024-11-11 15:03:04,851 Epoch 247/2000
2024-11-11 15:03:20,549 Current Learning Rate: 0.0013018445
2024-11-11 15:03:20,550 Train Loss: 0.0069857, Val Loss: 0.0067104
2024-11-11 15:03:20,550 Epoch 248/2000
2024-11-11 15:03:36,330 Current Learning Rate: 0.0013551569
2024-11-11 15:03:36,331 Train Loss: 0.0074562, Val Loss: 0.0067111
2024-11-11 15:03:36,331 Epoch 249/2000
2024-11-11 15:03:52,210 Current Learning Rate: 0.0014093685
2024-11-11 15:03:52,210 Train Loss: 0.0072090, Val Loss: 0.0068691
2024-11-11 15:03:52,211 Epoch 250/2000
2024-11-11 15:04:08,081 Current Learning Rate: 0.0014644661
2024-11-11 15:04:08,947 Train Loss: 0.0070478, Val Loss: 0.0065327
2024-11-11 15:04:08,947 Epoch 251/2000
2024-11-11 15:04:24,339 Current Learning Rate: 0.0015204360
2024-11-11 15:04:24,340 Train Loss: 0.0070886, Val Loss: 0.0065896
2024-11-11 15:04:24,340 Epoch 252/2000
2024-11-11 15:04:40,240 Current Learning Rate: 0.0015772645
2024-11-11 15:04:40,240 Train Loss: 0.0071351, Val Loss: 0.0065516
2024-11-11 15:04:40,240 Epoch 253/2000
2024-11-11 15:04:56,623 Current Learning Rate: 0.0016349374
2024-11-11 15:04:57,379 Train Loss: 0.0068435, Val Loss: 0.0064097
2024-11-11 15:04:57,379 Epoch 254/2000
2024-11-11 15:05:12,359 Current Learning Rate: 0.0016934407
2024-11-11 15:05:12,360 Train Loss: 0.0068755, Val Loss: 0.0077757
2024-11-11 15:05:12,360 Epoch 255/2000
2024-11-11 15:05:28,933 Current Learning Rate: 0.0017527598
2024-11-11 15:05:28,933 Train Loss: 0.0070848, Val Loss: 0.0064744
2024-11-11 15:05:28,934 Epoch 256/2000
2024-11-11 15:05:43,919 Current Learning Rate: 0.0018128801
2024-11-11 15:05:43,920 Train Loss: 0.0069190, Val Loss: 0.0065420
2024-11-11 15:05:43,920 Epoch 257/2000
2024-11-11 15:05:59,080 Current Learning Rate: 0.0018737867
2024-11-11 15:05:59,781 Train Loss: 0.0067664, Val Loss: 0.0063515
2024-11-11 15:05:59,781 Epoch 258/2000
2024-11-11 15:06:15,350 Current Learning Rate: 0.0019354647
2024-11-11 15:06:16,171 Train Loss: 0.0069395, Val Loss: 0.0062760
2024-11-11 15:06:16,172 Epoch 259/2000
2024-11-11 15:06:32,221 Current Learning Rate: 0.0019978989
2024-11-11 15:06:32,221 Train Loss: 0.0067314, Val Loss: 0.0064107
2024-11-11 15:06:32,222 Epoch 260/2000
2024-11-11 15:06:49,390 Current Learning Rate: 0.0020610737
2024-11-11 15:06:49,391 Train Loss: 0.0069326, Val Loss: 0.0065176
2024-11-11 15:06:49,391 Epoch 261/2000
2024-11-11 15:07:05,060 Current Learning Rate: 0.0021249737
2024-11-11 15:07:05,060 Train Loss: 0.0069830, Val Loss: 0.0064641
2024-11-11 15:07:05,061 Epoch 262/2000
2024-11-11 15:07:20,826 Current Learning Rate: 0.0021895831
2024-11-11 15:07:21,915 Train Loss: 0.0068408, Val Loss: 0.0062663
2024-11-11 15:07:21,915 Epoch 263/2000
2024-11-11 15:07:38,287 Current Learning Rate: 0.0022548859
2024-11-11 15:07:38,288 Train Loss: 0.0067675, Val Loss: 0.0064039
2024-11-11 15:07:38,288 Epoch 264/2000
2024-11-11 15:07:54,780 Current Learning Rate: 0.0023208660
2024-11-11 15:07:55,593 Train Loss: 0.0069052, Val Loss: 0.0062365
2024-11-11 15:07:55,593 Epoch 265/2000
2024-11-11 15:08:10,051 Current Learning Rate: 0.0023875072
2024-11-11 15:08:10,927 Train Loss: 0.0067346, Val Loss: 0.0061322
2024-11-11 15:08:10,927 Epoch 266/2000
2024-11-11 15:08:26,223 Current Learning Rate: 0.0024547929
2024-11-11 15:08:26,224 Train Loss: 0.0069941, Val Loss: 0.0063703
2024-11-11 15:08:26,224 Epoch 267/2000
2024-11-11 15:08:41,711 Current Learning Rate: 0.0025227067
2024-11-11 15:08:41,712 Train Loss: 0.0069256, Val Loss: 0.0064368
2024-11-11 15:08:41,712 Epoch 268/2000
2024-11-11 15:08:58,433 Current Learning Rate: 0.0025912316
2024-11-11 15:08:58,434 Train Loss: 0.0069142, Val Loss: 0.0061911
2024-11-11 15:08:58,434 Epoch 269/2000
2024-11-11 15:09:14,540 Current Learning Rate: 0.0026603509
2024-11-11 15:09:14,541 Train Loss: 0.0067452, Val Loss: 0.0063971
2024-11-11 15:09:14,541 Epoch 270/2000
2024-11-11 15:09:31,423 Current Learning Rate: 0.0027300475
2024-11-11 15:09:31,423 Train Loss: 0.0066415, Val Loss: 0.0061838
2024-11-11 15:09:31,423 Epoch 271/2000
2024-11-11 15:09:46,756 Current Learning Rate: 0.0028003042
2024-11-11 15:09:47,536 Train Loss: 0.0068366, Val Loss: 0.0059799
2024-11-11 15:09:47,536 Epoch 272/2000
2024-11-11 15:10:02,691 Current Learning Rate: 0.0028711035
2024-11-11 15:10:02,692 Train Loss: 0.0064761, Val Loss: 0.0064109
2024-11-11 15:10:02,692 Epoch 273/2000
2024-11-11 15:10:18,359 Current Learning Rate: 0.0029424282
2024-11-11 15:10:18,359 Train Loss: 0.0079866, Val Loss: 0.0064800
2024-11-11 15:10:18,359 Epoch 274/2000
2024-11-11 15:10:33,310 Current Learning Rate: 0.0030142605
2024-11-11 15:10:34,040 Train Loss: 0.0064719, Val Loss: 0.0059216
2024-11-11 15:10:34,040 Epoch 275/2000
2024-11-11 15:10:48,702 Current Learning Rate: 0.0030865828
2024-11-11 15:10:49,468 Train Loss: 0.0063961, Val Loss: 0.0059214
2024-11-11 15:10:49,469 Epoch 276/2000
2024-11-11 15:11:03,948 Current Learning Rate: 0.0031593772
2024-11-11 15:11:03,949 Train Loss: 0.0064374, Val Loss: 0.0063072
2024-11-11 15:11:03,949 Epoch 277/2000
2024-11-11 15:11:20,020 Current Learning Rate: 0.0032326258
2024-11-11 15:11:20,020 Train Loss: 0.0063913, Val Loss: 0.0059689
2024-11-11 15:11:20,020 Epoch 278/2000
2024-11-11 15:11:35,884 Current Learning Rate: 0.0033063104
2024-11-11 15:11:36,678 Train Loss: 0.0063676, Val Loss: 0.0058734
2024-11-11 15:11:36,678 Epoch 279/2000
2024-11-11 15:11:51,198 Current Learning Rate: 0.0033804129
2024-11-11 15:11:51,199 Train Loss: 0.0070263, Val Loss: 0.0060165
2024-11-11 15:11:51,199 Epoch 280/2000
2024-11-11 15:12:06,757 Current Learning Rate: 0.0034549150
2024-11-11 15:12:07,563 Train Loss: 0.0064468, Val Loss: 0.0058134
2024-11-11 15:12:07,563 Epoch 281/2000
2024-11-11 15:12:22,441 Current Learning Rate: 0.0035297984
2024-11-11 15:12:22,442 Train Loss: 0.0063803, Val Loss: 0.0059726
2024-11-11 15:12:22,442 Epoch 282/2000
2024-11-11 15:12:38,403 Current Learning Rate: 0.0036050445
2024-11-11 15:12:38,404 Train Loss: 0.0065073, Val Loss: 0.0061817
2024-11-11 15:12:38,404 Epoch 283/2000
2024-11-11 15:12:54,608 Current Learning Rate: 0.0036806348
2024-11-11 15:12:54,608 Train Loss: 0.0064057, Val Loss: 0.0058602
2024-11-11 15:12:54,609 Epoch 284/2000
2024-11-11 15:13:09,226 Current Learning Rate: 0.0037565506
2024-11-11 15:13:10,089 Train Loss: 0.0066021, Val Loss: 0.0057578
2024-11-11 15:13:10,089 Epoch 285/2000
2024-11-11 15:13:24,732 Current Learning Rate: 0.0038327732
2024-11-11 15:13:25,544 Train Loss: 0.0062451, Val Loss: 0.0056805
2024-11-11 15:13:25,544 Epoch 286/2000
2024-11-11 15:13:40,205 Current Learning Rate: 0.0039092838
2024-11-11 15:13:40,983 Train Loss: 0.0062877, Val Loss: 0.0055990
2024-11-11 15:13:40,984 Epoch 287/2000
2024-11-11 15:13:55,568 Current Learning Rate: 0.0039860635
2024-11-11 15:13:55,569 Train Loss: 0.0062811, Val Loss: 0.0060695
2024-11-11 15:13:55,569 Epoch 288/2000
2024-11-11 15:14:11,163 Current Learning Rate: 0.0040630934
2024-11-11 15:14:11,164 Train Loss: 0.0060915, Val Loss: 0.0056859
2024-11-11 15:14:11,164 Epoch 289/2000
2024-11-11 15:14:27,326 Current Learning Rate: 0.0041403545
2024-11-11 15:14:27,327 Train Loss: 0.0065595, Val Loss: 0.0059594
2024-11-11 15:14:27,327 Epoch 290/2000
2024-11-11 15:14:43,423 Current Learning Rate: 0.0042178277
2024-11-11 15:14:43,423 Train Loss: 0.0061185, Val Loss: 0.0057626
2024-11-11 15:14:43,423 Epoch 291/2000
2024-11-11 15:14:58,230 Current Learning Rate: 0.0042954938
2024-11-11 15:14:58,230 Train Loss: 0.0063215, Val Loss: 0.0056026
2024-11-11 15:14:58,230 Epoch 292/2000
2024-11-11 15:15:14,010 Current Learning Rate: 0.0043733338
2024-11-11 15:15:14,831 Train Loss: 0.0060687, Val Loss: 0.0055891
2024-11-11 15:15:14,832 Epoch 293/2000
2024-11-11 15:15:29,437 Current Learning Rate: 0.0044513284
2024-11-11 15:15:29,438 Train Loss: 0.0068635, Val Loss: 0.0066914
2024-11-11 15:15:29,438 Epoch 294/2000
2024-11-11 15:15:45,421 Current Learning Rate: 0.0045294584
2024-11-11 15:15:45,422 Train Loss: 0.0064227, Val Loss: 0.0056219
2024-11-11 15:15:45,422 Epoch 295/2000
2024-11-11 15:16:01,130 Current Learning Rate: 0.0046077045
2024-11-11 15:16:01,883 Train Loss: 0.0058091, Val Loss: 0.0054769
2024-11-11 15:16:01,883 Epoch 296/2000
2024-11-11 15:16:17,237 Current Learning Rate: 0.0046860474
2024-11-11 15:16:18,313 Train Loss: 0.0060037, Val Loss: 0.0054160
2024-11-11 15:16:18,313 Epoch 297/2000
2024-11-11 15:16:34,185 Current Learning Rate: 0.0047644677
2024-11-11 15:16:34,186 Train Loss: 0.0059141, Val Loss: 0.0054338
2024-11-11 15:16:34,187 Epoch 298/2000
2024-11-11 15:16:48,856 Current Learning Rate: 0.0048429462
2024-11-11 15:16:48,857 Train Loss: 0.0058849, Val Loss: 0.0055657
2024-11-11 15:16:48,857 Epoch 299/2000
2024-11-11 15:17:04,508 Current Learning Rate: 0.0049214634
2024-11-11 15:17:04,509 Train Loss: 0.0061243, Val Loss: 0.0054230
2024-11-11 15:17:04,509 Epoch 300/2000
2024-11-11 15:17:19,857 Current Learning Rate: 0.0050000000
2024-11-11 15:17:19,858 Train Loss: 0.0058888, Val Loss: 0.0054293
2024-11-11 15:17:19,858 Epoch 301/2000
2024-11-11 15:17:35,862 Current Learning Rate: 0.0050785366
2024-11-11 15:17:35,863 Train Loss: 0.0057903, Val Loss: 0.0056493
2024-11-11 15:17:35,863 Epoch 302/2000
2024-11-11 15:17:51,352 Current Learning Rate: 0.0051570538
2024-11-11 15:17:52,087 Train Loss: 0.0056770, Val Loss: 0.0053373
2024-11-11 15:17:52,088 Epoch 303/2000
2024-11-11 15:18:06,363 Current Learning Rate: 0.0052355323
2024-11-11 15:18:06,364 Train Loss: 0.0060834, Val Loss: 0.0056789
2024-11-11 15:18:06,364 Epoch 304/2000
2024-11-11 15:18:21,697 Current Learning Rate: 0.0053139526
2024-11-11 15:18:21,698 Train Loss: 0.0057881, Val Loss: 0.0055695
2024-11-11 15:18:21,698 Epoch 305/2000
2024-11-11 15:18:37,500 Current Learning Rate: 0.0053922955
2024-11-11 15:18:38,558 Train Loss: 0.0057892, Val Loss: 0.0052579
2024-11-11 15:18:38,558 Epoch 306/2000
2024-11-11 15:18:54,840 Current Learning Rate: 0.0054705416
2024-11-11 15:18:54,841 Train Loss: 0.0056334, Val Loss: 0.0060894
2024-11-11 15:18:54,841 Epoch 307/2000
2024-11-11 15:19:11,104 Current Learning Rate: 0.0055486716
2024-11-11 15:19:11,105 Train Loss: 0.0059123, Val Loss: 0.0059840
2024-11-11 15:19:11,105 Epoch 308/2000
2024-11-11 15:19:26,922 Current Learning Rate: 0.0056266662
2024-11-11 15:19:26,922 Train Loss: 0.0056471, Val Loss: 0.0054598
2024-11-11 15:19:26,923 Epoch 309/2000
2024-11-11 15:19:41,710 Current Learning Rate: 0.0057045062
2024-11-11 15:19:41,710 Train Loss: 0.0057048, Val Loss: 0.0055944
2024-11-11 15:19:41,710 Epoch 310/2000
2024-11-11 15:19:58,069 Current Learning Rate: 0.0057821723
2024-11-11 15:19:58,070 Train Loss: 0.0056177, Val Loss: 0.0061200
2024-11-11 15:19:58,070 Epoch 311/2000
2024-11-11 15:20:13,596 Current Learning Rate: 0.0058596455
2024-11-11 15:20:14,350 Train Loss: 0.0056118, Val Loss: 0.0051161
2024-11-11 15:20:14,350 Epoch 312/2000
2024-11-11 15:20:29,284 Current Learning Rate: 0.0059369066
2024-11-11 15:20:29,285 Train Loss: 0.0054531, Val Loss: 0.0070531
2024-11-11 15:20:29,286 Epoch 313/2000
2024-11-11 15:20:44,765 Current Learning Rate: 0.0060139365
2024-11-11 15:20:44,765 Train Loss: 0.0058016, Val Loss: 0.0055967
2024-11-11 15:20:44,765 Epoch 314/2000
2024-11-11 15:21:00,523 Current Learning Rate: 0.0060907162
2024-11-11 15:21:01,227 Train Loss: 0.0054130, Val Loss: 0.0049143
2024-11-11 15:21:01,227 Epoch 315/2000
2024-11-11 15:21:15,949 Current Learning Rate: 0.0061672268
2024-11-11 15:21:15,949 Train Loss: 0.0056265, Val Loss: 0.0052043
2024-11-11 15:21:15,949 Epoch 316/2000
2024-11-11 15:21:31,215 Current Learning Rate: 0.0062434494
2024-11-11 15:21:31,215 Train Loss: 0.0054177, Val Loss: 0.0049175
2024-11-11 15:21:31,215 Epoch 317/2000
2024-11-11 15:21:47,057 Current Learning Rate: 0.0063193652
2024-11-11 15:21:47,060 Train Loss: 0.0052038, Val Loss: 0.0055301
2024-11-11 15:21:47,066 Epoch 318/2000
2024-11-11 15:22:03,473 Current Learning Rate: 0.0063949555
2024-11-11 15:22:03,474 Train Loss: 0.0052530, Val Loss: 0.0049315
2024-11-11 15:22:03,474 Epoch 319/2000
2024-11-11 15:22:19,409 Current Learning Rate: 0.0064702016
2024-11-11 15:22:19,409 Train Loss: 0.0053585, Val Loss: 0.0050218
2024-11-11 15:22:19,409 Epoch 320/2000
2024-11-11 15:22:35,355 Current Learning Rate: 0.0065450850
2024-11-11 15:22:35,356 Train Loss: 0.0053360, Val Loss: 0.0052287
2024-11-11 15:22:35,357 Epoch 321/2000
2024-11-11 15:22:52,864 Current Learning Rate: 0.0066195871
2024-11-11 15:22:53,914 Train Loss: 0.0052233, Val Loss: 0.0048462
2024-11-11 15:22:53,915 Epoch 322/2000
2024-11-11 15:23:10,018 Current Learning Rate: 0.0066936896
2024-11-11 15:23:10,019 Train Loss: 0.0053973, Val Loss: 0.0054552
2024-11-11 15:23:10,019 Epoch 323/2000
2024-11-11 15:23:25,870 Current Learning Rate: 0.0067673742
2024-11-11 15:23:26,659 Train Loss: 0.0050311, Val Loss: 0.0045520
2024-11-11 15:23:26,660 Epoch 324/2000
2024-11-11 15:23:42,273 Current Learning Rate: 0.0068406228
2024-11-11 15:23:42,274 Train Loss: 0.0051359, Val Loss: 0.0046777
2024-11-11 15:23:42,275 Epoch 325/2000
2024-11-11 15:23:58,454 Current Learning Rate: 0.0069134172
2024-11-11 15:23:58,454 Train Loss: 0.0051051, Val Loss: 0.0048157
2024-11-11 15:23:58,454 Epoch 326/2000
2024-11-11 15:24:13,276 Current Learning Rate: 0.0069857395
2024-11-11 15:24:13,935 Train Loss: 0.0048852, Val Loss: 0.0045308
2024-11-11 15:24:13,935 Epoch 327/2000
2024-11-11 15:24:29,332 Current Learning Rate: 0.0070575718
2024-11-11 15:24:30,019 Train Loss: 0.0048601, Val Loss: 0.0044837
2024-11-11 15:24:30,019 Epoch 328/2000
2024-11-11 15:24:44,791 Current Learning Rate: 0.0071288965
2024-11-11 15:24:44,791 Train Loss: 0.0047553, Val Loss: 0.0045518
2024-11-11 15:24:44,791 Epoch 329/2000
2024-11-11 15:25:01,267 Current Learning Rate: 0.0071996958
2024-11-11 15:25:03,762 Train Loss: 0.0048570, Val Loss: 0.0044149
2024-11-11 15:25:03,762 Epoch 330/2000
2024-11-11 15:25:19,017 Current Learning Rate: 0.0072699525
2024-11-11 15:25:19,018 Train Loss: 0.0048096, Val Loss: 0.0045326
2024-11-11 15:25:19,018 Epoch 331/2000
2024-11-11 15:25:35,225 Current Learning Rate: 0.0073396491
2024-11-11 15:25:35,230 Train Loss: 0.0048643, Val Loss: 0.0046675
2024-11-11 15:25:35,231 Epoch 332/2000
2024-11-11 15:25:51,301 Current Learning Rate: 0.0074087684
2024-11-11 15:25:52,133 Train Loss: 0.0046107, Val Loss: 0.0042613
2024-11-11 15:25:52,134 Epoch 333/2000
2024-11-11 15:26:07,117 Current Learning Rate: 0.0074772933
2024-11-11 15:26:07,118 Train Loss: 0.0049250, Val Loss: 0.0052920
2024-11-11 15:26:07,118 Epoch 334/2000
2024-11-11 15:26:22,818 Current Learning Rate: 0.0075452071
2024-11-11 15:26:22,819 Train Loss: 0.0047885, Val Loss: 0.0043423
2024-11-11 15:26:22,820 Epoch 335/2000
2024-11-11 15:26:39,199 Current Learning Rate: 0.0076124928
2024-11-11 15:26:39,200 Train Loss: 0.0046367, Val Loss: 0.0043337
2024-11-11 15:26:39,200 Epoch 336/2000
2024-11-11 15:26:54,778 Current Learning Rate: 0.0076791340
2024-11-11 15:26:55,840 Train Loss: 0.0043341, Val Loss: 0.0041157
2024-11-11 15:26:55,840 Epoch 337/2000
2024-11-11 15:27:11,824 Current Learning Rate: 0.0077451141
2024-11-11 15:27:11,825 Train Loss: 0.0045052, Val Loss: 0.0042326
2024-11-11 15:27:11,826 Epoch 338/2000
2024-11-11 15:27:27,594 Current Learning Rate: 0.0078104169
2024-11-11 15:27:28,597 Train Loss: 0.0045867, Val Loss: 0.0040947
2024-11-11 15:27:28,597 Epoch 339/2000
2024-11-11 15:27:43,955 Current Learning Rate: 0.0078750263
2024-11-11 15:27:43,956 Train Loss: 0.0042196, Val Loss: 0.0041421
2024-11-11 15:27:43,956 Epoch 340/2000
2024-11-11 15:27:59,980 Current Learning Rate: 0.0079389263
2024-11-11 15:28:00,772 Train Loss: 0.0041555, Val Loss: 0.0039471
2024-11-11 15:28:00,773 Epoch 341/2000
2024-11-11 15:28:15,631 Current Learning Rate: 0.0080021011
2024-11-11 15:28:15,631 Train Loss: 0.0041709, Val Loss: 0.0042655
2024-11-11 15:28:15,631 Epoch 342/2000
2024-11-11 15:28:31,013 Current Learning Rate: 0.0080645353
2024-11-11 15:28:31,014 Train Loss: 0.0045411, Val Loss: 0.0039864
2024-11-11 15:28:31,014 Epoch 343/2000
2024-11-11 15:28:46,956 Current Learning Rate: 0.0081262133
2024-11-11 15:28:46,956 Train Loss: 0.0043177, Val Loss: 0.0039841
2024-11-11 15:28:46,956 Epoch 344/2000
2024-11-11 15:29:02,963 Current Learning Rate: 0.0081871199
2024-11-11 15:29:02,964 Train Loss: 0.0041448, Val Loss: 0.0041350
2024-11-11 15:29:02,964 Epoch 345/2000
2024-11-11 15:29:19,665 Current Learning Rate: 0.0082472402
2024-11-11 15:29:20,507 Train Loss: 0.0041028, Val Loss: 0.0038796
2024-11-11 15:29:20,508 Epoch 346/2000
2024-11-11 15:29:36,213 Current Learning Rate: 0.0083065593
2024-11-11 15:29:36,213 Train Loss: 0.0041301, Val Loss: 0.0040616
2024-11-11 15:29:36,214 Epoch 347/2000
2024-11-11 15:29:52,477 Current Learning Rate: 0.0083650626
2024-11-11 15:29:53,221 Train Loss: 0.0041402, Val Loss: 0.0038527
2024-11-11 15:29:53,221 Epoch 348/2000
2024-11-11 15:30:07,524 Current Learning Rate: 0.0084227355
2024-11-11 15:30:07,524 Train Loss: 0.0040083, Val Loss: 0.0039554
2024-11-11 15:30:07,525 Epoch 349/2000
2024-11-11 15:30:23,061 Current Learning Rate: 0.0084795640
2024-11-11 15:30:23,062 Train Loss: 0.0041215, Val Loss: 0.0038532
2024-11-11 15:30:23,062 Epoch 350/2000
2024-11-11 15:30:40,449 Current Learning Rate: 0.0085355339
2024-11-11 15:30:40,450 Train Loss: 0.0039445, Val Loss: 0.0039694
2024-11-11 15:30:40,450 Epoch 351/2000
2024-11-11 15:30:56,072 Current Learning Rate: 0.0085906315
2024-11-11 15:30:56,872 Train Loss: 0.0041709, Val Loss: 0.0038109
2024-11-11 15:30:56,872 Epoch 352/2000
2024-11-11 15:31:12,185 Current Learning Rate: 0.0086448431
2024-11-11 15:31:12,994 Train Loss: 0.0038993, Val Loss: 0.0037419
2024-11-11 15:31:12,995 Epoch 353/2000
2024-11-11 15:31:28,227 Current Learning Rate: 0.0086981555
2024-11-11 15:31:28,228 Train Loss: 0.0042222, Val Loss: 0.0046172
2024-11-11 15:31:28,228 Epoch 354/2000
2024-11-11 15:31:45,014 Current Learning Rate: 0.0087505553
2024-11-11 15:31:45,015 Train Loss: 0.0041261, Val Loss: 0.0038533
2024-11-11 15:31:45,015 Epoch 355/2000
2024-11-11 15:32:00,327 Current Learning Rate: 0.0088020298
2024-11-11 15:32:01,366 Train Loss: 0.0037649, Val Loss: 0.0036230
2024-11-11 15:32:01,367 Epoch 356/2000
2024-11-11 15:32:16,636 Current Learning Rate: 0.0088525662
2024-11-11 15:32:16,637 Train Loss: 0.0037887, Val Loss: 0.0039364
2024-11-11 15:32:16,637 Epoch 357/2000
2024-11-11 15:32:32,839 Current Learning Rate: 0.0089021520
2024-11-11 15:32:33,660 Train Loss: 0.0037912, Val Loss: 0.0035895
2024-11-11 15:32:33,660 Epoch 358/2000
2024-11-11 15:32:48,544 Current Learning Rate: 0.0089507751
2024-11-11 15:32:49,341 Train Loss: 0.0037223, Val Loss: 0.0034912
2024-11-11 15:32:49,342 Epoch 359/2000
2024-11-11 15:33:04,195 Current Learning Rate: 0.0089984233
2024-11-11 15:33:05,024 Train Loss: 0.0037459, Val Loss: 0.0034155
2024-11-11 15:33:05,025 Epoch 360/2000
2024-11-11 15:33:19,243 Current Learning Rate: 0.0090450850
2024-11-11 15:33:19,243 Train Loss: 0.0038109, Val Loss: 0.0038005
2024-11-11 15:33:19,244 Epoch 361/2000
2024-11-11 15:33:35,441 Current Learning Rate: 0.0090907486
2024-11-11 15:33:35,442 Train Loss: 0.0037431, Val Loss: 0.0034637
2024-11-11 15:33:35,442 Epoch 362/2000
2024-11-11 15:33:52,118 Current Learning Rate: 0.0091354029
2024-11-11 15:33:52,119 Train Loss: 0.0036233, Val Loss: 0.0037645
2024-11-11 15:33:52,119 Epoch 363/2000
2024-11-11 15:34:08,131 Current Learning Rate: 0.0091790368
2024-11-11 15:34:08,132 Train Loss: 0.0035652, Val Loss: 0.0034323
2024-11-11 15:34:08,132 Epoch 364/2000
2024-11-11 15:34:24,458 Current Learning Rate: 0.0092216396
2024-11-11 15:34:25,437 Train Loss: 0.0034464, Val Loss: 0.0032751
2024-11-11 15:34:25,438 Epoch 365/2000
2024-11-11 15:34:40,604 Current Learning Rate: 0.0092632008
2024-11-11 15:34:40,606 Train Loss: 0.0036464, Val Loss: 0.0034118
2024-11-11 15:34:40,606 Epoch 366/2000
2024-11-11 15:34:56,395 Current Learning Rate: 0.0093037101
2024-11-11 15:34:56,395 Train Loss: 0.0034116, Val Loss: 0.0041605
2024-11-11 15:34:56,396 Epoch 367/2000
2024-11-11 15:35:12,169 Current Learning Rate: 0.0093431576
2024-11-11 15:35:13,266 Train Loss: 0.0033963, Val Loss: 0.0032637
2024-11-11 15:35:13,267 Epoch 368/2000
2024-11-11 15:35:28,503 Current Learning Rate: 0.0093815334
2024-11-11 15:35:28,504 Train Loss: 0.0034644, Val Loss: 0.0032756
2024-11-11 15:35:28,504 Epoch 369/2000
2024-11-11 15:35:44,770 Current Learning Rate: 0.0094188282
2024-11-11 15:35:44,771 Train Loss: 0.0032996, Val Loss: 0.0033540
2024-11-11 15:35:44,771 Epoch 370/2000
2024-11-11 15:36:01,298 Current Learning Rate: 0.0094550326
2024-11-11 15:36:01,298 Train Loss: 0.0033698, Val Loss: 0.0033233
2024-11-11 15:36:01,298 Epoch 371/2000
2024-11-11 15:36:16,099 Current Learning Rate: 0.0094901379
2024-11-11 15:36:16,099 Train Loss: 0.0035610, Val Loss: 0.0034147
2024-11-11 15:36:16,099 Epoch 372/2000
2024-11-11 15:36:31,398 Current Learning Rate: 0.0095241353
2024-11-11 15:36:32,187 Train Loss: 0.0035433, Val Loss: 0.0032449
2024-11-11 15:36:32,187 Epoch 373/2000
2024-11-11 15:36:48,188 Current Learning Rate: 0.0095570164
2024-11-11 15:36:48,967 Train Loss: 0.0033498, Val Loss: 0.0032401
2024-11-11 15:36:48,967 Epoch 374/2000
2024-11-11 15:37:04,010 Current Learning Rate: 0.0095887731
2024-11-11 15:37:04,829 Train Loss: 0.0033526, Val Loss: 0.0030753
2024-11-11 15:37:04,829 Epoch 375/2000
2024-11-11 15:37:20,099 Current Learning Rate: 0.0096193977
2024-11-11 15:37:20,100 Train Loss: 0.0032628, Val Loss: 0.0031094
2024-11-11 15:37:20,100 Epoch 376/2000
2024-11-11 15:37:35,793 Current Learning Rate: 0.0096488824
2024-11-11 15:37:36,589 Train Loss: 0.0031918, Val Loss: 0.0030707
2024-11-11 15:37:36,590 Epoch 377/2000
2024-11-11 15:37:51,872 Current Learning Rate: 0.0096772202
2024-11-11 15:37:51,873 Train Loss: 0.0030032, Val Loss: 0.0031136
2024-11-11 15:37:51,873 Epoch 378/2000
2024-11-11 15:38:07,306 Current Learning Rate: 0.0097044038
2024-11-11 15:38:07,306 Train Loss: 0.0031911, Val Loss: 0.0030854
2024-11-11 15:38:07,307 Epoch 379/2000
2024-11-11 15:38:22,613 Current Learning Rate: 0.0097304268
2024-11-11 15:38:22,613 Train Loss: 0.0033952, Val Loss: 0.0031591
2024-11-11 15:38:22,614 Epoch 380/2000
2024-11-11 15:38:38,057 Current Learning Rate: 0.0097552826
2024-11-11 15:38:38,832 Train Loss: 0.0030825, Val Loss: 0.0030423
2024-11-11 15:38:38,833 Epoch 381/2000
2024-11-11 15:38:53,509 Current Learning Rate: 0.0097789651
2024-11-11 15:38:53,510 Train Loss: 0.0032867, Val Loss: 0.0033022
2024-11-11 15:38:53,510 Epoch 382/2000
2024-11-11 15:39:09,111 Current Learning Rate: 0.0098014684
2024-11-11 15:39:09,867 Train Loss: 0.0031884, Val Loss: 0.0029888
2024-11-11 15:39:09,868 Epoch 383/2000
2024-11-11 15:39:24,444 Current Learning Rate: 0.0098227871
2024-11-11 15:39:24,445 Train Loss: 0.0031045, Val Loss: 0.0031278
2024-11-11 15:39:24,445 Epoch 384/2000
2024-11-11 15:39:40,576 Current Learning Rate: 0.0098429158
2024-11-11 15:39:40,577 Train Loss: 0.0031473, Val Loss: 0.0030145
2024-11-11 15:39:40,577 Epoch 385/2000
2024-11-11 15:39:56,625 Current Learning Rate: 0.0098618496
2024-11-11 15:39:57,505 Train Loss: 0.0030393, Val Loss: 0.0029522
2024-11-11 15:39:57,505 Epoch 386/2000
2024-11-11 15:40:13,526 Current Learning Rate: 0.0098795838
2024-11-11 15:40:14,284 Train Loss: 0.0031005, Val Loss: 0.0029025
2024-11-11 15:40:14,284 Epoch 387/2000
2024-11-11 15:40:29,570 Current Learning Rate: 0.0098961141
2024-11-11 15:40:29,571 Train Loss: 0.0029602, Val Loss: 0.0030039
2024-11-11 15:40:29,571 Epoch 388/2000
2024-11-11 15:40:46,762 Current Learning Rate: 0.0099114363
2024-11-11 15:40:47,634 Train Loss: 0.0029413, Val Loss: 0.0028920
2024-11-11 15:40:47,634 Epoch 389/2000
2024-11-11 15:41:03,492 Current Learning Rate: 0.0099255466
2024-11-11 15:41:03,493 Train Loss: 0.0029506, Val Loss: 0.0029535
2024-11-11 15:41:03,493 Epoch 390/2000
2024-11-11 15:41:19,282 Current Learning Rate: 0.0099384417
2024-11-11 15:41:19,992 Train Loss: 0.0029540, Val Loss: 0.0028542
2024-11-11 15:41:19,992 Epoch 391/2000
2024-11-11 15:41:34,271 Current Learning Rate: 0.0099501183
2024-11-11 15:41:35,082 Train Loss: 0.0027288, Val Loss: 0.0027654
2024-11-11 15:41:35,082 Epoch 392/2000
2024-11-11 15:41:50,359 Current Learning Rate: 0.0099605735
2024-11-11 15:41:50,360 Train Loss: 0.0029981, Val Loss: 0.0027909
2024-11-11 15:41:50,360 Epoch 393/2000
2024-11-11 15:42:06,693 Current Learning Rate: 0.0099698048
2024-11-11 15:42:06,693 Train Loss: 0.0028139, Val Loss: 0.0027993
2024-11-11 15:42:06,693 Epoch 394/2000
2024-11-11 15:42:22,873 Current Learning Rate: 0.0099778098
2024-11-11 15:42:23,586 Train Loss: 0.0028165, Val Loss: 0.0027495
2024-11-11 15:42:23,586 Epoch 395/2000
2024-11-11 15:42:38,952 Current Learning Rate: 0.0099845867
2024-11-11 15:42:38,953 Train Loss: 0.0027342, Val Loss: 0.0029531
2024-11-11 15:42:38,953 Epoch 396/2000
2024-11-11 15:42:55,143 Current Learning Rate: 0.0099901336
2024-11-11 15:42:55,144 Train Loss: 0.0028151, Val Loss: 0.0028677
2024-11-11 15:42:55,144 Epoch 397/2000
2024-11-11 15:43:11,346 Current Learning Rate: 0.0099944494
2024-11-11 15:43:11,347 Train Loss: 0.0028728, Val Loss: 0.0028663
2024-11-11 15:43:11,347 Epoch 398/2000
2024-11-11 15:43:27,473 Current Learning Rate: 0.0099975328
2024-11-11 15:43:28,501 Train Loss: 0.0027439, Val Loss: 0.0026992
2024-11-11 15:43:28,502 Epoch 399/2000
2024-11-11 15:43:43,849 Current Learning Rate: 0.0099993832
2024-11-11 15:43:43,850 Train Loss: 0.0029023, Val Loss: 0.0027593
2024-11-11 15:43:43,850 Epoch 400/2000
2024-11-11 15:44:00,709 Current Learning Rate: 0.0100000000
2024-11-11 15:44:00,710 Train Loss: 0.0027569, Val Loss: 0.0027231
2024-11-11 15:44:00,710 Epoch 401/2000
2024-11-11 15:44:17,753 Current Learning Rate: 0.0099993832
2024-11-11 15:44:18,592 Train Loss: 0.0027272, Val Loss: 0.0026476
2024-11-11 15:44:18,593 Epoch 402/2000
2024-11-11 15:44:33,953 Current Learning Rate: 0.0099975328
2024-11-11 15:44:33,954 Train Loss: 0.0026140, Val Loss: 0.0026716
2024-11-11 15:44:33,955 Epoch 403/2000
2024-11-11 15:44:51,240 Current Learning Rate: 0.0099944494
2024-11-11 15:44:52,250 Train Loss: 0.0025904, Val Loss: 0.0026408
2024-11-11 15:44:52,251 Epoch 404/2000
2024-11-11 15:45:07,899 Current Learning Rate: 0.0099901336
2024-11-11 15:45:07,900 Train Loss: 0.0025123, Val Loss: 0.0027146
2024-11-11 15:45:07,900 Epoch 405/2000
2024-11-11 15:45:24,531 Current Learning Rate: 0.0099845867
2024-11-11 15:45:24,532 Train Loss: 0.0027804, Val Loss: 0.0026874
2024-11-11 15:45:24,532 Epoch 406/2000
2024-11-11 15:45:39,425 Current Learning Rate: 0.0099778098
2024-11-11 15:45:40,239 Train Loss: 0.0026509, Val Loss: 0.0026173
2024-11-11 15:45:40,239 Epoch 407/2000
2024-11-11 15:45:56,723 Current Learning Rate: 0.0099698048
2024-11-11 15:45:57,423 Train Loss: 0.0026102, Val Loss: 0.0025745
2024-11-11 15:45:57,423 Epoch 408/2000
2024-11-11 15:46:11,774 Current Learning Rate: 0.0099605735
2024-11-11 15:46:11,775 Train Loss: 0.0026633, Val Loss: 0.0026304
2024-11-11 15:46:11,775 Epoch 409/2000
2024-11-11 15:46:27,259 Current Learning Rate: 0.0099501183
2024-11-11 15:46:28,016 Train Loss: 0.0027500, Val Loss: 0.0025415
2024-11-11 15:46:28,017 Epoch 410/2000
2024-11-11 15:46:42,588 Current Learning Rate: 0.0099384417
2024-11-11 15:46:42,589 Train Loss: 0.0025426, Val Loss: 0.0025611
2024-11-11 15:46:42,589 Epoch 411/2000
2024-11-11 15:46:58,088 Current Learning Rate: 0.0099255466
2024-11-11 15:46:58,089 Train Loss: 0.0025220, Val Loss: 0.0025420
2024-11-11 15:46:58,089 Epoch 412/2000
2024-11-11 15:47:13,169 Current Learning Rate: 0.0099114363
2024-11-11 15:47:13,169 Train Loss: 0.0027022, Val Loss: 0.0026206
2024-11-11 15:47:13,169 Epoch 413/2000
2024-11-11 15:47:29,396 Current Learning Rate: 0.0098961141
2024-11-11 15:47:30,431 Train Loss: 0.0024756, Val Loss: 0.0024991
2024-11-11 15:47:30,432 Epoch 414/2000
2024-11-11 15:47:46,902 Current Learning Rate: 0.0098795838
2024-11-11 15:47:47,977 Train Loss: 0.0023801, Val Loss: 0.0024905
2024-11-11 15:47:47,978 Epoch 415/2000
2024-11-11 15:48:04,159 Current Learning Rate: 0.0098618496
2024-11-11 15:48:04,162 Train Loss: 0.0023671, Val Loss: 0.0025034
2024-11-11 15:48:04,162 Epoch 416/2000
2024-11-11 15:48:19,640 Current Learning Rate: 0.0098429158
2024-11-11 15:48:19,641 Train Loss: 0.0026003, Val Loss: 0.0025255
2024-11-11 15:48:19,641 Epoch 417/2000
2024-11-11 15:48:34,829 Current Learning Rate: 0.0098227871
2024-11-11 15:48:35,666 Train Loss: 0.0024705, Val Loss: 0.0024665
2024-11-11 15:48:35,666 Epoch 418/2000
2024-11-11 15:48:50,252 Current Learning Rate: 0.0098014684
2024-11-11 15:48:50,998 Train Loss: 0.0023990, Val Loss: 0.0024348
2024-11-11 15:48:50,998 Epoch 419/2000
2024-11-11 15:49:05,824 Current Learning Rate: 0.0097789651
2024-11-11 15:49:06,610 Train Loss: 0.0024215, Val Loss: 0.0023730
2024-11-11 15:49:06,610 Epoch 420/2000
2024-11-11 15:49:21,374 Current Learning Rate: 0.0097552826
2024-11-11 15:49:21,375 Train Loss: 0.0023986, Val Loss: 0.0025001
2024-11-11 15:49:21,375 Epoch 421/2000
2024-11-11 15:49:36,450 Current Learning Rate: 0.0097304268
2024-11-11 15:49:36,450 Train Loss: 0.0024147, Val Loss: 0.0024293
2024-11-11 15:49:36,451 Epoch 422/2000
2024-11-11 15:49:52,287 Current Learning Rate: 0.0097044038
2024-11-11 15:49:52,288 Train Loss: 0.0026629, Val Loss: 0.0026045
2024-11-11 15:49:52,288 Epoch 423/2000
2024-11-11 15:50:08,273 Current Learning Rate: 0.0096772202
2024-11-11 15:50:08,274 Train Loss: 0.0024435, Val Loss: 0.0026035
2024-11-11 15:50:08,274 Epoch 424/2000
2024-11-11 15:50:24,311 Current Learning Rate: 0.0096488824
2024-11-11 15:50:24,312 Train Loss: 0.0023706, Val Loss: 0.0023908
2024-11-11 15:50:24,313 Epoch 425/2000
2024-11-11 15:50:39,933 Current Learning Rate: 0.0096193977
2024-11-11 15:50:39,934 Train Loss: 0.0023457, Val Loss: 0.0023849
2024-11-11 15:50:39,934 Epoch 426/2000
2024-11-11 15:50:55,838 Current Learning Rate: 0.0095887731
2024-11-11 15:50:55,839 Train Loss: 0.0025702, Val Loss: 0.0023846
2024-11-11 15:50:55,840 Epoch 427/2000
2024-11-11 15:51:11,786 Current Learning Rate: 0.0095570164
2024-11-11 15:51:12,601 Train Loss: 0.0022596, Val Loss: 0.0022983
2024-11-11 15:51:12,601 Epoch 428/2000
2024-11-11 15:51:27,757 Current Learning Rate: 0.0095241353
2024-11-11 15:51:27,758 Train Loss: 0.0024092, Val Loss: 0.0023213
2024-11-11 15:51:27,759 Epoch 429/2000
2024-11-11 15:51:44,023 Current Learning Rate: 0.0094901379
2024-11-11 15:51:44,024 Train Loss: 0.0023035, Val Loss: 0.0023651
2024-11-11 15:51:44,024 Epoch 430/2000
2024-11-11 15:51:59,926 Current Learning Rate: 0.0094550326
2024-11-11 15:51:59,927 Train Loss: 0.0022316, Val Loss: 0.0023187
2024-11-11 15:51:59,927 Epoch 431/2000
2024-11-11 15:52:16,328 Current Learning Rate: 0.0094188282
2024-11-11 15:52:17,345 Train Loss: 0.0022601, Val Loss: 0.0022593
2024-11-11 15:52:17,345 Epoch 432/2000
2024-11-11 15:52:33,331 Current Learning Rate: 0.0093815334
2024-11-11 15:52:33,332 Train Loss: 0.0023575, Val Loss: 0.0023146
2024-11-11 15:52:33,332 Epoch 433/2000
2024-11-11 15:52:49,105 Current Learning Rate: 0.0093431576
2024-11-11 15:52:49,106 Train Loss: 0.0022776, Val Loss: 0.0023683
2024-11-11 15:52:49,107 Epoch 434/2000
2024-11-11 15:53:04,540 Current Learning Rate: 0.0093037101
2024-11-11 15:53:04,540 Train Loss: 0.0021982, Val Loss: 0.0023392
2024-11-11 15:53:04,541 Epoch 435/2000
2024-11-11 15:53:19,733 Current Learning Rate: 0.0092632008
2024-11-11 15:53:19,734 Train Loss: 0.0022392, Val Loss: 0.0023363
2024-11-11 15:53:19,734 Epoch 436/2000
2024-11-11 15:53:35,784 Current Learning Rate: 0.0092216396
2024-11-11 15:53:36,549 Train Loss: 0.0021519, Val Loss: 0.0022499
2024-11-11 15:53:36,550 Epoch 437/2000
2024-11-11 15:53:51,659 Current Learning Rate: 0.0091790368
2024-11-11 15:53:52,652 Train Loss: 0.0021430, Val Loss: 0.0022377
2024-11-11 15:53:52,652 Epoch 438/2000
2024-11-11 15:54:08,774 Current Learning Rate: 0.0091354029
2024-11-11 15:54:08,775 Train Loss: 0.0023445, Val Loss: 0.0023493
2024-11-11 15:54:08,775 Epoch 439/2000
2024-11-11 15:54:24,482 Current Learning Rate: 0.0090907486
2024-11-11 15:54:25,521 Train Loss: 0.0023121, Val Loss: 0.0022053
2024-11-11 15:54:25,521 Epoch 440/2000
2024-11-11 15:54:41,306 Current Learning Rate: 0.0090450850
2024-11-11 15:54:42,079 Train Loss: 0.0023536, Val Loss: 0.0021599
2024-11-11 15:54:42,079 Epoch 441/2000
2024-11-11 15:54:57,043 Current Learning Rate: 0.0089984233
2024-11-11 15:54:57,956 Train Loss: 0.0019961, Val Loss: 0.0020880
2024-11-11 15:54:57,957 Epoch 442/2000
2024-11-11 15:55:13,785 Current Learning Rate: 0.0089507751
2024-11-11 15:55:13,786 Train Loss: 0.0020018, Val Loss: 0.0021359
2024-11-11 15:55:13,786 Epoch 443/2000
2024-11-11 15:55:30,641 Current Learning Rate: 0.0089021520
2024-11-11 15:55:30,642 Train Loss: 0.0023603, Val Loss: 0.0021651
2024-11-11 15:55:30,643 Epoch 444/2000
2024-11-11 15:55:45,897 Current Learning Rate: 0.0088525662
2024-11-11 15:55:45,898 Train Loss: 0.0021467, Val Loss: 0.0021570
2024-11-11 15:55:45,898 Epoch 445/2000
2024-11-11 15:56:02,691 Current Learning Rate: 0.0088020298
2024-11-11 15:56:02,692 Train Loss: 0.0021638, Val Loss: 0.0022031
2024-11-11 15:56:02,692 Epoch 446/2000
2024-11-11 15:56:18,638 Current Learning Rate: 0.0087505553
2024-11-11 15:56:18,639 Train Loss: 0.0019990, Val Loss: 0.0021428
2024-11-11 15:56:18,639 Epoch 447/2000
2024-11-11 15:56:34,779 Current Learning Rate: 0.0086981555
2024-11-11 15:56:34,780 Train Loss: 0.0021922, Val Loss: 0.0022111
2024-11-11 15:56:34,780 Epoch 448/2000
2024-11-11 15:56:51,500 Current Learning Rate: 0.0086448431
2024-11-11 15:56:51,501 Train Loss: 0.0019944, Val Loss: 0.0020895
2024-11-11 15:56:51,501 Epoch 449/2000
2024-11-11 15:57:08,394 Current Learning Rate: 0.0085906315
2024-11-11 15:57:08,395 Train Loss: 0.0020744, Val Loss: 0.0021320
2024-11-11 15:57:08,395 Epoch 450/2000
2024-11-11 15:57:24,634 Current Learning Rate: 0.0085355339
2024-11-11 15:57:24,635 Train Loss: 0.0021858, Val Loss: 0.0022044
2024-11-11 15:57:24,636 Epoch 451/2000
2024-11-11 15:57:40,688 Current Learning Rate: 0.0084795640
2024-11-11 15:57:41,618 Train Loss: 0.0021739, Val Loss: 0.0020811
2024-11-11 15:57:41,618 Epoch 452/2000
2024-11-11 15:57:57,554 Current Learning Rate: 0.0084227355
2024-11-11 15:57:58,361 Train Loss: 0.0020215, Val Loss: 0.0020513
2024-11-11 15:57:58,361 Epoch 453/2000
2024-11-11 15:58:13,493 Current Learning Rate: 0.0083650626
2024-11-11 15:58:13,494 Train Loss: 0.0021197, Val Loss: 0.0020636
2024-11-11 15:58:13,494 Epoch 454/2000
2024-11-11 15:58:29,027 Current Learning Rate: 0.0083065593
2024-11-11 15:58:29,027 Train Loss: 0.0020417, Val Loss: 0.0020677
2024-11-11 15:58:29,028 Epoch 455/2000
2024-11-11 15:58:44,414 Current Learning Rate: 0.0082472402
2024-11-11 15:58:44,415 Train Loss: 0.0020983, Val Loss: 0.0021948
2024-11-11 15:58:44,415 Epoch 456/2000
2024-11-11 15:59:00,094 Current Learning Rate: 0.0081871199
2024-11-11 15:59:00,095 Train Loss: 0.0019864, Val Loss: 0.0020734
2024-11-11 15:59:00,095 Epoch 457/2000
2024-11-11 15:59:15,407 Current Learning Rate: 0.0081262133
2024-11-11 15:59:15,407 Train Loss: 0.0020466, Val Loss: 0.0020746
2024-11-11 15:59:15,407 Epoch 458/2000
2024-11-11 15:59:30,863 Current Learning Rate: 0.0080645353
2024-11-11 15:59:30,864 Train Loss: 0.0020444, Val Loss: 0.0021021
2024-11-11 15:59:30,864 Epoch 459/2000
2024-11-11 15:59:46,313 Current Learning Rate: 0.0080021011
2024-11-11 15:59:46,313 Train Loss: 0.0021171, Val Loss: 0.0024517
2024-11-11 15:59:46,313 Epoch 460/2000
2024-11-11 16:00:01,747 Current Learning Rate: 0.0079389263
2024-11-11 16:00:04,353 Train Loss: 0.0019752, Val Loss: 0.0020446
2024-11-11 16:00:04,354 Epoch 461/2000
2024-11-11 16:00:18,705 Current Learning Rate: 0.0078750263
2024-11-11 16:00:19,504 Train Loss: 0.0020459, Val Loss: 0.0020055
2024-11-11 16:00:19,505 Epoch 462/2000
2024-11-11 16:00:34,124 Current Learning Rate: 0.0078104169
2024-11-11 16:00:34,913 Train Loss: 0.0021494, Val Loss: 0.0020054
2024-11-11 16:00:34,913 Epoch 463/2000
2024-11-11 16:00:49,508 Current Learning Rate: 0.0077451141
2024-11-11 16:00:50,383 Train Loss: 0.0018689, Val Loss: 0.0019831
2024-11-11 16:00:50,383 Epoch 464/2000
2024-11-11 16:01:05,465 Current Learning Rate: 0.0076791340
2024-11-11 16:01:06,264 Train Loss: 0.0019285, Val Loss: 0.0019549
2024-11-11 16:01:06,264 Epoch 465/2000
2024-11-11 16:01:21,254 Current Learning Rate: 0.0076124928
2024-11-11 16:01:21,254 Train Loss: 0.0019481, Val Loss: 0.0021419
2024-11-11 16:01:21,254 Epoch 466/2000
2024-11-11 16:01:36,962 Current Learning Rate: 0.0075452071
2024-11-11 16:01:36,963 Train Loss: 0.0019473, Val Loss: 0.0019738
2024-11-11 16:01:36,963 Epoch 467/2000
2024-11-11 16:01:52,832 Current Learning Rate: 0.0074772933
2024-11-11 16:01:52,833 Train Loss: 0.0017876, Val Loss: 0.0019702
2024-11-11 16:01:52,833 Epoch 468/2000
2024-11-11 16:02:09,078 Current Learning Rate: 0.0074087684
2024-11-11 16:02:09,079 Train Loss: 0.0018833, Val Loss: 0.0020495
2024-11-11 16:02:09,079 Epoch 469/2000
2024-11-11 16:02:25,453 Current Learning Rate: 0.0073396491
2024-11-11 16:02:26,538 Train Loss: 0.0019062, Val Loss: 0.0019353
2024-11-11 16:02:26,539 Epoch 470/2000
2024-11-11 16:02:42,158 Current Learning Rate: 0.0072699525
2024-11-11 16:02:42,158 Train Loss: 0.0019746, Val Loss: 0.0020509
2024-11-11 16:02:42,159 Epoch 471/2000
2024-11-11 16:02:58,381 Current Learning Rate: 0.0071996958
2024-11-11 16:02:59,207 Train Loss: 0.0021009, Val Loss: 0.0019330
2024-11-11 16:02:59,208 Epoch 472/2000
2024-11-11 16:03:14,271 Current Learning Rate: 0.0071288965
2024-11-11 16:03:14,272 Train Loss: 0.0019569, Val Loss: 0.0019589
2024-11-11 16:03:14,273 Epoch 473/2000
2024-11-11 16:03:31,044 Current Learning Rate: 0.0070575718
2024-11-11 16:03:31,044 Train Loss: 0.0019271, Val Loss: 0.0019746
2024-11-11 16:03:31,045 Epoch 474/2000
2024-11-11 16:03:46,930 Current Learning Rate: 0.0069857395
2024-11-11 16:03:47,804 Train Loss: 0.0018458, Val Loss: 0.0019274
2024-11-11 16:03:47,804 Epoch 475/2000
2024-11-11 16:04:03,052 Current Learning Rate: 0.0069134172
2024-11-11 16:04:04,410 Train Loss: 0.0018403, Val Loss: 0.0018845
2024-11-11 16:04:04,410 Epoch 476/2000
2024-11-11 16:04:19,964 Current Learning Rate: 0.0068406228
2024-11-11 16:04:19,965 Train Loss: 0.0017707, Val Loss: 0.0018862
2024-11-11 16:04:19,965 Epoch 477/2000
2024-11-11 16:04:35,230 Current Learning Rate: 0.0067673742
2024-11-11 16:04:36,004 Train Loss: 0.0017440, Val Loss: 0.0018785
2024-11-11 16:04:36,004 Epoch 478/2000
2024-11-11 16:04:51,096 Current Learning Rate: 0.0066936896
2024-11-11 16:04:51,097 Train Loss: 0.0019223, Val Loss: 0.0019294
2024-11-11 16:04:51,098 Epoch 479/2000
2024-11-11 16:05:06,516 Current Learning Rate: 0.0066195871
2024-11-11 16:05:07,498 Train Loss: 0.0018153, Val Loss: 0.0018524
2024-11-11 16:05:07,498 Epoch 480/2000
2024-11-11 16:05:23,821 Current Learning Rate: 0.0065450850
2024-11-11 16:05:23,822 Train Loss: 0.0018126, Val Loss: 0.0018859
2024-11-11 16:05:23,822 Epoch 481/2000
2024-11-11 16:05:40,373 Current Learning Rate: 0.0064702016
2024-11-11 16:05:40,373 Train Loss: 0.0017999, Val Loss: 0.0018556
2024-11-11 16:05:40,373 Epoch 482/2000
2024-11-11 16:05:56,273 Current Learning Rate: 0.0063949555
2024-11-11 16:05:57,308 Train Loss: 0.0016655, Val Loss: 0.0018388
2024-11-11 16:05:57,308 Epoch 483/2000
2024-11-11 16:06:12,362 Current Learning Rate: 0.0063193652
2024-11-11 16:06:12,363 Train Loss: 0.0019307, Val Loss: 0.0019162
2024-11-11 16:06:12,363 Epoch 484/2000
2024-11-11 16:06:27,663 Current Learning Rate: 0.0062434494
2024-11-11 16:06:28,418 Train Loss: 0.0017061, Val Loss: 0.0018284
2024-11-11 16:06:28,419 Epoch 485/2000
2024-11-11 16:06:43,503 Current Learning Rate: 0.0061672268
2024-11-11 16:06:43,504 Train Loss: 0.0017286, Val Loss: 0.0018455
2024-11-11 16:06:43,504 Epoch 486/2000
2024-11-11 16:06:59,204 Current Learning Rate: 0.0060907162
2024-11-11 16:06:59,205 Train Loss: 0.0019324, Val Loss: 0.0019866
2024-11-11 16:06:59,205 Epoch 487/2000
2024-11-11 16:07:14,745 Current Learning Rate: 0.0060139365
2024-11-11 16:07:15,568 Train Loss: 0.0017491, Val Loss: 0.0018149
2024-11-11 16:07:15,568 Epoch 488/2000
2024-11-11 16:07:29,955 Current Learning Rate: 0.0059369066
2024-11-11 16:07:29,957 Train Loss: 0.0019003, Val Loss: 0.0019061
2024-11-11 16:07:29,957 Epoch 489/2000
2024-11-11 16:07:45,685 Current Learning Rate: 0.0058596455
2024-11-11 16:07:45,685 Train Loss: 0.0018472, Val Loss: 0.0018604
2024-11-11 16:07:45,685 Epoch 490/2000
2024-11-11 16:08:01,170 Current Learning Rate: 0.0057821723
2024-11-11 16:08:01,867 Train Loss: 0.0017701, Val Loss: 0.0017785
2024-11-11 16:08:01,868 Epoch 491/2000
2024-11-11 16:08:16,907 Current Learning Rate: 0.0057045062
2024-11-11 16:08:16,908 Train Loss: 0.0018862, Val Loss: 0.0018500
2024-11-11 16:08:16,908 Epoch 492/2000
2024-11-11 16:08:32,207 Current Learning Rate: 0.0056266662
2024-11-11 16:08:32,207 Train Loss: 0.0019619, Val Loss: 0.0018138
2024-11-11 16:08:32,207 Epoch 493/2000
2024-11-11 16:08:47,512 Current Learning Rate: 0.0055486716
2024-11-11 16:08:47,513 Train Loss: 0.0018586, Val Loss: 0.0018043
2024-11-11 16:08:47,513 Epoch 494/2000
2024-11-11 16:09:03,379 Current Learning Rate: 0.0054705416
2024-11-11 16:09:03,379 Train Loss: 0.0018664, Val Loss: 0.0018559
2024-11-11 16:09:03,380 Epoch 495/2000
2024-11-11 16:09:19,344 Current Learning Rate: 0.0053922955
2024-11-11 16:09:20,380 Train Loss: 0.0019069, Val Loss: 0.0017716
2024-11-11 16:09:20,380 Epoch 496/2000
2024-11-11 16:09:36,318 Current Learning Rate: 0.0053139526
2024-11-11 16:09:37,269 Train Loss: 0.0015875, Val Loss: 0.0017484
2024-11-11 16:09:37,269 Epoch 497/2000
2024-11-11 16:09:52,623 Current Learning Rate: 0.0052355323
2024-11-11 16:09:52,624 Train Loss: 0.0017934, Val Loss: 0.0018129
2024-11-11 16:09:52,624 Epoch 498/2000
2024-11-11 16:10:08,125 Current Learning Rate: 0.0051570538
2024-11-11 16:10:08,125 Train Loss: 0.0017281, Val Loss: 0.0017944
2024-11-11 16:10:08,125 Epoch 499/2000
2024-11-11 16:10:23,982 Current Learning Rate: 0.0050785366
2024-11-11 16:10:23,982 Train Loss: 0.0017292, Val Loss: 0.0017568
2024-11-11 16:10:23,983 Epoch 500/2000
2024-11-11 16:10:40,212 Current Learning Rate: 0.0050000000
2024-11-11 16:10:41,256 Train Loss: 0.0015728, Val Loss: 0.0017277
2024-11-11 16:10:41,256 Epoch 501/2000
2024-11-11 16:10:57,505 Current Learning Rate: 0.0049214634
2024-11-11 16:10:57,506 Train Loss: 0.0016176, Val Loss: 0.0017323
2024-11-11 16:10:57,507 Epoch 502/2000
2024-11-11 16:11:13,288 Current Learning Rate: 0.0048429462
2024-11-11 16:11:13,289 Train Loss: 0.0016708, Val Loss: 0.0017368
2024-11-11 16:11:13,289 Epoch 503/2000
2024-11-11 16:11:28,709 Current Learning Rate: 0.0047644677
2024-11-11 16:11:29,408 Train Loss: 0.0017153, Val Loss: 0.0017276
2024-11-11 16:11:29,408 Epoch 504/2000
2024-11-11 16:11:43,653 Current Learning Rate: 0.0046860474
2024-11-11 16:11:44,407 Train Loss: 0.0016124, Val Loss: 0.0017100
2024-11-11 16:11:44,408 Epoch 505/2000
2024-11-11 16:11:59,632 Current Learning Rate: 0.0046077045
2024-11-11 16:12:00,691 Train Loss: 0.0015919, Val Loss: 0.0017059
2024-11-11 16:12:00,692 Epoch 506/2000
2024-11-11 16:12:15,848 Current Learning Rate: 0.0045294584
2024-11-11 16:12:15,848 Train Loss: 0.0017435, Val Loss: 0.0017152
2024-11-11 16:12:15,849 Epoch 507/2000
2024-11-11 16:12:31,233 Current Learning Rate: 0.0044513284
2024-11-11 16:12:31,233 Train Loss: 0.0015542, Val Loss: 0.0017474
2024-11-11 16:12:31,233 Epoch 508/2000
2024-11-11 16:12:47,634 Current Learning Rate: 0.0043733338
2024-11-11 16:12:47,634 Train Loss: 0.0017828, Val Loss: 0.0017657
2024-11-11 16:12:47,634 Epoch 509/2000
2024-11-11 16:13:03,377 Current Learning Rate: 0.0042954938
2024-11-11 16:13:05,795 Train Loss: 0.0016038, Val Loss: 0.0016854
2024-11-11 16:13:05,795 Epoch 510/2000
2024-11-11 16:13:20,775 Current Learning Rate: 0.0042178277
2024-11-11 16:13:21,841 Train Loss: 0.0016788, Val Loss: 0.0016779
2024-11-11 16:13:21,842 Epoch 511/2000
2024-11-11 16:13:37,672 Current Learning Rate: 0.0041403545
2024-11-11 16:13:38,760 Train Loss: 0.0016675, Val Loss: 0.0016651
2024-11-11 16:13:38,760 Epoch 512/2000
2024-11-11 16:13:54,901 Current Learning Rate: 0.0040630934
2024-11-11 16:13:54,902 Train Loss: 0.0017742, Val Loss: 0.0016794
2024-11-11 16:13:54,902 Epoch 513/2000
2024-11-11 16:14:10,716 Current Learning Rate: 0.0039860635
2024-11-11 16:14:11,598 Train Loss: 0.0015026, Val Loss: 0.0016478
2024-11-11 16:14:11,598 Epoch 514/2000
2024-11-11 16:14:26,196 Current Learning Rate: 0.0039092838
2024-11-11 16:14:26,197 Train Loss: 0.0016885, Val Loss: 0.0017014
2024-11-11 16:14:26,197 Epoch 515/2000
2024-11-11 16:14:41,627 Current Learning Rate: 0.0038327732
2024-11-11 16:14:41,627 Train Loss: 0.0017836, Val Loss: 0.0018343
2024-11-11 16:14:41,627 Epoch 516/2000
2024-11-11 16:14:57,435 Current Learning Rate: 0.0037565506
2024-11-11 16:14:57,435 Train Loss: 0.0015580, Val Loss: 0.0016799
2024-11-11 16:14:57,435 Epoch 517/2000
2024-11-11 16:15:12,908 Current Learning Rate: 0.0036806348
2024-11-11 16:15:12,908 Train Loss: 0.0015340, Val Loss: 0.0016495
2024-11-11 16:15:12,908 Epoch 518/2000
2024-11-11 16:15:28,391 Current Learning Rate: 0.0036050445
2024-11-11 16:15:28,392 Train Loss: 0.0015838, Val Loss: 0.0016501
2024-11-11 16:15:28,392 Epoch 519/2000
2024-11-11 16:15:44,020 Current Learning Rate: 0.0035297984
2024-11-11 16:15:44,787 Train Loss: 0.0015158, Val Loss: 0.0016460
2024-11-11 16:15:44,788 Epoch 520/2000
2024-11-11 16:15:59,636 Current Learning Rate: 0.0034549150
2024-11-11 16:15:59,637 Train Loss: 0.0016864, Val Loss: 0.0016732
2024-11-11 16:15:59,637 Epoch 521/2000
2024-11-11 16:16:16,119 Current Learning Rate: 0.0033804129
2024-11-11 16:16:16,119 Train Loss: 0.0015563, Val Loss: 0.0016919
2024-11-11 16:16:16,120 Epoch 522/2000
2024-11-11 16:16:31,722 Current Learning Rate: 0.0033063104
2024-11-11 16:16:31,722 Train Loss: 0.0015584, Val Loss: 0.0016514
2024-11-11 16:16:31,722 Epoch 523/2000
2024-11-11 16:16:48,232 Current Learning Rate: 0.0032326258
2024-11-11 16:16:48,233 Train Loss: 0.0014985, Val Loss: 0.0016575
2024-11-11 16:16:48,233 Epoch 524/2000
2024-11-11 16:17:04,498 Current Learning Rate: 0.0031593772
2024-11-11 16:17:04,498 Train Loss: 0.0015001, Val Loss: 0.0016723
2024-11-11 16:17:04,498 Epoch 525/2000
2024-11-11 16:17:19,873 Current Learning Rate: 0.0030865828
2024-11-11 16:17:20,731 Train Loss: 0.0015969, Val Loss: 0.0016347
2024-11-11 16:17:20,731 Epoch 526/2000
2024-11-11 16:17:35,853 Current Learning Rate: 0.0030142605
2024-11-11 16:17:36,882 Train Loss: 0.0015379, Val Loss: 0.0016232
2024-11-11 16:17:36,883 Epoch 527/2000
2024-11-11 16:17:52,338 Current Learning Rate: 0.0029424282
2024-11-11 16:17:53,220 Train Loss: 0.0014947, Val Loss: 0.0016079
2024-11-11 16:17:53,220 Epoch 528/2000
2024-11-11 16:18:08,765 Current Learning Rate: 0.0028711035
2024-11-11 16:18:08,767 Train Loss: 0.0017007, Val Loss: 0.0016654
2024-11-11 16:18:08,767 Epoch 529/2000
2024-11-11 16:18:25,173 Current Learning Rate: 0.0028003042
2024-11-11 16:18:25,174 Train Loss: 0.0015099, Val Loss: 0.0016081
2024-11-11 16:18:25,174 Epoch 530/2000
2024-11-11 16:18:40,740 Current Learning Rate: 0.0027300475
2024-11-11 16:18:41,493 Train Loss: 0.0015892, Val Loss: 0.0015988
2024-11-11 16:18:41,493 Epoch 531/2000
2024-11-11 16:18:56,898 Current Learning Rate: 0.0026603509
2024-11-11 16:18:57,683 Train Loss: 0.0014729, Val Loss: 0.0015968
2024-11-11 16:18:57,684 Epoch 532/2000
2024-11-11 16:19:13,202 Current Learning Rate: 0.0025912316
2024-11-11 16:19:13,203 Train Loss: 0.0015314, Val Loss: 0.0015976
2024-11-11 16:19:13,203 Epoch 533/2000
2024-11-11 16:19:29,360 Current Learning Rate: 0.0025227067
2024-11-11 16:19:30,177 Train Loss: 0.0014599, Val Loss: 0.0015746
2024-11-11 16:19:30,177 Epoch 534/2000
2024-11-11 16:19:45,423 Current Learning Rate: 0.0024547929
2024-11-11 16:19:45,424 Train Loss: 0.0015706, Val Loss: 0.0015865
2024-11-11 16:19:45,424 Epoch 535/2000
2024-11-11 16:20:00,908 Current Learning Rate: 0.0023875072
2024-11-11 16:20:00,909 Train Loss: 0.0015245, Val Loss: 0.0015782
2024-11-11 16:20:00,909 Epoch 536/2000
2024-11-11 16:20:16,538 Current Learning Rate: 0.0023208660
2024-11-11 16:20:17,281 Train Loss: 0.0014032, Val Loss: 0.0015686
2024-11-11 16:20:17,281 Epoch 537/2000
2024-11-11 16:20:32,061 Current Learning Rate: 0.0022548859
2024-11-11 16:20:32,062 Train Loss: 0.0015010, Val Loss: 0.0015706
2024-11-11 16:20:32,062 Epoch 538/2000
2024-11-11 16:20:47,831 Current Learning Rate: 0.0021895831
2024-11-11 16:20:47,832 Train Loss: 0.0014061, Val Loss: 0.0015709
2024-11-11 16:20:47,832 Epoch 539/2000
2024-11-11 16:21:04,394 Current Learning Rate: 0.0021249737
2024-11-11 16:21:04,394 Train Loss: 0.0014642, Val Loss: 0.0015716
2024-11-11 16:21:04,395 Epoch 540/2000
2024-11-11 16:21:20,644 Current Learning Rate: 0.0020610737
2024-11-11 16:21:20,644 Train Loss: 0.0015021, Val Loss: 0.0016078
2024-11-11 16:21:20,645 Epoch 541/2000
2024-11-11 16:21:36,011 Current Learning Rate: 0.0019978989
2024-11-11 16:21:36,011 Train Loss: 0.0014069, Val Loss: 0.0015928
2024-11-11 16:21:36,012 Epoch 542/2000
2024-11-11 16:21:52,204 Current Learning Rate: 0.0019354647
2024-11-11 16:21:52,936 Train Loss: 0.0015883, Val Loss: 0.0015588
2024-11-11 16:21:52,936 Epoch 543/2000
2024-11-11 16:22:07,769 Current Learning Rate: 0.0018737867
2024-11-11 16:22:07,770 Train Loss: 0.0014828, Val Loss: 0.0015647
2024-11-11 16:22:07,770 Epoch 544/2000
2024-11-11 16:22:24,511 Current Learning Rate: 0.0018128801
2024-11-11 16:22:24,511 Train Loss: 0.0014293, Val Loss: 0.0015666
2024-11-11 16:22:24,511 Epoch 545/2000
2024-11-11 16:22:40,551 Current Learning Rate: 0.0017527598
2024-11-11 16:22:41,294 Train Loss: 0.0013610, Val Loss: 0.0015508
2024-11-11 16:22:41,294 Epoch 546/2000
2024-11-11 16:22:57,430 Current Learning Rate: 0.0016934407
2024-11-11 16:22:58,195 Train Loss: 0.0016783, Val Loss: 0.0015387
2024-11-11 16:22:58,195 Epoch 547/2000
2024-11-11 16:23:13,081 Current Learning Rate: 0.0016349374
2024-11-11 16:23:13,082 Train Loss: 0.0014395, Val Loss: 0.0015399
2024-11-11 16:23:13,082 Epoch 548/2000
2024-11-11 16:23:29,144 Current Learning Rate: 0.0015772645
2024-11-11 16:23:29,144 Train Loss: 0.0015553, Val Loss: 0.0015414
2024-11-11 16:23:29,144 Epoch 549/2000
2024-11-11 16:23:45,321 Current Learning Rate: 0.0015204360
2024-11-11 16:23:46,052 Train Loss: 0.0015024, Val Loss: 0.0015371
2024-11-11 16:23:46,052 Epoch 550/2000
2024-11-11 16:24:01,416 Current Learning Rate: 0.0014644661
2024-11-11 16:24:05,218 Train Loss: 0.0014474, Val Loss: 0.0015369
2024-11-11 16:24:05,219 Epoch 551/2000
2024-11-11 16:24:19,288 Current Learning Rate: 0.0014093685
2024-11-11 16:24:19,289 Train Loss: 0.0013989, Val Loss: 0.0015375
2024-11-11 16:24:19,290 Epoch 552/2000
2024-11-11 16:24:34,434 Current Learning Rate: 0.0013551569
2024-11-11 16:24:35,144 Train Loss: 0.0014447, Val Loss: 0.0015220
2024-11-11 16:24:35,145 Epoch 553/2000
2024-11-11 16:24:49,936 Current Learning Rate: 0.0013018445
2024-11-11 16:24:50,707 Train Loss: 0.0014534, Val Loss: 0.0015164
2024-11-11 16:24:50,707 Epoch 554/2000
2024-11-11 16:25:05,563 Current Learning Rate: 0.0012494447
2024-11-11 16:25:06,451 Train Loss: 0.0013869, Val Loss: 0.0015128
2024-11-11 16:25:06,451 Epoch 555/2000
2024-11-11 16:25:21,363 Current Learning Rate: 0.0011979702
2024-11-11 16:25:22,177 Train Loss: 0.0013326, Val Loss: 0.0015067
2024-11-11 16:25:22,177 Epoch 556/2000
2024-11-11 16:25:37,131 Current Learning Rate: 0.0011474338
2024-11-11 16:25:37,132 Train Loss: 0.0013669, Val Loss: 0.0015067
2024-11-11 16:25:37,132 Epoch 557/2000
2024-11-11 16:25:52,878 Current Learning Rate: 0.0010978480
2024-11-11 16:25:53,583 Train Loss: 0.0013728, Val Loss: 0.0015037
2024-11-11 16:25:53,583 Epoch 558/2000
2024-11-11 16:26:08,387 Current Learning Rate: 0.0010492249
2024-11-11 16:26:09,163 Train Loss: 0.0013686, Val Loss: 0.0015006
2024-11-11 16:26:09,163 Epoch 559/2000
2024-11-11 16:26:25,106 Current Learning Rate: 0.0010015767
2024-11-11 16:26:26,082 Train Loss: 0.0014711, Val Loss: 0.0014985
2024-11-11 16:26:26,082 Epoch 560/2000
2024-11-11 16:26:41,589 Current Learning Rate: 0.0009549150
2024-11-11 16:26:42,643 Train Loss: 0.0015281, Val Loss: 0.0014979
2024-11-11 16:26:42,644 Epoch 561/2000
2024-11-11 16:26:58,540 Current Learning Rate: 0.0009092514
2024-11-11 16:26:59,323 Train Loss: 0.0013669, Val Loss: 0.0014935
2024-11-11 16:26:59,323 Epoch 562/2000
2024-11-11 16:27:14,382 Current Learning Rate: 0.0008645971
2024-11-11 16:27:15,265 Train Loss: 0.0015915, Val Loss: 0.0014933
2024-11-11 16:27:15,266 Epoch 563/2000
2024-11-11 16:27:30,403 Current Learning Rate: 0.0008209632
2024-11-11 16:27:31,160 Train Loss: 0.0014926, Val Loss: 0.0014908
2024-11-11 16:27:31,160 Epoch 564/2000
2024-11-11 16:27:45,602 Current Learning Rate: 0.0007783604
2024-11-11 16:27:46,381 Train Loss: 0.0013564, Val Loss: 0.0014878
2024-11-11 16:27:46,381 Epoch 565/2000
2024-11-11 16:28:01,003 Current Learning Rate: 0.0007367992
2024-11-11 16:28:01,776 Train Loss: 0.0015443, Val Loss: 0.0014873
2024-11-11 16:28:01,776 Epoch 566/2000
2024-11-11 16:28:17,254 Current Learning Rate: 0.0006962899
2024-11-11 16:28:17,979 Train Loss: 0.0014868, Val Loss: 0.0014851
2024-11-11 16:28:17,980 Epoch 567/2000
2024-11-11 16:28:32,957 Current Learning Rate: 0.0006568424
2024-11-11 16:28:33,715 Train Loss: 0.0013553, Val Loss: 0.0014838
2024-11-11 16:28:33,715 Epoch 568/2000
2024-11-11 16:28:49,021 Current Learning Rate: 0.0006184666
2024-11-11 16:28:49,021 Train Loss: 0.0015204, Val Loss: 0.0014845
2024-11-11 16:28:49,022 Epoch 569/2000
2024-11-11 16:29:06,017 Current Learning Rate: 0.0005811718
2024-11-11 16:29:07,093 Train Loss: 0.0014230, Val Loss: 0.0014805
2024-11-11 16:29:07,094 Epoch 570/2000
2024-11-11 16:29:21,762 Current Learning Rate: 0.0005449674
2024-11-11 16:29:22,454 Train Loss: 0.0013558, Val Loss: 0.0014793
2024-11-11 16:29:22,454 Epoch 571/2000
2024-11-11 16:29:38,196 Current Learning Rate: 0.0005098621
2024-11-11 16:29:38,978 Train Loss: 0.0013530, Val Loss: 0.0014775
2024-11-11 16:29:38,978 Epoch 572/2000
2024-11-11 16:29:54,993 Current Learning Rate: 0.0004758647
2024-11-11 16:29:55,770 Train Loss: 0.0013935, Val Loss: 0.0014764
2024-11-11 16:29:55,770 Epoch 573/2000
2024-11-11 16:30:10,327 Current Learning Rate: 0.0004429836
2024-11-11 16:30:11,045 Train Loss: 0.0014663, Val Loss: 0.0014754
2024-11-11 16:30:11,045 Epoch 574/2000
2024-11-11 16:30:26,011 Current Learning Rate: 0.0004112269
2024-11-11 16:30:26,699 Train Loss: 0.0013592, Val Loss: 0.0014747
2024-11-11 16:30:26,699 Epoch 575/2000
2024-11-11 16:30:41,293 Current Learning Rate: 0.0003806023
2024-11-11 16:30:42,035 Train Loss: 0.0013511, Val Loss: 0.0014742
2024-11-11 16:30:42,035 Epoch 576/2000
2024-11-11 16:30:57,009 Current Learning Rate: 0.0003511176
2024-11-11 16:30:57,711 Train Loss: 0.0014025, Val Loss: 0.0014733
2024-11-11 16:30:57,711 Epoch 577/2000
2024-11-11 16:31:12,475 Current Learning Rate: 0.0003227798
2024-11-11 16:31:13,269 Train Loss: 0.0014400, Val Loss: 0.0014729
2024-11-11 16:31:13,270 Epoch 578/2000
2024-11-11 16:31:29,072 Current Learning Rate: 0.0002955962
2024-11-11 16:31:29,903 Train Loss: 0.0014251, Val Loss: 0.0014718
2024-11-11 16:31:29,904 Epoch 579/2000
2024-11-11 16:31:45,246 Current Learning Rate: 0.0002695732
2024-11-11 16:31:46,282 Train Loss: 0.0015810, Val Loss: 0.0014708
2024-11-11 16:31:46,283 Epoch 580/2000
2024-11-11 16:32:01,929 Current Learning Rate: 0.0002447174
2024-11-11 16:32:05,203 Train Loss: 0.0013351, Val Loss: 0.0014699
2024-11-11 16:32:05,203 Epoch 581/2000
2024-11-11 16:32:20,290 Current Learning Rate: 0.0002210349
2024-11-11 16:32:20,291 Train Loss: 0.0014548, Val Loss: 0.0014701
2024-11-11 16:32:20,291 Epoch 582/2000
2024-11-11 16:32:35,617 Current Learning Rate: 0.0001985316
2024-11-11 16:32:36,381 Train Loss: 0.0013962, Val Loss: 0.0014694
2024-11-11 16:32:36,381 Epoch 583/2000
2024-11-11 16:32:51,372 Current Learning Rate: 0.0001772129
2024-11-11 16:32:51,372 Train Loss: 0.0015118, Val Loss: 0.0014702
2024-11-11 16:32:51,373 Epoch 584/2000
2024-11-11 16:33:07,100 Current Learning Rate: 0.0001570842
2024-11-11 16:33:07,101 Train Loss: 0.0015893, Val Loss: 0.0014707
2024-11-11 16:33:07,101 Epoch 585/2000
2024-11-11 16:33:22,612 Current Learning Rate: 0.0001381504
2024-11-11 16:33:22,612 Train Loss: 0.0013931, Val Loss: 0.0014706
2024-11-11 16:33:22,612 Epoch 586/2000
2024-11-11 16:33:38,755 Current Learning Rate: 0.0001204162
2024-11-11 16:33:39,479 Train Loss: 0.0013333, Val Loss: 0.0014691
2024-11-11 16:33:39,480 Epoch 587/2000
2024-11-11 16:33:54,944 Current Learning Rate: 0.0001038859
2024-11-11 16:33:55,726 Train Loss: 0.0014591, Val Loss: 0.0014679
2024-11-11 16:33:55,727 Epoch 588/2000
2024-11-11 16:34:10,394 Current Learning Rate: 0.0000885637
2024-11-11 16:34:11,158 Train Loss: 0.0012854, Val Loss: 0.0014673
2024-11-11 16:34:11,158 Epoch 589/2000
2024-11-11 16:34:26,034 Current Learning Rate: 0.0000744534
2024-11-11 16:34:26,780 Train Loss: 0.0013903, Val Loss: 0.0014670
2024-11-11 16:34:26,781 Epoch 590/2000
2024-11-11 16:34:41,404 Current Learning Rate: 0.0000615583
2024-11-11 16:34:42,162 Train Loss: 0.0015155, Val Loss: 0.0014668
2024-11-11 16:34:42,163 Epoch 591/2000
2024-11-11 16:34:56,787 Current Learning Rate: 0.0000498817
2024-11-11 16:34:57,567 Train Loss: 0.0014036, Val Loss: 0.0014665
2024-11-11 16:34:57,568 Epoch 592/2000
2024-11-11 16:35:11,852 Current Learning Rate: 0.0000394265
2024-11-11 16:35:11,853 Train Loss: 0.0014947, Val Loss: 0.0014666
2024-11-11 16:35:11,853 Epoch 593/2000
2024-11-11 16:35:26,980 Current Learning Rate: 0.0000301952
2024-11-11 16:35:27,731 Train Loss: 0.0013346, Val Loss: 0.0014663
2024-11-11 16:35:27,731 Epoch 594/2000
2024-11-11 16:35:42,223 Current Learning Rate: 0.0000221902
2024-11-11 16:35:42,927 Train Loss: 0.0014882, Val Loss: 0.0014663
2024-11-11 16:35:42,927 Epoch 595/2000
2024-11-11 16:35:58,503 Current Learning Rate: 0.0000154133
2024-11-11 16:35:59,237 Train Loss: 0.0014868, Val Loss: 0.0014662
2024-11-11 16:35:59,237 Epoch 596/2000
2024-11-11 16:36:15,399 Current Learning Rate: 0.0000098664
2024-11-11 16:36:16,405 Train Loss: 0.0013868, Val Loss: 0.0014660
2024-11-11 16:36:16,405 Epoch 597/2000
2024-11-11 16:36:31,701 Current Learning Rate: 0.0000055506
2024-11-11 16:36:31,702 Train Loss: 0.0013281, Val Loss: 0.0014660
2024-11-11 16:36:31,702 Epoch 598/2000
2024-11-11 16:36:47,029 Current Learning Rate: 0.0000024672
2024-11-11 16:36:47,030 Train Loss: 0.0013248, Val Loss: 0.0014660
2024-11-11 16:36:47,030 Epoch 599/2000
2024-11-11 16:37:03,169 Current Learning Rate: 0.0000006168
2024-11-11 16:37:05,082 Train Loss: 0.0013347, Val Loss: 0.0014660
2024-11-11 16:37:05,083 Epoch 600/2000
2024-11-11 16:37:19,616 Current Learning Rate: 0.0000000000
2024-11-11 16:37:19,617 Train Loss: 0.0013388, Val Loss: 0.0014660
2024-11-11 16:37:19,617 Epoch 601/2000
2024-11-11 16:37:35,108 Current Learning Rate: 0.0000006168
2024-11-11 16:37:35,996 Train Loss: 0.0015185, Val Loss: 0.0014659
2024-11-11 16:37:35,997 Epoch 602/2000
2024-11-11 16:37:51,057 Current Learning Rate: 0.0000024672
2024-11-11 16:37:51,830 Train Loss: 0.0014399, Val Loss: 0.0014658
2024-11-11 16:37:51,830 Epoch 603/2000
2024-11-11 16:38:07,352 Current Learning Rate: 0.0000055506
2024-11-11 16:38:07,353 Train Loss: 0.0012863, Val Loss: 0.0014660
2024-11-11 16:38:07,353 Epoch 604/2000
2024-11-11 16:38:23,196 Current Learning Rate: 0.0000098664
2024-11-11 16:38:23,196 Train Loss: 0.0013989, Val Loss: 0.0014660
2024-11-11 16:38:23,197 Epoch 605/2000
2024-11-11 16:38:38,614 Current Learning Rate: 0.0000154133
2024-11-11 16:38:38,614 Train Loss: 0.0013243, Val Loss: 0.0014660
2024-11-11 16:38:38,614 Epoch 606/2000
2024-11-11 16:38:53,839 Current Learning Rate: 0.0000221902
2024-11-11 16:38:53,840 Train Loss: 0.0013791, Val Loss: 0.0014659
2024-11-11 16:38:53,840 Epoch 607/2000
2024-11-11 16:39:09,180 Current Learning Rate: 0.0000301952
2024-11-11 16:39:09,181 Train Loss: 0.0012837, Val Loss: 0.0014660
2024-11-11 16:39:09,181 Epoch 608/2000
2024-11-11 16:39:25,202 Current Learning Rate: 0.0000394265
2024-11-11 16:39:25,202 Train Loss: 0.0013361, Val Loss: 0.0014661
2024-11-11 16:39:25,202 Epoch 609/2000
2024-11-11 16:39:40,942 Current Learning Rate: 0.0000498817
2024-11-11 16:39:40,943 Train Loss: 0.0014310, Val Loss: 0.0014659
2024-11-11 16:39:40,943 Epoch 610/2000
2024-11-11 16:39:56,618 Current Learning Rate: 0.0000615583
2024-11-11 16:39:56,618 Train Loss: 0.0013358, Val Loss: 0.0014660
2024-11-11 16:39:56,618 Epoch 611/2000
2024-11-11 16:40:12,312 Current Learning Rate: 0.0000744534
2024-11-11 16:40:13,033 Train Loss: 0.0014237, Val Loss: 0.0014657
2024-11-11 16:40:13,033 Epoch 612/2000
2024-11-11 16:40:27,999 Current Learning Rate: 0.0000885637
2024-11-11 16:40:28,000 Train Loss: 0.0014800, Val Loss: 0.0014659
2024-11-11 16:40:28,000 Epoch 613/2000
2024-11-11 16:40:43,907 Current Learning Rate: 0.0001038859
2024-11-11 16:40:43,908 Train Loss: 0.0013293, Val Loss: 0.0014664
2024-11-11 16:40:43,908 Epoch 614/2000
2024-11-11 16:41:00,248 Current Learning Rate: 0.0001204162
2024-11-11 16:41:00,248 Train Loss: 0.0014150, Val Loss: 0.0014674
2024-11-11 16:41:00,249 Epoch 615/2000
2024-11-11 16:41:16,347 Current Learning Rate: 0.0001381504
2024-11-11 16:41:16,347 Train Loss: 0.0012844, Val Loss: 0.0014670
2024-11-11 16:41:16,347 Epoch 616/2000
2024-11-11 16:41:32,143 Current Learning Rate: 0.0001570842
2024-11-11 16:41:32,143 Train Loss: 0.0013916, Val Loss: 0.0014657
2024-11-11 16:41:32,144 Epoch 617/2000
2024-11-11 16:41:48,156 Current Learning Rate: 0.0001772129
2024-11-11 16:41:48,888 Train Loss: 0.0014848, Val Loss: 0.0014650
2024-11-11 16:41:48,888 Epoch 618/2000
2024-11-11 16:42:03,341 Current Learning Rate: 0.0001985316
2024-11-11 16:42:03,342 Train Loss: 0.0013309, Val Loss: 0.0014655
2024-11-11 16:42:03,342 Epoch 619/2000
2024-11-11 16:42:18,910 Current Learning Rate: 0.0002210349
2024-11-11 16:42:18,911 Train Loss: 0.0013759, Val Loss: 0.0014656
2024-11-11 16:42:18,911 Epoch 620/2000
2024-11-11 16:42:34,685 Current Learning Rate: 0.0002447174
2024-11-11 16:42:34,685 Train Loss: 0.0013422, Val Loss: 0.0014651
2024-11-11 16:42:34,685 Epoch 621/2000
2024-11-11 16:42:49,700 Current Learning Rate: 0.0002695732
2024-11-11 16:42:49,701 Train Loss: 0.0013875, Val Loss: 0.0014653
2024-11-11 16:42:49,701 Epoch 622/2000
2024-11-11 16:43:05,760 Current Learning Rate: 0.0002955962
2024-11-11 16:43:06,424 Train Loss: 0.0013393, Val Loss: 0.0014647
2024-11-11 16:43:06,425 Epoch 623/2000
2024-11-11 16:43:21,992 Current Learning Rate: 0.0003227798
2024-11-11 16:43:21,992 Train Loss: 0.0013753, Val Loss: 0.0014654
2024-11-11 16:43:21,993 Epoch 624/2000
2024-11-11 16:43:37,681 Current Learning Rate: 0.0003511176
2024-11-11 16:43:38,352 Train Loss: 0.0012840, Val Loss: 0.0014643
2024-11-11 16:43:38,352 Epoch 625/2000
2024-11-11 16:43:53,361 Current Learning Rate: 0.0003806023
2024-11-11 16:43:53,362 Train Loss: 0.0014439, Val Loss: 0.0014646
2024-11-11 16:43:53,362 Epoch 626/2000
2024-11-11 16:44:09,429 Current Learning Rate: 0.0004112269
2024-11-11 16:44:10,150 Train Loss: 0.0013898, Val Loss: 0.0014640
2024-11-11 16:44:10,150 Epoch 627/2000
2024-11-11 16:44:25,270 Current Learning Rate: 0.0004429836
2024-11-11 16:44:25,271 Train Loss: 0.0014299, Val Loss: 0.0014642
2024-11-11 16:44:25,271 Epoch 628/2000
2024-11-11 16:44:41,604 Current Learning Rate: 0.0004758647
2024-11-11 16:44:41,605 Train Loss: 0.0014359, Val Loss: 0.0014646
2024-11-11 16:44:41,605 Epoch 629/2000
2024-11-11 16:44:58,060 Current Learning Rate: 0.0005098621
2024-11-11 16:44:58,060 Train Loss: 0.0013414, Val Loss: 0.0014663
2024-11-11 16:44:58,061 Epoch 630/2000
2024-11-11 16:45:14,057 Current Learning Rate: 0.0005449674
2024-11-11 16:45:14,057 Train Loss: 0.0012910, Val Loss: 0.0014674
2024-11-11 16:45:14,058 Epoch 631/2000
2024-11-11 16:45:29,900 Current Learning Rate: 0.0005811718
2024-11-11 16:45:29,901 Train Loss: 0.0014344, Val Loss: 0.0014734
2024-11-11 16:45:29,901 Epoch 632/2000
2024-11-11 16:45:45,480 Current Learning Rate: 0.0006184666
2024-11-11 16:45:45,480 Train Loss: 0.0013955, Val Loss: 0.0014720
2024-11-11 16:45:45,481 Epoch 633/2000
2024-11-11 16:46:02,147 Current Learning Rate: 0.0006568424
2024-11-11 16:46:02,148 Train Loss: 0.0012893, Val Loss: 0.0014700
2024-11-11 16:46:02,148 Epoch 634/2000
2024-11-11 16:46:17,450 Current Learning Rate: 0.0006962899
2024-11-11 16:46:17,451 Train Loss: 0.0014054, Val Loss: 0.0014676
2024-11-11 16:46:17,451 Epoch 635/2000
2024-11-11 16:46:33,174 Current Learning Rate: 0.0007367992
2024-11-11 16:46:33,175 Train Loss: 0.0014313, Val Loss: 0.0014680
2024-11-11 16:46:33,175 Epoch 636/2000
2024-11-11 16:46:49,428 Current Learning Rate: 0.0007783604
2024-11-11 16:46:49,428 Train Loss: 0.0013911, Val Loss: 0.0014703
2024-11-11 16:46:49,428 Epoch 637/2000
2024-11-11 16:47:04,960 Current Learning Rate: 0.0008209632
2024-11-11 16:47:05,715 Train Loss: 0.0013219, Val Loss: 0.0014604
2024-11-11 16:47:05,716 Epoch 638/2000
2024-11-11 16:47:21,127 Current Learning Rate: 0.0008645971
2024-11-11 16:47:21,872 Train Loss: 0.0012867, Val Loss: 0.0014600
2024-11-11 16:47:21,872 Epoch 639/2000
2024-11-11 16:47:37,088 Current Learning Rate: 0.0009092514
2024-11-11 16:47:37,089 Train Loss: 0.0015195, Val Loss: 0.0014849
2024-11-11 16:47:37,089 Epoch 640/2000
2024-11-11 16:47:53,161 Current Learning Rate: 0.0009549150
2024-11-11 16:47:53,161 Train Loss: 0.0014029, Val Loss: 0.0014726
2024-11-11 16:47:53,161 Epoch 641/2000
2024-11-11 16:48:09,991 Current Learning Rate: 0.0010015767
2024-11-11 16:48:10,764 Train Loss: 0.0012848, Val Loss: 0.0014594
2024-11-11 16:48:10,764 Epoch 642/2000
2024-11-11 16:48:25,904 Current Learning Rate: 0.0010492249
2024-11-11 16:48:25,905 Train Loss: 0.0014437, Val Loss: 0.0014886
2024-11-11 16:48:25,905 Epoch 643/2000
2024-11-11 16:48:42,463 Current Learning Rate: 0.0010978480
2024-11-11 16:48:42,464 Train Loss: 0.0014407, Val Loss: 0.0014649
2024-11-11 16:48:42,464 Epoch 644/2000
2024-11-11 16:48:57,818 Current Learning Rate: 0.0011474338
2024-11-11 16:48:57,819 Train Loss: 0.0013760, Val Loss: 0.0014609
2024-11-11 16:48:57,819 Epoch 645/2000
2024-11-11 16:49:13,324 Current Learning Rate: 0.0011979702
2024-11-11 16:49:13,325 Train Loss: 0.0013695, Val Loss: 0.0015007
2024-11-11 16:49:13,325 Epoch 646/2000
2024-11-11 16:49:28,425 Current Learning Rate: 0.0012494447
2024-11-11 16:49:28,426 Train Loss: 0.0013328, Val Loss: 0.0014894
2024-11-11 16:49:28,426 Epoch 647/2000
2024-11-11 16:49:43,877 Current Learning Rate: 0.0013018445
2024-11-11 16:49:43,878 Train Loss: 0.0013234, Val Loss: 0.0015706
2024-11-11 16:49:43,878 Epoch 648/2000
2024-11-11 16:49:59,402 Current Learning Rate: 0.0013551569
2024-11-11 16:50:00,113 Train Loss: 0.0012977, Val Loss: 0.0014579
2024-11-11 16:50:00,113 Epoch 649/2000
2024-11-11 16:50:14,718 Current Learning Rate: 0.0014093685
2024-11-11 16:50:14,719 Train Loss: 0.0013465, Val Loss: 0.0014898
2024-11-11 16:50:14,720 Epoch 650/2000
2024-11-11 16:50:30,729 Current Learning Rate: 0.0014644661
2024-11-11 16:50:30,729 Train Loss: 0.0013614, Val Loss: 0.0014663
2024-11-11 16:50:30,730 Epoch 651/2000
2024-11-11 16:50:47,357 Current Learning Rate: 0.0015204360
2024-11-11 16:50:47,357 Train Loss: 0.0014855, Val Loss: 0.0015023
2024-11-11 16:50:47,358 Epoch 652/2000
2024-11-11 16:51:03,667 Current Learning Rate: 0.0015772645
2024-11-11 16:51:03,667 Train Loss: 0.0014394, Val Loss: 0.0015347
2024-11-11 16:51:03,667 Epoch 653/2000
2024-11-11 16:51:19,923 Current Learning Rate: 0.0016349374
2024-11-11 16:51:19,923 Train Loss: 0.0018136, Val Loss: 0.0015625
2024-11-11 16:51:19,924 Epoch 654/2000
2024-11-11 16:51:35,867 Current Learning Rate: 0.0016934407
2024-11-11 16:51:35,867 Train Loss: 0.0014872, Val Loss: 0.0015376
2024-11-11 16:51:35,867 Epoch 655/2000
2024-11-11 16:51:51,460 Current Learning Rate: 0.0017527598
2024-11-11 16:51:51,461 Train Loss: 0.0014274, Val Loss: 0.0014770
2024-11-11 16:51:51,461 Epoch 656/2000
2024-11-11 16:52:08,309 Current Learning Rate: 0.0018128801
2024-11-11 16:52:08,310 Train Loss: 0.0013513, Val Loss: 0.0014603
2024-11-11 16:52:08,310 Epoch 657/2000
2024-11-11 16:52:23,949 Current Learning Rate: 0.0018737867
2024-11-11 16:52:24,645 Train Loss: 0.0013941, Val Loss: 0.0014547
2024-11-11 16:52:24,645 Epoch 658/2000
2024-11-11 16:52:40,023 Current Learning Rate: 0.0019354647
2024-11-11 16:52:40,789 Train Loss: 0.0014043, Val Loss: 0.0014433
2024-11-11 16:52:40,790 Epoch 659/2000
2024-11-11 16:52:56,441 Current Learning Rate: 0.0019978989
2024-11-11 16:52:56,442 Train Loss: 0.0014911, Val Loss: 0.0014573
2024-11-11 16:52:56,442 Epoch 660/2000
2024-11-11 16:53:13,614 Current Learning Rate: 0.0020610737
2024-11-11 16:53:13,615 Train Loss: 0.0013329, Val Loss: 0.0014559
2024-11-11 16:53:13,615 Epoch 661/2000
2024-11-11 16:53:29,847 Current Learning Rate: 0.0021249737
2024-11-11 16:53:29,849 Train Loss: 0.0015343, Val Loss: 0.0014612
2024-11-11 16:53:29,849 Epoch 662/2000
2024-11-11 16:53:45,481 Current Learning Rate: 0.0021895831
2024-11-11 16:53:45,482 Train Loss: 0.0014109, Val Loss: 0.0014702
2024-11-11 16:53:45,482 Epoch 663/2000
2024-11-11 16:54:01,231 Current Learning Rate: 0.0022548859
2024-11-11 16:54:01,232 Train Loss: 0.0014110, Val Loss: 0.0014773
2024-11-11 16:54:01,232 Epoch 664/2000
2024-11-11 16:54:17,349 Current Learning Rate: 0.0023208660
2024-11-11 16:54:17,350 Train Loss: 0.0013513, Val Loss: 0.0014659
2024-11-11 16:54:17,350 Epoch 665/2000
2024-11-11 16:54:33,349 Current Learning Rate: 0.0023875072
2024-11-11 16:54:33,350 Train Loss: 0.0013578, Val Loss: 0.0014904
2024-11-11 16:54:33,350 Epoch 666/2000
2024-11-11 16:54:49,275 Current Learning Rate: 0.0024547929
2024-11-11 16:54:49,275 Train Loss: 0.0013009, Val Loss: 0.0014890
2024-11-11 16:54:49,276 Epoch 667/2000
2024-11-11 16:55:04,513 Current Learning Rate: 0.0025227067
2024-11-11 16:55:04,514 Train Loss: 0.0016047, Val Loss: 0.0015879
2024-11-11 16:55:04,514 Epoch 668/2000
2024-11-11 16:55:19,894 Current Learning Rate: 0.0025912316
2024-11-11 16:55:19,895 Train Loss: 0.0015072, Val Loss: 0.0016243
2024-11-11 16:55:19,895 Epoch 669/2000
2024-11-11 16:55:35,822 Current Learning Rate: 0.0026603509
2024-11-11 16:55:35,823 Train Loss: 0.0016115, Val Loss: 0.0015441
2024-11-11 16:55:35,823 Epoch 670/2000
2024-11-11 16:55:52,155 Current Learning Rate: 0.0027300475
2024-11-11 16:55:52,156 Train Loss: 0.0013965, Val Loss: 0.0014989
2024-11-11 16:55:52,156 Epoch 671/2000
2024-11-11 16:56:08,921 Current Learning Rate: 0.0028003042
2024-11-11 16:56:08,922 Train Loss: 0.0014858, Val Loss: 0.0015511
2024-11-11 16:56:08,922 Epoch 672/2000
2024-11-11 16:56:24,914 Current Learning Rate: 0.0028711035
2024-11-11 16:56:24,915 Train Loss: 0.0014088, Val Loss: 0.0014852
2024-11-11 16:56:24,915 Epoch 673/2000
2024-11-11 16:56:41,031 Current Learning Rate: 0.0029424282
2024-11-11 16:56:41,031 Train Loss: 0.0013549, Val Loss: 0.0014781
2024-11-11 16:56:41,032 Epoch 674/2000
2024-11-11 16:56:56,604 Current Learning Rate: 0.0030142605
2024-11-11 16:56:56,605 Train Loss: 0.0013579, Val Loss: 0.0015252
2024-11-11 16:56:56,605 Epoch 675/2000
2024-11-11 16:57:12,803 Current Learning Rate: 0.0030865828
2024-11-11 16:57:12,805 Train Loss: 0.0015570, Val Loss: 0.0015282
2024-11-11 16:57:12,805 Epoch 676/2000
2024-11-11 16:57:28,459 Current Learning Rate: 0.0031593772
2024-11-11 16:57:28,459 Train Loss: 0.0014531, Val Loss: 0.0014883
2024-11-11 16:57:28,459 Epoch 677/2000
2024-11-11 16:57:43,221 Current Learning Rate: 0.0032326258
2024-11-11 16:57:43,221 Train Loss: 0.0013234, Val Loss: 0.0015019
2024-11-11 16:57:43,221 Epoch 678/2000
2024-11-11 16:57:58,533 Current Learning Rate: 0.0033063104
2024-11-11 16:57:58,534 Train Loss: 0.0013389, Val Loss: 0.0014453
2024-11-11 16:57:58,534 Epoch 679/2000
2024-11-11 16:58:14,635 Current Learning Rate: 0.0033804129
2024-11-11 16:58:15,303 Train Loss: 0.0012721, Val Loss: 0.0014410
2024-11-11 16:58:15,303 Epoch 680/2000
2024-11-11 16:58:30,687 Current Learning Rate: 0.0034549150
2024-11-11 16:58:31,340 Train Loss: 0.0012667, Val Loss: 0.0014372
2024-11-11 16:58:31,340 Epoch 681/2000
2024-11-11 16:58:45,805 Current Learning Rate: 0.0035297984
2024-11-11 16:58:45,806 Train Loss: 0.0013364, Val Loss: 0.0014814
2024-11-11 16:58:45,806 Epoch 682/2000
2024-11-11 16:59:01,372 Current Learning Rate: 0.0036050445
2024-11-11 16:59:01,372 Train Loss: 0.0014830, Val Loss: 0.0014760
2024-11-11 16:59:01,373 Epoch 683/2000
2024-11-11 16:59:16,758 Current Learning Rate: 0.0036806348
2024-11-11 16:59:16,758 Train Loss: 0.0013517, Val Loss: 0.0015127
2024-11-11 16:59:16,758 Epoch 684/2000
2024-11-11 16:59:32,008 Current Learning Rate: 0.0037565506
2024-11-11 16:59:32,008 Train Loss: 0.0015798, Val Loss: 0.0015486
2024-11-11 16:59:32,009 Epoch 685/2000
2024-11-11 16:59:47,634 Current Learning Rate: 0.0038327732
2024-11-11 16:59:47,634 Train Loss: 0.0013792, Val Loss: 0.0016443
2024-11-11 16:59:47,635 Epoch 686/2000
2024-11-11 17:00:03,985 Current Learning Rate: 0.0039092838
2024-11-11 17:00:03,985 Train Loss: 0.0015946, Val Loss: 0.0015486
2024-11-11 17:00:03,985 Epoch 687/2000
2024-11-11 17:00:20,674 Current Learning Rate: 0.0039860635
2024-11-11 17:00:20,675 Train Loss: 0.0013978, Val Loss: 0.0014972
2024-11-11 17:00:20,675 Epoch 688/2000
2024-11-11 17:00:36,709 Current Learning Rate: 0.0040630934
2024-11-11 17:00:36,709 Train Loss: 0.0014477, Val Loss: 0.0014903
2024-11-11 17:00:36,710 Epoch 689/2000
2024-11-11 17:00:51,531 Current Learning Rate: 0.0041403545
2024-11-11 17:00:51,532 Train Loss: 0.0014564, Val Loss: 0.0015632
2024-11-11 17:00:51,532 Epoch 690/2000
2024-11-11 17:01:06,923 Current Learning Rate: 0.0042178277
2024-11-11 17:01:06,924 Train Loss: 0.0015276, Val Loss: 0.0015127
2024-11-11 17:01:06,924 Epoch 691/2000
2024-11-11 17:01:22,501 Current Learning Rate: 0.0042954938
2024-11-11 17:01:22,502 Train Loss: 0.0014424, Val Loss: 0.0015113
2024-11-11 17:01:22,502 Epoch 692/2000
2024-11-11 17:01:37,948 Current Learning Rate: 0.0043733338
2024-11-11 17:01:37,948 Train Loss: 0.0013432, Val Loss: 0.0015069
2024-11-11 17:01:37,949 Epoch 693/2000
2024-11-11 17:01:53,419 Current Learning Rate: 0.0044513284
2024-11-11 17:01:53,420 Train Loss: 0.0016064, Val Loss: 0.0016084
2024-11-11 17:01:53,420 Epoch 694/2000
2024-11-11 17:02:09,692 Current Learning Rate: 0.0045294584
2024-11-11 17:02:09,692 Train Loss: 0.0016979, Val Loss: 0.0015626
2024-11-11 17:02:09,693 Epoch 695/2000
2024-11-11 17:02:25,502 Current Learning Rate: 0.0046077045
2024-11-11 17:02:25,503 Train Loss: 0.0013709, Val Loss: 0.0015908
2024-11-11 17:02:25,503 Epoch 696/2000
2024-11-11 17:02:42,140 Current Learning Rate: 0.0046860474
2024-11-11 17:02:42,140 Train Loss: 0.0013405, Val Loss: 0.0014735
2024-11-11 17:02:42,141 Epoch 697/2000
2024-11-11 17:02:58,552 Current Learning Rate: 0.0047644677
2024-11-11 17:02:58,553 Train Loss: 0.0015411, Val Loss: 0.0015687
2024-11-11 17:02:58,553 Epoch 698/2000
2024-11-11 17:03:14,633 Current Learning Rate: 0.0048429462
2024-11-11 17:03:14,634 Train Loss: 0.0014065, Val Loss: 0.0015166
2024-11-11 17:03:14,634 Epoch 699/2000
2024-11-11 17:03:31,389 Current Learning Rate: 0.0049214634
2024-11-11 17:03:31,389 Train Loss: 0.0019209, Val Loss: 0.0015936
2024-11-11 17:03:31,389 Epoch 700/2000
2024-11-11 17:03:47,452 Current Learning Rate: 0.0050000000
2024-11-11 17:03:47,452 Train Loss: 0.0014235, Val Loss: 0.0015003
2024-11-11 17:03:47,453 Epoch 701/2000
2024-11-11 17:04:03,088 Current Learning Rate: 0.0050785366
2024-11-11 17:04:03,089 Train Loss: 0.0013756, Val Loss: 0.0015143
2024-11-11 17:04:03,090 Epoch 702/2000
2024-11-11 17:04:18,856 Current Learning Rate: 0.0051570538
2024-11-11 17:04:18,856 Train Loss: 0.0013229, Val Loss: 0.0015112
2024-11-11 17:04:18,857 Epoch 703/2000
2024-11-11 17:04:35,025 Current Learning Rate: 0.0052355323
2024-11-11 17:04:35,026 Train Loss: 0.0014407, Val Loss: 0.0015284
2024-11-11 17:04:35,026 Epoch 704/2000
2024-11-11 17:04:51,159 Current Learning Rate: 0.0053139526
2024-11-11 17:04:51,159 Train Loss: 0.0017120, Val Loss: 0.0016282
2024-11-11 17:04:51,159 Epoch 705/2000
2024-11-11 17:05:07,342 Current Learning Rate: 0.0053922955
2024-11-11 17:05:07,342 Train Loss: 0.0015467, Val Loss: 0.0015771
2024-11-11 17:05:07,343 Epoch 706/2000
2024-11-11 17:05:23,397 Current Learning Rate: 0.0054705416
2024-11-11 17:05:23,398 Train Loss: 0.0013901, Val Loss: 0.0014990
2024-11-11 17:05:23,398 Epoch 707/2000
2024-11-11 17:05:39,743 Current Learning Rate: 0.0055486716
2024-11-11 17:05:39,744 Train Loss: 0.0015504, Val Loss: 0.0014934
2024-11-11 17:05:39,744 Epoch 708/2000
2024-11-11 17:05:55,815 Current Learning Rate: 0.0056266662
2024-11-11 17:05:55,815 Train Loss: 0.0014991, Val Loss: 0.0014600
2024-11-11 17:05:55,815 Epoch 709/2000
2024-11-11 17:06:11,530 Current Learning Rate: 0.0057045062
2024-11-11 17:06:11,530 Train Loss: 0.0013958, Val Loss: 0.0014611
2024-11-11 17:06:11,530 Epoch 710/2000
2024-11-11 17:06:28,017 Current Learning Rate: 0.0057821723
2024-11-11 17:06:28,018 Train Loss: 0.0012802, Val Loss: 0.0015189
2024-11-11 17:06:28,018 Epoch 711/2000
2024-11-11 17:06:44,563 Current Learning Rate: 0.0058596455
2024-11-11 17:06:44,564 Train Loss: 0.0013394, Val Loss: 0.0014677
2024-11-11 17:06:44,564 Epoch 712/2000
2024-11-11 17:07:00,097 Current Learning Rate: 0.0059369066
2024-11-11 17:07:00,097 Train Loss: 0.0014255, Val Loss: 0.0014887
2024-11-11 17:07:00,097 Epoch 713/2000
2024-11-11 17:07:15,474 Current Learning Rate: 0.0060139365
2024-11-11 17:07:15,475 Train Loss: 0.0014388, Val Loss: 0.0015676
2024-11-11 17:07:15,475 Epoch 714/2000
2024-11-11 17:07:30,828 Current Learning Rate: 0.0060907162
2024-11-11 17:07:30,828 Train Loss: 0.0017099, Val Loss: 0.0017168
2024-11-11 17:07:30,829 Epoch 715/2000
2024-11-11 17:07:46,216 Current Learning Rate: 0.0061672268
2024-11-11 17:07:46,216 Train Loss: 0.0017264, Val Loss: 0.0015712
2024-11-11 17:07:46,216 Epoch 716/2000
2024-11-11 17:08:02,523 Current Learning Rate: 0.0062434494
2024-11-11 17:08:02,524 Train Loss: 0.0014189, Val Loss: 0.0014860
2024-11-11 17:08:02,524 Epoch 717/2000
2024-11-11 17:08:18,619 Current Learning Rate: 0.0063193652
2024-11-11 17:08:18,620 Train Loss: 0.0014357, Val Loss: 0.0015216
2024-11-11 17:08:18,620 Epoch 718/2000
2024-11-11 17:08:35,352 Current Learning Rate: 0.0063949555
2024-11-11 17:08:35,353 Train Loss: 0.0013072, Val Loss: 0.0014546
2024-11-11 17:08:35,353 Epoch 719/2000
2024-11-11 17:08:51,000 Current Learning Rate: 0.0064702016
2024-11-11 17:08:51,000 Train Loss: 0.0014518, Val Loss: 0.0014949
2024-11-11 17:08:51,001 Epoch 720/2000
2024-11-11 17:09:06,208 Current Learning Rate: 0.0065450850
2024-11-11 17:09:06,208 Train Loss: 0.0014047, Val Loss: 0.0015171
2024-11-11 17:09:06,209 Epoch 721/2000
2024-11-11 17:09:21,760 Current Learning Rate: 0.0066195871
2024-11-11 17:09:21,761 Train Loss: 0.0014210, Val Loss: 0.0014804
2024-11-11 17:09:21,761 Epoch 722/2000
2024-11-11 17:09:37,125 Current Learning Rate: 0.0066936896
2024-11-11 17:09:37,125 Train Loss: 0.0014459, Val Loss: 0.0018787
2024-11-11 17:09:37,125 Epoch 723/2000
2024-11-11 17:09:52,220 Current Learning Rate: 0.0067673742
2024-11-11 17:09:52,220 Train Loss: 0.0013806, Val Loss: 0.0015081
2024-11-11 17:09:52,221 Epoch 724/2000
2024-11-11 17:10:07,619 Current Learning Rate: 0.0068406228
2024-11-11 17:10:07,619 Train Loss: 0.0014868, Val Loss: 0.0015729
2024-11-11 17:10:07,620 Epoch 725/2000
2024-11-11 17:10:23,573 Current Learning Rate: 0.0069134172
2024-11-11 17:10:23,574 Train Loss: 0.0016074, Val Loss: 0.0017101
2024-11-11 17:10:23,574 Epoch 726/2000
2024-11-11 17:10:39,384 Current Learning Rate: 0.0069857395
2024-11-11 17:10:39,385 Train Loss: 0.0013524, Val Loss: 0.0014477
2024-11-11 17:10:39,385 Epoch 727/2000
2024-11-11 17:10:55,280 Current Learning Rate: 0.0070575718
2024-11-11 17:10:55,281 Train Loss: 0.0013677, Val Loss: 0.0014720
2024-11-11 17:10:55,281 Epoch 728/2000
2024-11-11 17:11:10,835 Current Learning Rate: 0.0071288965
2024-11-11 17:11:10,836 Train Loss: 0.0014198, Val Loss: 0.0015157
2024-11-11 17:11:10,836 Epoch 729/2000
2024-11-11 17:11:27,227 Current Learning Rate: 0.0071996958
2024-11-11 17:11:27,228 Train Loss: 0.0012735, Val Loss: 0.0014381
2024-11-11 17:11:27,228 Epoch 730/2000
2024-11-11 17:11:43,332 Current Learning Rate: 0.0072699525
2024-11-11 17:11:43,332 Train Loss: 0.0015931, Val Loss: 0.0018424
2024-11-11 17:11:43,333 Epoch 731/2000
2024-11-11 17:12:00,490 Current Learning Rate: 0.0073396491
2024-11-11 17:12:00,491 Train Loss: 0.0016338, Val Loss: 0.0015356
2024-11-11 17:12:00,491 Epoch 732/2000
2024-11-11 17:12:16,637 Current Learning Rate: 0.0074087684
2024-11-11 17:12:16,637 Train Loss: 0.0014040, Val Loss: 0.0014609
2024-11-11 17:12:16,638 Epoch 733/2000
2024-11-11 17:12:31,840 Current Learning Rate: 0.0074772933
2024-11-11 17:12:32,671 Train Loss: 0.0012633, Val Loss: 0.0014138
2024-11-11 17:12:32,671 Epoch 734/2000
2024-11-11 17:12:46,926 Current Learning Rate: 0.0075452071
2024-11-11 17:12:46,927 Train Loss: 0.0013944, Val Loss: 0.0015161
2024-11-11 17:12:46,927 Epoch 735/2000
2024-11-11 17:13:02,279 Current Learning Rate: 0.0076124928
2024-11-11 17:13:02,280 Train Loss: 0.0014223, Val Loss: 0.0015125
2024-11-11 17:13:02,280 Epoch 736/2000
2024-11-11 17:13:17,803 Current Learning Rate: 0.0076791340
2024-11-11 17:13:17,804 Train Loss: 0.0013919, Val Loss: 0.0015442
2024-11-11 17:13:17,804 Epoch 737/2000
2024-11-11 17:13:32,973 Current Learning Rate: 0.0077451141
2024-11-11 17:13:32,973 Train Loss: 0.0013839, Val Loss: 0.0015243
2024-11-11 17:13:32,974 Epoch 738/2000
2024-11-11 17:13:48,400 Current Learning Rate: 0.0078104169
2024-11-11 17:13:48,400 Train Loss: 0.0014401, Val Loss: 0.0015702
2024-11-11 17:13:48,400 Epoch 739/2000
2024-11-11 17:14:03,439 Current Learning Rate: 0.0078750263
2024-11-11 17:14:03,439 Train Loss: 0.0018107, Val Loss: 0.0018858
2024-11-11 17:14:03,439 Epoch 740/2000
2024-11-11 17:14:18,950 Current Learning Rate: 0.0079389263
2024-11-11 17:14:18,950 Train Loss: 0.0016465, Val Loss: 0.0016108
2024-11-11 17:14:18,950 Epoch 741/2000
2024-11-11 17:14:35,116 Current Learning Rate: 0.0080021011
2024-11-11 17:14:35,117 Train Loss: 0.0014754, Val Loss: 0.0014901
2024-11-11 17:14:35,117 Epoch 742/2000
2024-11-11 17:14:51,098 Current Learning Rate: 0.0080645353
2024-11-11 17:14:51,098 Train Loss: 0.0013448, Val Loss: 0.0014771
2024-11-11 17:14:51,098 Epoch 743/2000
2024-11-11 17:15:06,433 Current Learning Rate: 0.0081262133
2024-11-11 17:15:06,434 Train Loss: 0.0014345, Val Loss: 0.0014970
2024-11-11 17:15:06,434 Epoch 744/2000
2024-11-11 17:15:21,907 Current Learning Rate: 0.0081871199
2024-11-11 17:15:21,907 Train Loss: 0.0014178, Val Loss: 0.0014923
2024-11-11 17:15:21,908 Epoch 745/2000
2024-11-11 17:15:38,443 Current Learning Rate: 0.0082472402
2024-11-11 17:15:38,444 Train Loss: 0.0013038, Val Loss: 0.0014537
2024-11-11 17:15:38,444 Epoch 746/2000
2024-11-11 17:15:54,349 Current Learning Rate: 0.0083065593
2024-11-11 17:15:54,350 Train Loss: 0.0015127, Val Loss: 0.0015143
2024-11-11 17:15:54,350 Epoch 747/2000
2024-11-11 17:16:10,352 Current Learning Rate: 0.0083650626
2024-11-11 17:16:10,352 Train Loss: 0.0015203, Val Loss: 0.0014412
2024-11-11 17:16:10,353 Epoch 748/2000
2024-11-11 17:16:26,800 Current Learning Rate: 0.0084227355
2024-11-11 17:16:26,801 Train Loss: 0.0014944, Val Loss: 0.0015220
2024-11-11 17:16:26,801 Epoch 749/2000
2024-11-11 17:16:42,648 Current Learning Rate: 0.0084795640
2024-11-11 17:16:43,437 Train Loss: 0.0012655, Val Loss: 0.0013929
2024-11-11 17:16:43,437 Epoch 750/2000
2024-11-11 17:16:59,023 Current Learning Rate: 0.0085355339
2024-11-11 17:16:59,024 Train Loss: 0.0014169, Val Loss: 0.0014294
2024-11-11 17:16:59,024 Epoch 751/2000
2024-11-11 17:17:15,535 Current Learning Rate: 0.0085906315
2024-11-11 17:17:15,535 Train Loss: 0.0014581, Val Loss: 0.0014705
2024-11-11 17:17:15,535 Epoch 752/2000
2024-11-11 17:17:31,363 Current Learning Rate: 0.0086448431
2024-11-11 17:17:32,141 Train Loss: 0.0012912, Val Loss: 0.0013847
2024-11-11 17:17:32,141 Epoch 753/2000
2024-11-11 17:17:47,246 Current Learning Rate: 0.0086981555
2024-11-11 17:17:47,247 Train Loss: 0.0013669, Val Loss: 0.0014633
2024-11-11 17:17:47,247 Epoch 754/2000
2024-11-11 17:18:03,462 Current Learning Rate: 0.0087505553
2024-11-11 17:18:03,463 Train Loss: 0.0013582, Val Loss: 0.0014893
2024-11-11 17:18:03,463 Epoch 755/2000
2024-11-11 17:18:20,208 Current Learning Rate: 0.0088020298
2024-11-11 17:18:20,209 Train Loss: 0.0012477, Val Loss: 0.0014051
2024-11-11 17:18:20,210 Epoch 756/2000
2024-11-11 17:18:37,412 Current Learning Rate: 0.0088525662
2024-11-11 17:18:37,413 Train Loss: 0.0012354, Val Loss: 0.0014067
2024-11-11 17:18:37,413 Epoch 757/2000
2024-11-11 17:18:54,361 Current Learning Rate: 0.0089021520
2024-11-11 17:18:54,362 Train Loss: 0.0012827, Val Loss: 0.0014827
2024-11-11 17:18:54,362 Epoch 758/2000
2024-11-11 17:19:11,100 Current Learning Rate: 0.0089507751
2024-11-11 17:19:11,101 Train Loss: 0.0013223, Val Loss: 0.0014795
2024-11-11 17:19:11,101 Epoch 759/2000
2024-11-11 17:19:26,911 Current Learning Rate: 0.0089984233
2024-11-11 17:19:26,912 Train Loss: 0.0014247, Val Loss: 0.0017839
2024-11-11 17:19:26,912 Epoch 760/2000
2024-11-11 17:19:42,660 Current Learning Rate: 0.0090450850
2024-11-11 17:19:42,661 Train Loss: 0.0013827, Val Loss: 0.0014344
2024-11-11 17:19:42,661 Epoch 761/2000
2024-11-11 17:19:59,085 Current Learning Rate: 0.0090907486
2024-11-11 17:19:59,086 Train Loss: 0.0014266, Val Loss: 0.0014995
2024-11-11 17:19:59,087 Epoch 762/2000
2024-11-11 17:20:14,751 Current Learning Rate: 0.0091354029
2024-11-11 17:20:14,752 Train Loss: 0.0014604, Val Loss: 0.0015419
2024-11-11 17:20:14,752 Epoch 763/2000
2024-11-11 17:20:30,831 Current Learning Rate: 0.0091790368
2024-11-11 17:20:30,832 Train Loss: 0.0014246, Val Loss: 0.0015470
2024-11-11 17:20:30,832 Epoch 764/2000
2024-11-11 17:20:46,613 Current Learning Rate: 0.0092216396
2024-11-11 17:20:46,615 Train Loss: 0.0013135, Val Loss: 0.0014740
2024-11-11 17:20:46,615 Epoch 765/2000
2024-11-11 17:21:03,003 Current Learning Rate: 0.0092632008
2024-11-11 17:21:03,004 Train Loss: 0.0014266, Val Loss: 0.0015145
2024-11-11 17:21:03,004 Epoch 766/2000
2024-11-11 17:21:19,068 Current Learning Rate: 0.0093037101
2024-11-11 17:21:19,068 Train Loss: 0.0013968, Val Loss: 0.0014449
2024-11-11 17:21:19,069 Epoch 767/2000
2024-11-11 17:21:35,054 Current Learning Rate: 0.0093431576
2024-11-11 17:21:35,054 Train Loss: 0.0013752, Val Loss: 0.0014473
2024-11-11 17:21:35,055 Epoch 768/2000
2024-11-11 17:21:50,172 Current Learning Rate: 0.0093815334
2024-11-11 17:21:50,172 Train Loss: 0.0013089, Val Loss: 0.0014230
2024-11-11 17:21:50,173 Epoch 769/2000
2024-11-11 17:22:05,512 Current Learning Rate: 0.0094188282
2024-11-11 17:22:05,514 Train Loss: 0.0013418, Val Loss: 0.0014052
2024-11-11 17:22:05,514 Epoch 770/2000
2024-11-11 17:22:20,416 Current Learning Rate: 0.0094550326
2024-11-11 17:22:20,417 Train Loss: 0.0013460, Val Loss: 0.0014729
2024-11-11 17:22:20,417 Epoch 771/2000
2024-11-11 17:22:36,114 Current Learning Rate: 0.0094901379
2024-11-11 17:22:36,115 Train Loss: 0.0013318, Val Loss: 0.0014820
2024-11-11 17:22:36,115 Epoch 772/2000
2024-11-11 17:22:51,343 Current Learning Rate: 0.0095241353
2024-11-11 17:22:52,057 Train Loss: 0.0011896, Val Loss: 0.0013285
2024-11-11 17:22:52,057 Epoch 773/2000
2024-11-11 17:23:07,033 Current Learning Rate: 0.0095570164
2024-11-11 17:23:07,034 Train Loss: 0.0013304, Val Loss: 0.0013801
2024-11-11 17:23:07,034 Epoch 774/2000
2024-11-11 17:23:22,702 Current Learning Rate: 0.0095887731
2024-11-11 17:23:22,703 Train Loss: 0.0013142, Val Loss: 0.0014194
2024-11-11 17:23:22,703 Epoch 775/2000
2024-11-11 17:23:38,813 Current Learning Rate: 0.0096193977
2024-11-11 17:23:38,813 Train Loss: 0.0013468, Val Loss: 0.0014064
2024-11-11 17:23:38,813 Epoch 776/2000
2024-11-11 17:23:55,001 Current Learning Rate: 0.0096488824
2024-11-11 17:23:55,001 Train Loss: 0.0012973, Val Loss: 0.0014213
2024-11-11 17:23:55,002 Epoch 777/2000
2024-11-11 17:24:10,834 Current Learning Rate: 0.0096772202
2024-11-11 17:24:11,587 Train Loss: 0.0011907, Val Loss: 0.0013259
2024-11-11 17:24:11,587 Epoch 778/2000
2024-11-11 17:24:25,919 Current Learning Rate: 0.0097044038
2024-11-11 17:24:25,920 Train Loss: 0.0013381, Val Loss: 0.0014263
2024-11-11 17:24:25,920 Epoch 779/2000
2024-11-11 17:24:42,361 Current Learning Rate: 0.0097304268
2024-11-11 17:24:42,362 Train Loss: 0.0013340, Val Loss: 0.0013705
2024-11-11 17:24:42,362 Epoch 780/2000
2024-11-11 17:24:58,263 Current Learning Rate: 0.0097552826
2024-11-11 17:24:58,264 Train Loss: 0.0013152, Val Loss: 0.0013503
2024-11-11 17:24:58,265 Epoch 781/2000
2024-11-11 17:25:14,052 Current Learning Rate: 0.0097789651
2024-11-11 17:25:14,053 Train Loss: 0.0012261, Val Loss: 0.0014349
2024-11-11 17:25:14,054 Epoch 782/2000
2024-11-11 17:25:30,706 Current Learning Rate: 0.0098014684
2024-11-11 17:25:30,707 Train Loss: 0.0013467, Val Loss: 0.0014464
2024-11-11 17:25:30,707 Epoch 783/2000
2024-11-11 17:25:47,583 Current Learning Rate: 0.0098227871
2024-11-11 17:25:47,584 Train Loss: 0.0012841, Val Loss: 0.0014398
2024-11-11 17:25:47,584 Epoch 784/2000
2024-11-11 17:26:02,644 Current Learning Rate: 0.0098429158
2024-11-11 17:26:02,645 Train Loss: 0.0012904, Val Loss: 0.0013725
2024-11-11 17:26:02,645 Epoch 785/2000
2024-11-11 17:26:19,072 Current Learning Rate: 0.0098618496
2024-11-11 17:26:19,073 Train Loss: 0.0013397, Val Loss: 0.0013750
2024-11-11 17:26:19,073 Epoch 786/2000
2024-11-11 17:26:34,064 Current Learning Rate: 0.0098795838
2024-11-11 17:26:34,065 Train Loss: 0.0013598, Val Loss: 0.0013267
2024-11-11 17:26:34,065 Epoch 787/2000
2024-11-11 17:26:49,310 Current Learning Rate: 0.0098961141
2024-11-11 17:26:49,310 Train Loss: 0.0013358, Val Loss: 0.0014850
2024-11-11 17:26:49,311 Epoch 788/2000
2024-11-11 17:27:04,770 Current Learning Rate: 0.0099114363
2024-11-11 17:27:04,771 Train Loss: 0.0014228, Val Loss: 0.0015173
2024-11-11 17:27:04,771 Epoch 789/2000
2024-11-11 17:27:20,899 Current Learning Rate: 0.0099255466
2024-11-11 17:27:20,899 Train Loss: 0.0014229, Val Loss: 0.0014432
2024-11-11 17:27:20,899 Epoch 790/2000
2024-11-11 17:27:37,233 Current Learning Rate: 0.0099384417
2024-11-11 17:27:37,234 Train Loss: 0.0012249, Val Loss: 0.0013717
2024-11-11 17:27:37,234 Epoch 791/2000
2024-11-11 17:27:53,191 Current Learning Rate: 0.0099501183
2024-11-11 17:27:53,192 Train Loss: 0.0011699, Val Loss: 0.0013676
2024-11-11 17:27:53,192 Epoch 792/2000
2024-11-11 17:28:09,828 Current Learning Rate: 0.0099605735
2024-11-11 17:28:09,828 Train Loss: 0.0012559, Val Loss: 0.0013603
2024-11-11 17:28:09,828 Epoch 793/2000
2024-11-11 17:28:26,718 Current Learning Rate: 0.0099698048
2024-11-11 17:28:26,719 Train Loss: 0.0012179, Val Loss: 0.0013536
2024-11-11 17:28:26,720 Epoch 794/2000
2024-11-11 17:28:42,520 Current Learning Rate: 0.0099778098
2024-11-11 17:28:42,521 Train Loss: 0.0013702, Val Loss: 0.0013276
2024-11-11 17:28:42,521 Epoch 795/2000
2024-11-11 17:28:58,514 Current Learning Rate: 0.0099845867
2024-11-11 17:28:59,554 Train Loss: 0.0011509, Val Loss: 0.0012980
2024-11-11 17:28:59,554 Epoch 796/2000
2024-11-11 17:29:14,723 Current Learning Rate: 0.0099901336
2024-11-11 17:29:14,724 Train Loss: 0.0012321, Val Loss: 0.0013927
2024-11-11 17:29:14,725 Epoch 797/2000
2024-11-11 17:29:30,497 Current Learning Rate: 0.0099944494
2024-11-11 17:29:30,497 Train Loss: 0.0012585, Val Loss: 0.0013159
2024-11-11 17:29:30,498 Epoch 798/2000
2024-11-11 17:29:46,559 Current Learning Rate: 0.0099975328
2024-11-11 17:29:46,559 Train Loss: 0.0011738, Val Loss: 0.0013762
2024-11-11 17:29:46,560 Epoch 799/2000
2024-11-11 17:30:01,241 Current Learning Rate: 0.0099993832
2024-11-11 17:30:01,242 Train Loss: 0.0011849, Val Loss: 0.0013323
2024-11-11 17:30:01,242 Epoch 800/2000
2024-11-11 17:30:16,359 Current Learning Rate: 0.0100000000
2024-11-11 17:30:16,359 Train Loss: 0.0012342, Val Loss: 0.0013504
2024-11-11 17:30:16,360 Epoch 801/2000
2024-11-11 17:30:31,872 Current Learning Rate: 0.0099993832
2024-11-11 17:30:31,873 Train Loss: 0.0012720, Val Loss: 0.0013579
2024-11-11 17:30:31,873 Epoch 802/2000
2024-11-11 17:30:47,663 Current Learning Rate: 0.0099975328
2024-11-11 17:30:47,663 Train Loss: 0.0012628, Val Loss: 0.0013589
2024-11-11 17:30:47,664 Epoch 803/2000
2024-11-11 17:31:03,484 Current Learning Rate: 0.0099944494
2024-11-11 17:31:03,484 Train Loss: 0.0013250, Val Loss: 0.0014446
2024-11-11 17:31:03,485 Epoch 804/2000
2024-11-11 17:31:19,722 Current Learning Rate: 0.0099901336
2024-11-11 17:31:19,722 Train Loss: 0.0012140, Val Loss: 0.0013398
2024-11-11 17:31:19,723 Epoch 805/2000
2024-11-11 17:31:35,267 Current Learning Rate: 0.0099845867
2024-11-11 17:31:35,268 Train Loss: 0.0013890, Val Loss: 0.0015312
2024-11-11 17:31:35,268 Epoch 806/2000
2024-11-11 17:31:51,629 Current Learning Rate: 0.0099778098
2024-11-11 17:31:51,630 Train Loss: 0.0015518, Val Loss: 0.0015830
2024-11-11 17:31:51,630 Epoch 807/2000
2024-11-11 17:32:06,988 Current Learning Rate: 0.0099698048
2024-11-11 17:32:06,989 Train Loss: 0.0013848, Val Loss: 0.0013396
2024-11-11 17:32:06,989 Epoch 808/2000
2024-11-11 17:32:22,774 Current Learning Rate: 0.0099605735
2024-11-11 17:32:22,774 Train Loss: 0.0013203, Val Loss: 0.0013689
2024-11-11 17:32:22,774 Epoch 809/2000
2024-11-11 17:32:39,129 Current Learning Rate: 0.0099501183
2024-11-11 17:32:39,130 Train Loss: 0.0011904, Val Loss: 0.0014270
2024-11-11 17:32:39,130 Epoch 810/2000
2024-11-11 17:32:54,516 Current Learning Rate: 0.0099384417
2024-11-11 17:32:54,516 Train Loss: 0.0012725, Val Loss: 0.0013571
2024-11-11 17:32:54,516 Epoch 811/2000
2024-11-11 17:33:10,881 Current Learning Rate: 0.0099255466
2024-11-11 17:33:10,882 Train Loss: 0.0011830, Val Loss: 0.0013175
2024-11-11 17:33:10,882 Epoch 812/2000
2024-11-11 17:33:26,183 Current Learning Rate: 0.0099114363
2024-11-11 17:33:26,184 Train Loss: 0.0011857, Val Loss: 0.0013092
2024-11-11 17:33:26,184 Epoch 813/2000
2024-11-11 17:33:41,410 Current Learning Rate: 0.0098961141
2024-11-11 17:33:42,181 Train Loss: 0.0011688, Val Loss: 0.0012712
2024-11-11 17:33:42,181 Epoch 814/2000
2024-11-11 17:33:57,011 Current Learning Rate: 0.0098795838
2024-11-11 17:33:57,012 Train Loss: 0.0013154, Val Loss: 0.0013275
2024-11-11 17:33:57,012 Epoch 815/2000
2024-11-11 17:34:12,243 Current Learning Rate: 0.0098618496
2024-11-11 17:34:12,244 Train Loss: 0.0012388, Val Loss: 0.0013311
2024-11-11 17:34:12,244 Epoch 816/2000
2024-11-11 17:34:27,673 Current Learning Rate: 0.0098429158
2024-11-11 17:34:27,674 Train Loss: 0.0010880, Val Loss: 0.0012854
2024-11-11 17:34:27,674 Epoch 817/2000
2024-11-11 17:34:43,558 Current Learning Rate: 0.0098227871
2024-11-11 17:34:43,558 Train Loss: 0.0014446, Val Loss: 0.0013267
2024-11-11 17:34:43,559 Epoch 818/2000
2024-11-11 17:34:59,665 Current Learning Rate: 0.0098014684
2024-11-11 17:34:59,665 Train Loss: 0.0012407, Val Loss: 0.0012922
2024-11-11 17:34:59,666 Epoch 819/2000
2024-11-11 17:35:15,079 Current Learning Rate: 0.0097789651
2024-11-11 17:35:15,080 Train Loss: 0.0012259, Val Loss: 0.0012754
2024-11-11 17:35:15,080 Epoch 820/2000
2024-11-11 17:35:30,349 Current Learning Rate: 0.0097552826
2024-11-11 17:35:31,163 Train Loss: 0.0011163, Val Loss: 0.0012624
2024-11-11 17:35:31,163 Epoch 821/2000
2024-11-11 17:35:45,653 Current Learning Rate: 0.0097304268
2024-11-11 17:35:45,653 Train Loss: 0.0012854, Val Loss: 0.0014429
2024-11-11 17:35:45,654 Epoch 822/2000
2024-11-11 17:36:00,763 Current Learning Rate: 0.0097044038
2024-11-11 17:36:00,764 Train Loss: 0.0012318, Val Loss: 0.0013586
2024-11-11 17:36:00,764 Epoch 823/2000
2024-11-11 17:36:16,369 Current Learning Rate: 0.0096772202
2024-11-11 17:36:16,369 Train Loss: 0.0012693, Val Loss: 0.0013378
2024-11-11 17:36:16,370 Epoch 824/2000
2024-11-11 17:36:32,042 Current Learning Rate: 0.0096488824
2024-11-11 17:36:32,042 Train Loss: 0.0012024, Val Loss: 0.0012881
2024-11-11 17:36:32,042 Epoch 825/2000
2024-11-11 17:36:48,104 Current Learning Rate: 0.0096193977
2024-11-11 17:36:48,779 Train Loss: 0.0012591, Val Loss: 0.0012407
2024-11-11 17:36:48,779 Epoch 826/2000
2024-11-11 17:37:04,131 Current Learning Rate: 0.0095887731
2024-11-11 17:37:04,839 Train Loss: 0.0010302, Val Loss: 0.0012064
2024-11-11 17:37:04,839 Epoch 827/2000
2024-11-11 17:37:19,939 Current Learning Rate: 0.0095570164
2024-11-11 17:37:20,693 Train Loss: 0.0011161, Val Loss: 0.0011938
2024-11-11 17:37:20,693 Epoch 828/2000
2024-11-11 17:37:35,742 Current Learning Rate: 0.0095241353
2024-11-11 17:37:35,743 Train Loss: 0.0012396, Val Loss: 0.0013392
2024-11-11 17:37:35,743 Epoch 829/2000
2024-11-11 17:37:51,809 Current Learning Rate: 0.0094901379
2024-11-11 17:37:51,810 Train Loss: 0.0012579, Val Loss: 0.0012809
2024-11-11 17:37:51,810 Epoch 830/2000
2024-11-11 17:38:08,141 Current Learning Rate: 0.0094550326
2024-11-11 17:38:08,141 Train Loss: 0.0010387, Val Loss: 0.0011989
2024-11-11 17:38:08,142 Epoch 831/2000
2024-11-11 17:38:24,454 Current Learning Rate: 0.0094188282
2024-11-11 17:38:24,454 Train Loss: 0.0013556, Val Loss: 0.0013783
2024-11-11 17:38:24,455 Epoch 832/2000
2024-11-11 17:38:41,277 Current Learning Rate: 0.0093815334
2024-11-11 17:38:41,277 Train Loss: 0.0012077, Val Loss: 0.0012947
2024-11-11 17:38:41,277 Epoch 833/2000
2024-11-11 17:38:57,796 Current Learning Rate: 0.0093431576
2024-11-11 17:38:57,796 Train Loss: 0.0013527, Val Loss: 0.0012552
2024-11-11 17:38:57,796 Epoch 834/2000
2024-11-11 17:39:13,801 Current Learning Rate: 0.0093037101
2024-11-11 17:39:13,802 Train Loss: 0.0011967, Val Loss: 0.0011970
2024-11-11 17:39:13,802 Epoch 835/2000
2024-11-11 17:39:30,820 Current Learning Rate: 0.0092632008
2024-11-11 17:39:31,854 Train Loss: 0.0010825, Val Loss: 0.0011737
2024-11-11 17:39:31,854 Epoch 836/2000
2024-11-11 17:39:47,179 Current Learning Rate: 0.0092216396
2024-11-11 17:39:48,045 Train Loss: 0.0010552, Val Loss: 0.0011468
2024-11-11 17:39:48,046 Epoch 837/2000
2024-11-11 17:40:03,981 Current Learning Rate: 0.0091790368
2024-11-11 17:40:04,817 Train Loss: 0.0010035, Val Loss: 0.0011338
2024-11-11 17:40:04,817 Epoch 838/2000
2024-11-11 17:40:20,042 Current Learning Rate: 0.0091354029
2024-11-11 17:40:20,043 Train Loss: 0.0010370, Val Loss: 0.0012440
2024-11-11 17:40:20,044 Epoch 839/2000
2024-11-11 17:40:36,161 Current Learning Rate: 0.0090907486
2024-11-11 17:40:36,162 Train Loss: 0.0010335, Val Loss: 0.0011683
2024-11-11 17:40:36,162 Epoch 840/2000
2024-11-11 17:40:52,126 Current Learning Rate: 0.0090450850
2024-11-11 17:40:52,127 Train Loss: 0.0010770, Val Loss: 0.0011526
2024-11-11 17:40:52,127 Epoch 841/2000
2024-11-11 17:41:07,702 Current Learning Rate: 0.0089984233
2024-11-11 17:41:07,703 Train Loss: 0.0010399, Val Loss: 0.0011815
2024-11-11 17:41:07,703 Epoch 842/2000
2024-11-11 17:41:23,659 Current Learning Rate: 0.0089507751
2024-11-11 17:41:23,659 Train Loss: 0.0011041, Val Loss: 0.0011996
2024-11-11 17:41:23,659 Epoch 843/2000
2024-11-11 17:41:39,361 Current Learning Rate: 0.0089021520
2024-11-11 17:41:39,362 Train Loss: 0.0010959, Val Loss: 0.0011954
2024-11-11 17:41:39,362 Epoch 844/2000
2024-11-11 17:41:54,416 Current Learning Rate: 0.0088525662
2024-11-11 17:41:54,417 Train Loss: 0.0010951, Val Loss: 0.0011798
2024-11-11 17:41:54,417 Epoch 845/2000
2024-11-11 17:42:09,721 Current Learning Rate: 0.0088020298
2024-11-11 17:42:09,721 Train Loss: 0.0010424, Val Loss: 0.0012155
2024-11-11 17:42:09,722 Epoch 846/2000
2024-11-11 17:42:25,807 Current Learning Rate: 0.0087505553
2024-11-11 17:42:25,808 Train Loss: 0.0009852, Val Loss: 0.0011684
2024-11-11 17:42:25,808 Epoch 847/2000
2024-11-11 17:42:41,697 Current Learning Rate: 0.0086981555
2024-11-11 17:42:41,697 Train Loss: 0.0011122, Val Loss: 0.0011711
2024-11-11 17:42:41,698 Epoch 848/2000
2024-11-11 17:42:57,606 Current Learning Rate: 0.0086448431
2024-11-11 17:42:57,607 Train Loss: 0.0009678, Val Loss: 0.0011346
2024-11-11 17:42:57,607 Epoch 849/2000
2024-11-11 17:43:13,178 Current Learning Rate: 0.0085906315
2024-11-11 17:43:13,179 Train Loss: 0.0010650, Val Loss: 0.0011588
2024-11-11 17:43:13,179 Epoch 850/2000
2024-11-11 17:43:28,776 Current Learning Rate: 0.0085355339
2024-11-11 17:43:28,777 Train Loss: 0.0011443, Val Loss: 0.0011940
2024-11-11 17:43:28,777 Epoch 851/2000
2024-11-11 17:43:44,447 Current Learning Rate: 0.0084795640
2024-11-11 17:43:44,448 Train Loss: 0.0011059, Val Loss: 0.0011838
2024-11-11 17:43:44,448 Epoch 852/2000
2024-11-11 17:44:00,672 Current Learning Rate: 0.0084227355
2024-11-11 17:44:00,672 Train Loss: 0.0010949, Val Loss: 0.0011581
2024-11-11 17:44:00,673 Epoch 853/2000
2024-11-11 17:44:16,009 Current Learning Rate: 0.0083650626
2024-11-11 17:44:16,010 Train Loss: 0.0010914, Val Loss: 0.0011674
2024-11-11 17:44:16,010 Epoch 854/2000
2024-11-11 17:44:31,467 Current Learning Rate: 0.0083065593
2024-11-11 17:44:31,468 Train Loss: 0.0010065, Val Loss: 0.0011374
2024-11-11 17:44:31,468 Epoch 855/2000
2024-11-11 17:44:47,646 Current Learning Rate: 0.0082472402
2024-11-11 17:44:48,469 Train Loss: 0.0010591, Val Loss: 0.0011211
2024-11-11 17:44:48,470 Epoch 856/2000
2024-11-11 17:45:04,162 Current Learning Rate: 0.0081871199
2024-11-11 17:45:04,163 Train Loss: 0.0011274, Val Loss: 0.0011416
2024-11-11 17:45:04,163 Epoch 857/2000
2024-11-11 17:45:20,103 Current Learning Rate: 0.0081262133
2024-11-11 17:45:20,104 Train Loss: 0.0010855, Val Loss: 0.0011784
2024-11-11 17:45:20,104 Epoch 858/2000
2024-11-11 17:45:35,618 Current Learning Rate: 0.0080645353
2024-11-11 17:45:35,618 Train Loss: 0.0010290, Val Loss: 0.0012133
2024-11-11 17:45:35,619 Epoch 859/2000
2024-11-11 17:45:51,041 Current Learning Rate: 0.0080021011
2024-11-11 17:45:51,041 Train Loss: 0.0012244, Val Loss: 0.0013354
2024-11-11 17:45:51,042 Epoch 860/2000
2024-11-11 17:46:07,061 Current Learning Rate: 0.0079389263
2024-11-11 17:46:07,062 Train Loss: 0.0010804, Val Loss: 0.0011281
2024-11-11 17:46:07,062 Epoch 861/2000
2024-11-11 17:46:23,099 Current Learning Rate: 0.0078750263
2024-11-11 17:46:24,133 Train Loss: 0.0009456, Val Loss: 0.0011056
2024-11-11 17:46:24,134 Epoch 862/2000
2024-11-11 17:46:39,370 Current Learning Rate: 0.0078104169
2024-11-11 17:46:39,371 Train Loss: 0.0009509, Val Loss: 0.0011377
2024-11-11 17:46:39,371 Epoch 863/2000
2024-11-11 17:46:54,790 Current Learning Rate: 0.0077451141
2024-11-11 17:46:55,573 Train Loss: 0.0009931, Val Loss: 0.0010930
2024-11-11 17:46:55,573 Epoch 864/2000
2024-11-11 17:47:10,014 Current Learning Rate: 0.0076791340
2024-11-11 17:47:10,015 Train Loss: 0.0010525, Val Loss: 0.0011675
2024-11-11 17:47:10,015 Epoch 865/2000
2024-11-11 17:47:25,041 Current Learning Rate: 0.0076124928
2024-11-11 17:47:25,042 Train Loss: 0.0011514, Val Loss: 0.0012510
2024-11-11 17:47:25,042 Epoch 866/2000
2024-11-11 17:47:39,866 Current Learning Rate: 0.0075452071
2024-11-11 17:47:39,866 Train Loss: 0.0011009, Val Loss: 0.0012376
2024-11-11 17:47:39,867 Epoch 867/2000
2024-11-11 17:47:55,423 Current Learning Rate: 0.0074772933
2024-11-11 17:47:55,424 Train Loss: 0.0010050, Val Loss: 0.0011109
2024-11-11 17:47:55,424 Epoch 868/2000
2024-11-11 17:48:10,209 Current Learning Rate: 0.0074087684
2024-11-11 17:48:10,209 Train Loss: 0.0010858, Val Loss: 0.0011313
2024-11-11 17:48:10,209 Epoch 869/2000
2024-11-11 17:48:25,503 Current Learning Rate: 0.0073396491
2024-11-11 17:48:26,213 Train Loss: 0.0009453, Val Loss: 0.0010767
2024-11-11 17:48:26,213 Epoch 870/2000
2024-11-11 17:48:41,576 Current Learning Rate: 0.0072699525
2024-11-11 17:48:41,577 Train Loss: 0.0009463, Val Loss: 0.0011096
2024-11-11 17:48:41,577 Epoch 871/2000
2024-11-11 17:48:56,868 Current Learning Rate: 0.0071996958
2024-11-11 17:48:57,665 Train Loss: 0.0008880, Val Loss: 0.0010729
2024-11-11 17:48:57,665 Epoch 872/2000
2024-11-11 17:49:12,267 Current Learning Rate: 0.0071288965
2024-11-11 17:49:13,042 Train Loss: 0.0009114, Val Loss: 0.0010709
2024-11-11 17:49:13,043 Epoch 873/2000
2024-11-11 17:49:27,643 Current Learning Rate: 0.0070575718
2024-11-11 17:49:28,404 Train Loss: 0.0009138, Val Loss: 0.0010572
2024-11-11 17:49:28,405 Epoch 874/2000
2024-11-11 17:49:43,480 Current Learning Rate: 0.0069857395
2024-11-11 17:49:43,481 Train Loss: 0.0009576, Val Loss: 0.0010831
2024-11-11 17:49:43,481 Epoch 875/2000
2024-11-11 17:49:58,731 Current Learning Rate: 0.0069134172
2024-11-11 17:49:58,731 Train Loss: 0.0011312, Val Loss: 0.0011140
2024-11-11 17:49:58,732 Epoch 876/2000
2024-11-11 17:50:14,029 Current Learning Rate: 0.0068406228
2024-11-11 17:50:14,029 Train Loss: 0.0009896, Val Loss: 0.0011163
2024-11-11 17:50:14,029 Epoch 877/2000
2024-11-11 17:50:31,767 Current Learning Rate: 0.0067673742
2024-11-11 17:50:31,767 Train Loss: 0.0009746, Val Loss: 0.0010994
2024-11-11 17:50:31,767 Epoch 878/2000
2024-11-11 17:50:47,210 Current Learning Rate: 0.0066936896
2024-11-11 17:50:47,210 Train Loss: 0.0010055, Val Loss: 0.0011662
2024-11-11 17:50:47,211 Epoch 879/2000
2024-11-11 17:51:02,431 Current Learning Rate: 0.0066195871
2024-11-11 17:51:02,432 Train Loss: 0.0010542, Val Loss: 0.0011160
2024-11-11 17:51:02,432 Epoch 880/2000
2024-11-11 17:51:18,395 Current Learning Rate: 0.0065450850
2024-11-11 17:51:18,395 Train Loss: 0.0009923, Val Loss: 0.0011051
2024-11-11 17:51:18,395 Epoch 881/2000
2024-11-11 17:51:34,914 Current Learning Rate: 0.0064702016
2024-11-11 17:51:34,914 Train Loss: 0.0008751, Val Loss: 0.0010801
2024-11-11 17:51:34,914 Epoch 882/2000
2024-11-11 17:51:50,837 Current Learning Rate: 0.0063949555
2024-11-11 17:51:50,838 Train Loss: 0.0009270, Val Loss: 0.0010854
2024-11-11 17:51:50,838 Epoch 883/2000
2024-11-11 17:52:06,233 Current Learning Rate: 0.0063193652
2024-11-11 17:52:06,234 Train Loss: 0.0009508, Val Loss: 0.0011039
2024-11-11 17:52:06,234 Epoch 884/2000
2024-11-11 17:52:21,901 Current Learning Rate: 0.0062434494
2024-11-11 17:52:21,901 Train Loss: 0.0010031, Val Loss: 0.0010686
2024-11-11 17:52:21,902 Epoch 885/2000
2024-11-11 17:52:37,634 Current Learning Rate: 0.0061672268
2024-11-11 17:52:37,635 Train Loss: 0.0010492, Val Loss: 0.0010652
2024-11-11 17:52:37,635 Epoch 886/2000
2024-11-11 17:52:53,867 Current Learning Rate: 0.0060907162
2024-11-11 17:52:54,606 Train Loss: 0.0009808, Val Loss: 0.0010377
2024-11-11 17:52:54,607 Epoch 887/2000
2024-11-11 17:53:10,104 Current Learning Rate: 0.0060139365
2024-11-11 17:53:10,911 Train Loss: 0.0009252, Val Loss: 0.0010329
2024-11-11 17:53:10,911 Epoch 888/2000
2024-11-11 17:53:25,175 Current Learning Rate: 0.0059369066
2024-11-11 17:53:25,176 Train Loss: 0.0009400, Val Loss: 0.0010599
2024-11-11 17:53:25,176 Epoch 889/2000
2024-11-11 17:53:41,353 Current Learning Rate: 0.0058596455
2024-11-11 17:53:41,353 Train Loss: 0.0010006, Val Loss: 0.0011004
2024-11-11 17:53:41,354 Epoch 890/2000
2024-11-11 17:53:57,976 Current Learning Rate: 0.0057821723
2024-11-11 17:53:57,976 Train Loss: 0.0008872, Val Loss: 0.0010590
2024-11-11 17:53:57,976 Epoch 891/2000
2024-11-11 17:54:13,476 Current Learning Rate: 0.0057045062
2024-11-11 17:54:14,595 Train Loss: 0.0009031, Val Loss: 0.0010181
2024-11-11 17:54:14,596 Epoch 892/2000
2024-11-11 17:54:30,074 Current Learning Rate: 0.0056266662
2024-11-11 17:54:30,818 Train Loss: 0.0008798, Val Loss: 0.0010091
2024-11-11 17:54:30,818 Epoch 893/2000
2024-11-11 17:54:45,569 Current Learning Rate: 0.0055486716
2024-11-11 17:54:45,570 Train Loss: 0.0008149, Val Loss: 0.0010137
2024-11-11 17:54:45,570 Epoch 894/2000
2024-11-11 17:55:01,152 Current Learning Rate: 0.0054705416
2024-11-11 17:55:01,152 Train Loss: 0.0009229, Val Loss: 0.0010990
2024-11-11 17:55:01,152 Epoch 895/2000
2024-11-11 17:55:16,684 Current Learning Rate: 0.0053922955
2024-11-11 17:55:16,685 Train Loss: 0.0008864, Val Loss: 0.0010769
2024-11-11 17:55:16,685 Epoch 896/2000
2024-11-11 17:55:32,393 Current Learning Rate: 0.0053139526
2024-11-11 17:55:32,394 Train Loss: 0.0008386, Val Loss: 0.0010794
2024-11-11 17:55:32,394 Epoch 897/2000
2024-11-11 17:55:48,947 Current Learning Rate: 0.0052355323
2024-11-11 17:55:48,948 Train Loss: 0.0009225, Val Loss: 0.0010566
2024-11-11 17:55:48,948 Epoch 898/2000
2024-11-11 17:56:05,724 Current Learning Rate: 0.0051570538
2024-11-11 17:56:05,725 Train Loss: 0.0009125, Val Loss: 0.0010158
2024-11-11 17:56:05,725 Epoch 899/2000
2024-11-11 17:56:22,070 Current Learning Rate: 0.0050785366
2024-11-11 17:56:22,070 Train Loss: 0.0010058, Val Loss: 0.0010308
2024-11-11 17:56:22,071 Epoch 900/2000
2024-11-11 17:56:38,467 Current Learning Rate: 0.0050000000
2024-11-11 17:56:38,467 Train Loss: 0.0010355, Val Loss: 0.0010150
2024-11-11 17:56:38,468 Epoch 901/2000
2024-11-11 17:56:54,282 Current Learning Rate: 0.0049214634
2024-11-11 17:56:54,282 Train Loss: 0.0009709, Val Loss: 0.0010411
2024-11-11 17:56:54,282 Epoch 902/2000
2024-11-11 17:57:10,396 Current Learning Rate: 0.0048429462
2024-11-11 17:57:11,133 Train Loss: 0.0008987, Val Loss: 0.0009861
2024-11-11 17:57:11,133 Epoch 903/2000
2024-11-11 17:57:26,603 Current Learning Rate: 0.0047644677
2024-11-11 17:57:27,392 Train Loss: 0.0008029, Val Loss: 0.0009636
2024-11-11 17:57:27,393 Epoch 904/2000
2024-11-11 17:57:42,786 Current Learning Rate: 0.0046860474
2024-11-11 17:57:42,786 Train Loss: 0.0009061, Val Loss: 0.0009678
2024-11-11 17:57:42,787 Epoch 905/2000
2024-11-11 17:57:58,751 Current Learning Rate: 0.0046077045
2024-11-11 17:57:58,751 Train Loss: 0.0008727, Val Loss: 0.0009731
2024-11-11 17:57:58,751 Epoch 906/2000
2024-11-11 17:58:14,351 Current Learning Rate: 0.0045294584
2024-11-11 17:58:14,352 Train Loss: 0.0008415, Val Loss: 0.0010212
2024-11-11 17:58:14,352 Epoch 907/2000
2024-11-11 17:58:29,628 Current Learning Rate: 0.0044513284
2024-11-11 17:58:29,628 Train Loss: 0.0008876, Val Loss: 0.0009804
2024-11-11 17:58:29,628 Epoch 908/2000
2024-11-11 17:58:45,530 Current Learning Rate: 0.0043733338
2024-11-11 17:58:45,531 Train Loss: 0.0009508, Val Loss: 0.0009787
2024-11-11 17:58:45,531 Epoch 909/2000
2024-11-11 17:59:01,382 Current Learning Rate: 0.0042954938
2024-11-11 17:59:04,223 Train Loss: 0.0009103, Val Loss: 0.0009574
2024-11-11 17:59:04,223 Epoch 910/2000
2024-11-11 17:59:19,515 Current Learning Rate: 0.0042178277
2024-11-11 17:59:19,516 Train Loss: 0.0008181, Val Loss: 0.0009654
2024-11-11 17:59:19,516 Epoch 911/2000
2024-11-11 17:59:35,741 Current Learning Rate: 0.0041403545
2024-11-11 17:59:35,741 Train Loss: 0.0009086, Val Loss: 0.0009607
2024-11-11 17:59:35,762 Epoch 912/2000
2024-11-11 17:59:50,502 Current Learning Rate: 0.0040630934
2024-11-11 17:59:51,211 Train Loss: 0.0008478, Val Loss: 0.0009561
2024-11-11 17:59:51,211 Epoch 913/2000
2024-11-11 18:00:06,437 Current Learning Rate: 0.0039860635
2024-11-11 18:00:06,438 Train Loss: 0.0008787, Val Loss: 0.0009568
2024-11-11 18:00:06,438 Epoch 914/2000
2024-11-11 18:00:22,238 Current Learning Rate: 0.0039092838
2024-11-11 18:00:23,027 Train Loss: 0.0008378, Val Loss: 0.0009512
2024-11-11 18:00:23,027 Epoch 915/2000
2024-11-11 18:00:39,011 Current Learning Rate: 0.0038327732
2024-11-11 18:00:39,882 Train Loss: 0.0008433, Val Loss: 0.0009452
2024-11-11 18:00:39,882 Epoch 916/2000
2024-11-11 18:00:55,021 Current Learning Rate: 0.0037565506
2024-11-11 18:00:55,869 Train Loss: 0.0008674, Val Loss: 0.0009419
2024-11-11 18:00:55,870 Epoch 917/2000
2024-11-11 18:01:10,901 Current Learning Rate: 0.0036806348
2024-11-11 18:01:12,011 Train Loss: 0.0007995, Val Loss: 0.0009365
2024-11-11 18:01:12,012 Epoch 918/2000
2024-11-11 18:01:28,396 Current Learning Rate: 0.0036050445
2024-11-11 18:01:29,410 Train Loss: 0.0008046, Val Loss: 0.0009340
2024-11-11 18:01:29,410 Epoch 919/2000
2024-11-11 18:01:45,619 Current Learning Rate: 0.0035297984
2024-11-11 18:01:45,619 Train Loss: 0.0009378, Val Loss: 0.0009402
2024-11-11 18:01:45,620 Epoch 920/2000
2024-11-11 18:02:01,212 Current Learning Rate: 0.0034549150
2024-11-11 18:02:01,213 Train Loss: 0.0008981, Val Loss: 0.0009435
2024-11-11 18:02:01,213 Epoch 921/2000
2024-11-11 18:02:18,387 Current Learning Rate: 0.0033804129
2024-11-11 18:02:18,388 Train Loss: 0.0009236, Val Loss: 0.0009714
2024-11-11 18:02:18,388 Epoch 922/2000
2024-11-11 18:02:34,038 Current Learning Rate: 0.0033063104
2024-11-11 18:02:34,039 Train Loss: 0.0007968, Val Loss: 0.0010329
2024-11-11 18:02:34,040 Epoch 923/2000
2024-11-11 18:02:49,900 Current Learning Rate: 0.0032326258
2024-11-11 18:02:49,901 Train Loss: 0.0008474, Val Loss: 0.0009551
2024-11-11 18:02:49,901 Epoch 924/2000
2024-11-11 18:03:05,886 Current Learning Rate: 0.0031593772
2024-11-11 18:03:05,887 Train Loss: 0.0008605, Val Loss: 0.0009423
2024-11-11 18:03:05,887 Epoch 925/2000
2024-11-11 18:03:22,163 Current Learning Rate: 0.0030865828
2024-11-11 18:03:22,164 Train Loss: 0.0007879, Val Loss: 0.0009484
2024-11-11 18:03:22,164 Epoch 926/2000
2024-11-11 18:03:37,523 Current Learning Rate: 0.0030142605
2024-11-11 18:03:37,523 Train Loss: 0.0007850, Val Loss: 0.0009422
2024-11-11 18:03:37,523 Epoch 927/2000
2024-11-11 18:03:53,902 Current Learning Rate: 0.0029424282
2024-11-11 18:03:53,902 Train Loss: 0.0008499, Val Loss: 0.0009353
2024-11-11 18:03:53,903 Epoch 928/2000
2024-11-11 18:04:09,285 Current Learning Rate: 0.0028711035
2024-11-11 18:04:10,081 Train Loss: 0.0007770, Val Loss: 0.0009291
2024-11-11 18:04:10,082 Epoch 929/2000
2024-11-11 18:04:25,290 Current Learning Rate: 0.0028003042
2024-11-11 18:04:26,011 Train Loss: 0.0007953, Val Loss: 0.0009265
2024-11-11 18:04:26,011 Epoch 930/2000
2024-11-11 18:04:41,158 Current Learning Rate: 0.0027300475
2024-11-11 18:04:41,159 Train Loss: 0.0008035, Val Loss: 0.0009290
2024-11-11 18:04:41,159 Epoch 931/2000
2024-11-11 18:04:56,901 Current Learning Rate: 0.0026603509
2024-11-11 18:04:57,656 Train Loss: 0.0008269, Val Loss: 0.0009218
2024-11-11 18:04:57,656 Epoch 932/2000
2024-11-11 18:05:12,502 Current Learning Rate: 0.0025912316
2024-11-11 18:05:13,290 Train Loss: 0.0008099, Val Loss: 0.0009216
2024-11-11 18:05:13,290 Epoch 933/2000
2024-11-11 18:05:27,932 Current Learning Rate: 0.0025227067
2024-11-11 18:05:28,759 Train Loss: 0.0007953, Val Loss: 0.0009200
2024-11-11 18:05:28,759 Epoch 934/2000
2024-11-11 18:05:44,188 Current Learning Rate: 0.0024547929
2024-11-11 18:05:44,189 Train Loss: 0.0008011, Val Loss: 0.0009216
2024-11-11 18:05:44,189 Epoch 935/2000
2024-11-11 18:06:00,530 Current Learning Rate: 0.0023875072
2024-11-11 18:06:01,545 Train Loss: 0.0007263, Val Loss: 0.0009178
2024-11-11 18:06:01,545 Epoch 936/2000
2024-11-11 18:06:16,635 Current Learning Rate: 0.0023208660
2024-11-11 18:06:17,452 Train Loss: 0.0007329, Val Loss: 0.0009140
2024-11-11 18:06:17,452 Epoch 937/2000
2024-11-11 18:06:32,328 Current Learning Rate: 0.0022548859
2024-11-11 18:06:33,096 Train Loss: 0.0007991, Val Loss: 0.0009119
2024-11-11 18:06:33,096 Epoch 938/2000
2024-11-11 18:06:49,252 Current Learning Rate: 0.0021895831
2024-11-11 18:06:49,985 Train Loss: 0.0007170, Val Loss: 0.0009097
2024-11-11 18:06:49,986 Epoch 939/2000
2024-11-11 18:07:04,742 Current Learning Rate: 0.0021249737
2024-11-11 18:07:04,743 Train Loss: 0.0008002, Val Loss: 0.0009106
2024-11-11 18:07:04,743 Epoch 940/2000
2024-11-11 18:07:20,276 Current Learning Rate: 0.0020610737
2024-11-11 18:07:21,109 Train Loss: 0.0007906, Val Loss: 0.0009069
2024-11-11 18:07:21,109 Epoch 941/2000
2024-11-11 18:07:36,298 Current Learning Rate: 0.0019978989
2024-11-11 18:07:36,299 Train Loss: 0.0008600, Val Loss: 0.0009077
2024-11-11 18:07:36,300 Epoch 942/2000
2024-11-11 18:07:51,999 Current Learning Rate: 0.0019354647
2024-11-11 18:07:52,859 Train Loss: 0.0008623, Val Loss: 0.0009002
2024-11-11 18:07:52,859 Epoch 943/2000
2024-11-11 18:08:08,792 Current Learning Rate: 0.0018737867
2024-11-11 18:08:09,871 Train Loss: 0.0007317, Val Loss: 0.0008945
2024-11-11 18:08:09,871 Epoch 944/2000
2024-11-11 18:08:25,556 Current Learning Rate: 0.0018128801
2024-11-11 18:08:26,498 Train Loss: 0.0007197, Val Loss: 0.0008931
2024-11-11 18:08:26,499 Epoch 945/2000
2024-11-11 18:08:42,506 Current Learning Rate: 0.0017527598
2024-11-11 18:08:43,408 Train Loss: 0.0008533, Val Loss: 0.0008858
2024-11-11 18:08:43,408 Epoch 946/2000
2024-11-11 18:08:59,082 Current Learning Rate: 0.0016934407
2024-11-11 18:08:59,864 Train Loss: 0.0007981, Val Loss: 0.0008846
2024-11-11 18:08:59,864 Epoch 947/2000
2024-11-11 18:09:14,839 Current Learning Rate: 0.0016349374
2024-11-11 18:09:15,885 Train Loss: 0.0007385, Val Loss: 0.0008841
2024-11-11 18:09:15,886 Epoch 948/2000
2024-11-11 18:09:32,184 Current Learning Rate: 0.0015772645
2024-11-11 18:09:33,217 Train Loss: 0.0007402, Val Loss: 0.0008811
2024-11-11 18:09:33,217 Epoch 949/2000
2024-11-11 18:09:48,867 Current Learning Rate: 0.0015204360
2024-11-11 18:09:49,828 Train Loss: 0.0007388, Val Loss: 0.0008791
2024-11-11 18:09:49,828 Epoch 950/2000
2024-11-11 18:10:05,394 Current Learning Rate: 0.0014644661
2024-11-11 18:10:06,428 Train Loss: 0.0007365, Val Loss: 0.0008788
2024-11-11 18:10:06,428 Epoch 951/2000
2024-11-11 18:10:22,291 Current Learning Rate: 0.0014093685
2024-11-11 18:10:23,337 Train Loss: 0.0008304, Val Loss: 0.0008780
2024-11-11 18:10:23,337 Epoch 952/2000
2024-11-11 18:10:39,622 Current Learning Rate: 0.0013551569
2024-11-11 18:10:40,525 Train Loss: 0.0007347, Val Loss: 0.0008769
2024-11-11 18:10:40,526 Epoch 953/2000
2024-11-11 18:10:56,240 Current Learning Rate: 0.0013018445
2024-11-11 18:10:57,262 Train Loss: 0.0007638, Val Loss: 0.0008759
2024-11-11 18:10:57,262 Epoch 954/2000
2024-11-11 18:11:13,149 Current Learning Rate: 0.0012494447
2024-11-11 18:11:13,942 Train Loss: 0.0006930, Val Loss: 0.0008750
2024-11-11 18:11:13,943 Epoch 955/2000
2024-11-11 18:11:29,040 Current Learning Rate: 0.0011979702
2024-11-11 18:11:30,024 Train Loss: 0.0007504, Val Loss: 0.0008742
2024-11-11 18:11:30,024 Epoch 956/2000
2024-11-11 18:11:46,069 Current Learning Rate: 0.0011474338
2024-11-11 18:11:46,819 Train Loss: 0.0007186, Val Loss: 0.0008726
2024-11-11 18:11:46,820 Epoch 957/2000
2024-11-11 18:12:02,281 Current Learning Rate: 0.0010978480
2024-11-11 18:12:04,771 Train Loss: 0.0008264, Val Loss: 0.0008722
2024-11-11 18:12:04,771 Epoch 958/2000
2024-11-11 18:12:20,546 Current Learning Rate: 0.0010492249
2024-11-11 18:12:21,276 Train Loss: 0.0008291, Val Loss: 0.0008717
2024-11-11 18:12:21,276 Epoch 959/2000
2024-11-11 18:12:36,206 Current Learning Rate: 0.0010015767
2024-11-11 18:12:37,073 Train Loss: 0.0007533, Val Loss: 0.0008700
2024-11-11 18:12:37,073 Epoch 960/2000
2024-11-11 18:12:51,389 Current Learning Rate: 0.0009549150
2024-11-11 18:12:51,389 Train Loss: 0.0008044, Val Loss: 0.0008701
2024-11-11 18:12:51,390 Epoch 961/2000
2024-11-11 18:13:06,881 Current Learning Rate: 0.0009092514
2024-11-11 18:13:06,881 Train Loss: 0.0009497, Val Loss: 0.0008702
2024-11-11 18:13:06,882 Epoch 962/2000
2024-11-11 18:13:22,443 Current Learning Rate: 0.0008645971
2024-11-11 18:13:23,260 Train Loss: 0.0007587, Val Loss: 0.0008681
2024-11-11 18:13:23,261 Epoch 963/2000
2024-11-11 18:13:38,024 Current Learning Rate: 0.0008209632
2024-11-11 18:13:38,840 Train Loss: 0.0007858, Val Loss: 0.0008676
2024-11-11 18:13:38,840 Epoch 964/2000
2024-11-11 18:13:53,426 Current Learning Rate: 0.0007783604
2024-11-11 18:13:54,195 Train Loss: 0.0007155, Val Loss: 0.0008664
2024-11-11 18:13:54,195 Epoch 965/2000
2024-11-11 18:14:09,379 Current Learning Rate: 0.0007367992
2024-11-11 18:14:10,408 Train Loss: 0.0008063, Val Loss: 0.0008662
2024-11-11 18:14:10,409 Epoch 966/2000
2024-11-11 18:14:26,624 Current Learning Rate: 0.0006962899
2024-11-11 18:14:27,655 Train Loss: 0.0007275, Val Loss: 0.0008653
2024-11-11 18:14:27,655 Epoch 967/2000
2024-11-11 18:14:43,990 Current Learning Rate: 0.0006568424
2024-11-11 18:14:45,042 Train Loss: 0.0007194, Val Loss: 0.0008642
2024-11-11 18:14:45,043 Epoch 968/2000
2024-11-11 18:15:00,679 Current Learning Rate: 0.0006184666
2024-11-11 18:15:01,587 Train Loss: 0.0006976, Val Loss: 0.0008634
2024-11-11 18:15:01,588 Epoch 969/2000
2024-11-11 18:15:16,646 Current Learning Rate: 0.0005811718
2024-11-11 18:15:17,453 Train Loss: 0.0007452, Val Loss: 0.0008625
2024-11-11 18:15:17,454 Epoch 970/2000
2024-11-11 18:15:31,720 Current Learning Rate: 0.0005449674
2024-11-11 18:15:32,505 Train Loss: 0.0007519, Val Loss: 0.0008623
2024-11-11 18:15:32,505 Epoch 971/2000
2024-11-11 18:15:47,098 Current Learning Rate: 0.0005098621
2024-11-11 18:15:47,849 Train Loss: 0.0006804, Val Loss: 0.0008617
2024-11-11 18:15:47,850 Epoch 972/2000
2024-11-11 18:16:02,872 Current Learning Rate: 0.0004758647
2024-11-11 18:16:05,217 Train Loss: 0.0007153, Val Loss: 0.0008612
2024-11-11 18:16:05,217 Epoch 973/2000
2024-11-11 18:16:19,452 Current Learning Rate: 0.0004429836
2024-11-11 18:16:20,279 Train Loss: 0.0006899, Val Loss: 0.0008603
2024-11-11 18:16:20,280 Epoch 974/2000
2024-11-11 18:16:34,776 Current Learning Rate: 0.0004112269
2024-11-11 18:16:35,585 Train Loss: 0.0008203, Val Loss: 0.0008598
2024-11-11 18:16:35,585 Epoch 975/2000
2024-11-11 18:16:50,092 Current Learning Rate: 0.0003806023
2024-11-11 18:16:50,869 Train Loss: 0.0007900, Val Loss: 0.0008593
2024-11-11 18:16:50,870 Epoch 976/2000
2024-11-11 18:17:05,491 Current Learning Rate: 0.0003511176
2024-11-11 18:17:06,261 Train Loss: 0.0007146, Val Loss: 0.0008588
2024-11-11 18:17:06,261 Epoch 977/2000
2024-11-11 18:17:20,973 Current Learning Rate: 0.0003227798
2024-11-11 18:17:21,749 Train Loss: 0.0007156, Val Loss: 0.0008587
2024-11-11 18:17:21,749 Epoch 978/2000
2024-11-11 18:17:36,300 Current Learning Rate: 0.0002955962
2024-11-11 18:17:36,301 Train Loss: 0.0008374, Val Loss: 0.0008588
2024-11-11 18:17:36,301 Epoch 979/2000
2024-11-11 18:17:51,839 Current Learning Rate: 0.0002695732
2024-11-11 18:17:52,660 Train Loss: 0.0008155, Val Loss: 0.0008584
2024-11-11 18:17:52,661 Epoch 980/2000
2024-11-11 18:18:07,838 Current Learning Rate: 0.0002447174
2024-11-11 18:18:08,876 Train Loss: 0.0007135, Val Loss: 0.0008578
2024-11-11 18:18:08,877 Epoch 981/2000
2024-11-11 18:18:25,298 Current Learning Rate: 0.0002210349
2024-11-11 18:18:26,311 Train Loss: 0.0007606, Val Loss: 0.0008577
2024-11-11 18:18:26,311 Epoch 982/2000
2024-11-11 18:18:42,014 Current Learning Rate: 0.0001985316
2024-11-11 18:18:43,048 Train Loss: 0.0007418, Val Loss: 0.0008576
2024-11-11 18:18:43,048 Epoch 983/2000
2024-11-11 18:18:58,550 Current Learning Rate: 0.0001772129
2024-11-11 18:18:58,551 Train Loss: 0.0007499, Val Loss: 0.0008576
2024-11-11 18:18:58,551 Epoch 984/2000
2024-11-11 18:19:14,874 Current Learning Rate: 0.0001570842
2024-11-11 18:19:15,859 Train Loss: 0.0007082, Val Loss: 0.0008571
2024-11-11 18:19:15,860 Epoch 985/2000
2024-11-11 18:19:31,580 Current Learning Rate: 0.0001381504
2024-11-11 18:19:32,311 Train Loss: 0.0007549, Val Loss: 0.0008567
2024-11-11 18:19:32,311 Epoch 986/2000
2024-11-11 18:19:46,666 Current Learning Rate: 0.0001204162
2024-11-11 18:19:47,473 Train Loss: 0.0007097, Val Loss: 0.0008566
2024-11-11 18:19:47,473 Epoch 987/2000
2024-11-11 18:20:02,133 Current Learning Rate: 0.0001038859
2024-11-11 18:20:04,571 Train Loss: 0.0007165, Val Loss: 0.0008563
2024-11-11 18:20:04,572 Epoch 988/2000
2024-11-11 18:20:18,843 Current Learning Rate: 0.0000885637
2024-11-11 18:20:18,843 Train Loss: 0.0007841, Val Loss: 0.0008564
2024-11-11 18:20:18,843 Epoch 989/2000
2024-11-11 18:20:34,100 Current Learning Rate: 0.0000744534
2024-11-11 18:20:34,857 Train Loss: 0.0007812, Val Loss: 0.0008561
2024-11-11 18:20:34,857 Epoch 990/2000
2024-11-11 18:20:50,019 Current Learning Rate: 0.0000615583
2024-11-11 18:20:50,019 Train Loss: 0.0007407, Val Loss: 0.0008561
2024-11-11 18:20:50,019 Epoch 991/2000
2024-11-11 18:21:05,671 Current Learning Rate: 0.0000498817
2024-11-11 18:21:06,509 Train Loss: 0.0007416, Val Loss: 0.0008561
2024-11-11 18:21:06,509 Epoch 992/2000
2024-11-11 18:21:21,475 Current Learning Rate: 0.0000394265
2024-11-11 18:21:22,260 Train Loss: 0.0007115, Val Loss: 0.0008559
2024-11-11 18:21:22,261 Epoch 993/2000
2024-11-11 18:21:36,963 Current Learning Rate: 0.0000301952
2024-11-11 18:21:37,822 Train Loss: 0.0007434, Val Loss: 0.0008559
2024-11-11 18:21:37,822 Epoch 994/2000
2024-11-11 18:21:52,460 Current Learning Rate: 0.0000221902
2024-11-11 18:21:52,460 Train Loss: 0.0007455, Val Loss: 0.0008559
2024-11-11 18:21:52,461 Epoch 995/2000
2024-11-11 18:22:07,811 Current Learning Rate: 0.0000154133
2024-11-11 18:22:08,572 Train Loss: 0.0008028, Val Loss: 0.0008559
2024-11-11 18:22:08,573 Epoch 996/2000
2024-11-11 18:22:23,237 Current Learning Rate: 0.0000098664
2024-11-11 18:22:24,004 Train Loss: 0.0008328, Val Loss: 0.0008558
2024-11-11 18:22:24,004 Epoch 997/2000
2024-11-11 18:22:39,122 Current Learning Rate: 0.0000055506
2024-11-11 18:22:39,124 Train Loss: 0.0008706, Val Loss: 0.0008559
2024-11-11 18:22:39,124 Epoch 998/2000
2024-11-11 18:22:55,385 Current Learning Rate: 0.0000024672
2024-11-11 18:22:55,386 Train Loss: 0.0008158, Val Loss: 0.0008558
2024-11-11 18:22:55,386 Epoch 999/2000
2024-11-11 18:23:11,204 Current Learning Rate: 0.0000006168
2024-11-11 18:23:11,941 Train Loss: 0.0007491, Val Loss: 0.0008558
2024-11-11 18:23:11,941 Epoch 1000/2000
2024-11-11 18:23:26,214 Current Learning Rate: 0.0000000000
2024-11-11 18:23:26,991 Train Loss: 0.0007738, Val Loss: 0.0008558
2024-11-11 18:23:26,992 Epoch 1001/2000
2024-11-11 18:23:41,600 Current Learning Rate: 0.0000006168
2024-11-11 18:23:41,601 Train Loss: 0.0007041, Val Loss: 0.0008558
2024-11-11 18:23:41,601 Epoch 1002/2000
2024-11-11 18:23:57,062 Current Learning Rate: 0.0000024672
2024-11-11 18:23:57,062 Train Loss: 0.0007447, Val Loss: 0.0008558
2024-11-11 18:23:57,062 Epoch 1003/2000
2024-11-11 18:24:12,435 Current Learning Rate: 0.0000055506
2024-11-11 18:24:13,188 Train Loss: 0.0007138, Val Loss: 0.0008558
2024-11-11 18:24:13,189 Epoch 1004/2000
2024-11-11 18:24:27,879 Current Learning Rate: 0.0000098664
2024-11-11 18:24:28,752 Train Loss: 0.0007444, Val Loss: 0.0008558
2024-11-11 18:24:28,752 Epoch 1005/2000
2024-11-11 18:24:44,092 Current Learning Rate: 0.0000154133
2024-11-11 18:24:44,824 Train Loss: 0.0007624, Val Loss: 0.0008557
2024-11-11 18:24:44,824 Epoch 1006/2000
2024-11-11 18:25:00,032 Current Learning Rate: 0.0000221902
2024-11-11 18:25:00,033 Train Loss: 0.0007101, Val Loss: 0.0008558
2024-11-11 18:25:00,033 Epoch 1007/2000
2024-11-11 18:25:15,982 Current Learning Rate: 0.0000301952
2024-11-11 18:25:15,983 Train Loss: 0.0006978, Val Loss: 0.0008558
2024-11-11 18:25:15,984 Epoch 1008/2000
2024-11-11 18:25:32,062 Current Learning Rate: 0.0000394265
2024-11-11 18:25:32,062 Train Loss: 0.0008121, Val Loss: 0.0008559
2024-11-11 18:25:32,063 Epoch 1009/2000
2024-11-11 18:25:47,703 Current Learning Rate: 0.0000498817
2024-11-11 18:25:47,704 Train Loss: 0.0007845, Val Loss: 0.0008559
2024-11-11 18:25:47,704 Epoch 1010/2000
2024-11-11 18:26:03,148 Current Learning Rate: 0.0000615583
2024-11-11 18:26:03,148 Train Loss: 0.0007137, Val Loss: 0.0008558
2024-11-11 18:26:03,148 Epoch 1011/2000
2024-11-11 18:26:18,618 Current Learning Rate: 0.0000744534
2024-11-11 18:26:18,618 Train Loss: 0.0007413, Val Loss: 0.0008558
2024-11-11 18:26:18,627 Epoch 1012/2000
2024-11-11 18:26:34,565 Current Learning Rate: 0.0000885637
2024-11-11 18:26:34,566 Train Loss: 0.0007160, Val Loss: 0.0008558
2024-11-11 18:26:34,566 Epoch 1013/2000
2024-11-11 18:26:50,242 Current Learning Rate: 0.0001038859
2024-11-11 18:26:50,242 Train Loss: 0.0007124, Val Loss: 0.0008558
2024-11-11 18:26:50,243 Epoch 1014/2000
2024-11-11 18:27:06,393 Current Learning Rate: 0.0001204162
2024-11-11 18:27:06,393 Train Loss: 0.0007834, Val Loss: 0.0008560
2024-11-11 18:27:06,393 Epoch 1015/2000
2024-11-11 18:27:21,665 Current Learning Rate: 0.0001381504
2024-11-11 18:27:21,665 Train Loss: 0.0007124, Val Loss: 0.0008559
2024-11-11 18:27:21,665 Epoch 1016/2000
2024-11-11 18:27:37,500 Current Learning Rate: 0.0001570842
2024-11-11 18:27:37,501 Train Loss: 0.0008102, Val Loss: 0.0008561
2024-11-11 18:27:37,501 Epoch 1017/2000
2024-11-11 18:27:53,433 Current Learning Rate: 0.0001772129
2024-11-11 18:27:53,433 Train Loss: 0.0007366, Val Loss: 0.0008566
2024-11-11 18:27:53,434 Epoch 1018/2000
2024-11-11 18:28:09,970 Current Learning Rate: 0.0001985316
2024-11-11 18:28:09,971 Train Loss: 0.0007153, Val Loss: 0.0008562
2024-11-11 18:28:09,971 Epoch 1019/2000
2024-11-11 18:28:25,920 Current Learning Rate: 0.0002210349
2024-11-11 18:28:25,921 Train Loss: 0.0006716, Val Loss: 0.0008560
2024-11-11 18:28:25,921 Epoch 1020/2000
2024-11-11 18:28:42,797 Current Learning Rate: 0.0002447174
2024-11-11 18:28:42,797 Train Loss: 0.0007545, Val Loss: 0.0008561
2024-11-11 18:28:42,798 Epoch 1021/2000
2024-11-11 18:28:59,024 Current Learning Rate: 0.0002695732
2024-11-11 18:28:59,024 Train Loss: 0.0008152, Val Loss: 0.0008583
2024-11-11 18:28:59,025 Epoch 1022/2000
2024-11-11 18:29:15,344 Current Learning Rate: 0.0002955962
2024-11-11 18:29:15,345 Train Loss: 0.0007431, Val Loss: 0.0008562
2024-11-11 18:29:15,345 Epoch 1023/2000
2024-11-11 18:29:30,976 Current Learning Rate: 0.0003227798
2024-11-11 18:29:30,978 Train Loss: 0.0008182, Val Loss: 0.0008566
2024-11-11 18:29:30,978 Epoch 1024/2000
2024-11-11 18:29:46,644 Current Learning Rate: 0.0003511176
2024-11-11 18:29:46,645 Train Loss: 0.0007122, Val Loss: 0.0008565
2024-11-11 18:29:46,645 Epoch 1025/2000
2024-11-11 18:30:02,054 Current Learning Rate: 0.0003806023
2024-11-11 18:30:02,054 Train Loss: 0.0007820, Val Loss: 0.0008568
2024-11-11 18:30:02,055 Epoch 1026/2000
2024-11-11 18:30:17,443 Current Learning Rate: 0.0004112269
2024-11-11 18:30:17,444 Train Loss: 0.0008278, Val Loss: 0.0008565
2024-11-11 18:30:17,444 Epoch 1027/2000
2024-11-11 18:30:33,751 Current Learning Rate: 0.0004429836
2024-11-11 18:30:33,751 Train Loss: 0.0007511, Val Loss: 0.0008568
2024-11-11 18:30:33,752 Epoch 1028/2000
2024-11-11 18:30:49,862 Current Learning Rate: 0.0004758647
2024-11-11 18:30:49,863 Train Loss: 0.0007972, Val Loss: 0.0008570
2024-11-11 18:30:49,863 Epoch 1029/2000
2024-11-11 18:31:05,444 Current Learning Rate: 0.0005098621
2024-11-11 18:31:05,444 Train Loss: 0.0008241, Val Loss: 0.0008577
2024-11-11 18:31:05,445 Epoch 1030/2000
2024-11-11 18:31:21,490 Current Learning Rate: 0.0005449674
2024-11-11 18:31:21,491 Train Loss: 0.0007440, Val Loss: 0.0008575
2024-11-11 18:31:21,491 Epoch 1031/2000
2024-11-11 18:31:37,003 Current Learning Rate: 0.0005811718
2024-11-11 18:31:37,004 Train Loss: 0.0007159, Val Loss: 0.0008574
2024-11-11 18:31:37,004 Epoch 1032/2000
2024-11-11 18:31:53,886 Current Learning Rate: 0.0006184666
2024-11-11 18:31:53,886 Train Loss: 0.0007748, Val Loss: 0.0008571
2024-11-11 18:31:53,887 Epoch 1033/2000
2024-11-11 18:32:10,155 Current Learning Rate: 0.0006568424
2024-11-11 18:32:10,155 Train Loss: 0.0006881, Val Loss: 0.0008574
2024-11-11 18:32:10,155 Epoch 1034/2000
2024-11-11 18:32:26,939 Current Learning Rate: 0.0006962899
2024-11-11 18:32:26,940 Train Loss: 0.0007358, Val Loss: 0.0008586
2024-11-11 18:32:26,940 Epoch 1035/2000
2024-11-11 18:32:42,501 Current Learning Rate: 0.0007367992
2024-11-11 18:32:42,502 Train Loss: 0.0007446, Val Loss: 0.0008596
2024-11-11 18:32:42,502 Epoch 1036/2000
2024-11-11 18:32:58,778 Current Learning Rate: 0.0007783604
2024-11-11 18:32:58,778 Train Loss: 0.0007498, Val Loss: 0.0008631
2024-11-11 18:32:58,778 Epoch 1037/2000
2024-11-11 18:33:14,752 Current Learning Rate: 0.0008209632
2024-11-11 18:33:14,753 Train Loss: 0.0006730, Val Loss: 0.0008626
2024-11-11 18:33:14,753 Epoch 1038/2000
2024-11-11 18:33:30,112 Current Learning Rate: 0.0008645971
2024-11-11 18:33:30,113 Train Loss: 0.0007084, Val Loss: 0.0008636
2024-11-11 18:33:30,113 Epoch 1039/2000
2024-11-11 18:33:45,602 Current Learning Rate: 0.0009092514
2024-11-11 18:33:45,602 Train Loss: 0.0007876, Val Loss: 0.0008646
2024-11-11 18:33:45,603 Epoch 1040/2000
2024-11-11 18:34:02,321 Current Learning Rate: 0.0009549150
2024-11-11 18:34:02,322 Train Loss: 0.0007497, Val Loss: 0.0008628
2024-11-11 18:34:02,323 Epoch 1041/2000
2024-11-11 18:34:18,006 Current Learning Rate: 0.0010015767
2024-11-11 18:34:18,007 Train Loss: 0.0007301, Val Loss: 0.0008607
2024-11-11 18:34:18,007 Epoch 1042/2000
2024-11-11 18:34:35,079 Current Learning Rate: 0.0010492249
2024-11-11 18:34:35,079 Train Loss: 0.0007282, Val Loss: 0.0008592
2024-11-11 18:34:35,080 Epoch 1043/2000
2024-11-11 18:34:51,134 Current Learning Rate: 0.0010978480
2024-11-11 18:34:52,226 Train Loss: 0.0007161, Val Loss: 0.0008554
2024-11-11 18:34:52,227 Epoch 1044/2000
2024-11-11 18:35:08,413 Current Learning Rate: 0.0011474338
2024-11-11 18:35:08,414 Train Loss: 0.0007101, Val Loss: 0.0008654
2024-11-11 18:35:08,415 Epoch 1045/2000
2024-11-11 18:35:23,793 Current Learning Rate: 0.0011979702
2024-11-11 18:35:23,794 Train Loss: 0.0008156, Val Loss: 0.0008671
2024-11-11 18:35:23,794 Epoch 1046/2000
2024-11-11 18:35:39,908 Current Learning Rate: 0.0012494447
2024-11-11 18:35:39,909 Train Loss: 0.0006861, Val Loss: 0.0008554
2024-11-11 18:35:39,909 Epoch 1047/2000
2024-11-11 18:35:55,655 Current Learning Rate: 0.0013018445
2024-11-11 18:35:55,656 Train Loss: 0.0007644, Val Loss: 0.0008688
2024-11-11 18:35:55,656 Epoch 1048/2000
2024-11-11 18:36:11,428 Current Learning Rate: 0.0013551569
2024-11-11 18:36:11,429 Train Loss: 0.0007607, Val Loss: 0.0008591
2024-11-11 18:36:11,429 Epoch 1049/2000
2024-11-11 18:36:28,192 Current Learning Rate: 0.0014093685
2024-11-11 18:36:28,192 Train Loss: 0.0007149, Val Loss: 0.0008560
2024-11-11 18:36:28,193 Epoch 1050/2000
2024-11-11 18:36:44,685 Current Learning Rate: 0.0014644661
2024-11-11 18:36:44,686 Train Loss: 0.0008344, Val Loss: 0.0008616
2024-11-11 18:36:44,686 Epoch 1051/2000
2024-11-11 18:37:00,530 Current Learning Rate: 0.0015204360
2024-11-11 18:37:00,531 Train Loss: 0.0006756, Val Loss: 0.0008582
2024-11-11 18:37:00,531 Epoch 1052/2000
2024-11-11 18:37:17,346 Current Learning Rate: 0.0015772645
2024-11-11 18:37:17,346 Train Loss: 0.0007516, Val Loss: 0.0008673
2024-11-11 18:37:17,347 Epoch 1053/2000
2024-11-11 18:37:32,817 Current Learning Rate: 0.0016349374
2024-11-11 18:37:32,818 Train Loss: 0.0006806, Val Loss: 0.0008589
2024-11-11 18:37:32,818 Epoch 1054/2000
2024-11-11 18:37:48,166 Current Learning Rate: 0.0016934407
2024-11-11 18:37:48,167 Train Loss: 0.0006719, Val Loss: 0.0008565
2024-11-11 18:37:48,167 Epoch 1055/2000
2024-11-11 18:38:03,475 Current Learning Rate: 0.0017527598
2024-11-11 18:38:03,476 Train Loss: 0.0007959, Val Loss: 0.0008839
2024-11-11 18:38:03,476 Epoch 1056/2000
2024-11-11 18:38:19,173 Current Learning Rate: 0.0018128801
2024-11-11 18:38:19,173 Train Loss: 0.0008522, Val Loss: 0.0009142
2024-11-11 18:38:19,174 Epoch 1057/2000
2024-11-11 18:38:34,421 Current Learning Rate: 0.0018737867
2024-11-11 18:38:34,421 Train Loss: 0.0007066, Val Loss: 0.0008673
2024-11-11 18:38:34,422 Epoch 1058/2000
2024-11-11 18:38:50,048 Current Learning Rate: 0.0019354647
2024-11-11 18:38:50,049 Train Loss: 0.0008207, Val Loss: 0.0009349
2024-11-11 18:38:50,049 Epoch 1059/2000
2024-11-11 18:39:05,891 Current Learning Rate: 0.0019978989
2024-11-11 18:39:05,892 Train Loss: 0.0007879, Val Loss: 0.0008692
2024-11-11 18:39:05,892 Epoch 1060/2000
2024-11-11 18:39:21,306 Current Learning Rate: 0.0020610737
2024-11-11 18:39:21,306 Train Loss: 0.0007605, Val Loss: 0.0008579
2024-11-11 18:39:21,307 Epoch 1061/2000
2024-11-11 18:39:36,695 Current Learning Rate: 0.0021249737
2024-11-11 18:39:37,468 Train Loss: 0.0007083, Val Loss: 0.0008546
2024-11-11 18:39:37,468 Epoch 1062/2000
2024-11-11 18:39:52,103 Current Learning Rate: 0.0021895831
2024-11-11 18:39:52,103 Train Loss: 0.0007383, Val Loss: 0.0008613
2024-11-11 18:39:52,104 Epoch 1063/2000
2024-11-11 18:40:08,285 Current Learning Rate: 0.0022548859
2024-11-11 18:40:09,076 Train Loss: 0.0007759, Val Loss: 0.0008530
2024-11-11 18:40:09,076 Epoch 1064/2000
2024-11-11 18:40:24,478 Current Learning Rate: 0.0023208660
2024-11-11 18:40:24,479 Train Loss: 0.0008380, Val Loss: 0.0009085
2024-11-11 18:40:24,479 Epoch 1065/2000
2024-11-11 18:40:39,403 Current Learning Rate: 0.0023875072
2024-11-11 18:40:39,404 Train Loss: 0.0007227, Val Loss: 0.0008622
2024-11-11 18:40:39,404 Epoch 1066/2000
2024-11-11 18:40:54,462 Current Learning Rate: 0.0024547929
2024-11-11 18:40:54,462 Train Loss: 0.0007269, Val Loss: 0.0008761
2024-11-11 18:40:54,462 Epoch 1067/2000
2024-11-11 18:41:09,654 Current Learning Rate: 0.0025227067
2024-11-11 18:41:09,654 Train Loss: 0.0007267, Val Loss: 0.0008554
2024-11-11 18:41:09,655 Epoch 1068/2000
2024-11-11 18:41:25,103 Current Learning Rate: 0.0025912316
2024-11-11 18:41:25,104 Train Loss: 0.0007440, Val Loss: 0.0008720
2024-11-11 18:41:25,104 Epoch 1069/2000
2024-11-11 18:41:40,784 Current Learning Rate: 0.0026603509
2024-11-11 18:41:40,785 Train Loss: 0.0007707, Val Loss: 0.0008668
2024-11-11 18:41:40,785 Epoch 1070/2000
2024-11-11 18:41:56,935 Current Learning Rate: 0.0027300475
2024-11-11 18:41:56,935 Train Loss: 0.0008534, Val Loss: 0.0008801
2024-11-11 18:41:56,936 Epoch 1071/2000
2024-11-11 18:42:13,057 Current Learning Rate: 0.0028003042
2024-11-11 18:42:13,059 Train Loss: 0.0007330, Val Loss: 0.0008630
2024-11-11 18:42:13,059 Epoch 1072/2000
2024-11-11 18:42:28,888 Current Learning Rate: 0.0028711035
2024-11-11 18:42:28,889 Train Loss: 0.0006813, Val Loss: 0.0008599
2024-11-11 18:42:28,889 Epoch 1073/2000
2024-11-11 18:42:44,922 Current Learning Rate: 0.0029424282
2024-11-11 18:42:44,923 Train Loss: 0.0007425, Val Loss: 0.0009174
2024-11-11 18:42:44,923 Epoch 1074/2000
2024-11-11 18:43:00,446 Current Learning Rate: 0.0030142605
2024-11-11 18:43:00,447 Train Loss: 0.0007606, Val Loss: 0.0008840
2024-11-11 18:43:00,447 Epoch 1075/2000
2024-11-11 18:43:15,730 Current Learning Rate: 0.0030865828
2024-11-11 18:43:15,730 Train Loss: 0.0008498, Val Loss: 0.0008971
2024-11-11 18:43:15,730 Epoch 1076/2000
2024-11-11 18:43:31,459 Current Learning Rate: 0.0031593772
2024-11-11 18:43:31,460 Train Loss: 0.0008042, Val Loss: 0.0009118
2024-11-11 18:43:31,460 Epoch 1077/2000
2024-11-11 18:43:47,950 Current Learning Rate: 0.0032326258
2024-11-11 18:43:47,951 Train Loss: 0.0007869, Val Loss: 0.0008806
2024-11-11 18:43:47,951 Epoch 1078/2000
2024-11-11 18:44:02,884 Current Learning Rate: 0.0033063104
2024-11-11 18:44:02,885 Train Loss: 0.0007446, Val Loss: 0.0008635
2024-11-11 18:44:02,885 Epoch 1079/2000
2024-11-11 18:44:18,644 Current Learning Rate: 0.0033804129
2024-11-11 18:44:18,644 Train Loss: 0.0008346, Val Loss: 0.0008997
2024-11-11 18:44:18,645 Epoch 1080/2000
2024-11-11 18:44:35,632 Current Learning Rate: 0.0034549150
2024-11-11 18:44:35,633 Train Loss: 0.0007886, Val Loss: 0.0008844
2024-11-11 18:44:35,633 Epoch 1081/2000
2024-11-11 18:44:51,473 Current Learning Rate: 0.0035297984
2024-11-11 18:44:51,474 Train Loss: 0.0007828, Val Loss: 0.0009006
2024-11-11 18:44:51,474 Epoch 1082/2000
2024-11-11 18:45:06,811 Current Learning Rate: 0.0036050445
2024-11-11 18:45:06,812 Train Loss: 0.0007277, Val Loss: 0.0008892
2024-11-11 18:45:06,812 Epoch 1083/2000
2024-11-11 18:45:23,103 Current Learning Rate: 0.0036806348
2024-11-11 18:45:23,104 Train Loss: 0.0007981, Val Loss: 0.0008976
2024-11-11 18:45:23,104 Epoch 1084/2000
2024-11-11 18:45:38,385 Current Learning Rate: 0.0037565506
2024-11-11 18:45:38,385 Train Loss: 0.0007050, Val Loss: 0.0008793
2024-11-11 18:45:38,385 Epoch 1085/2000
2024-11-11 18:45:54,414 Current Learning Rate: 0.0038327732
2024-11-11 18:45:54,415 Train Loss: 0.0009147, Val Loss: 0.0009100
2024-11-11 18:45:54,415 Epoch 1086/2000
2024-11-11 18:46:10,519 Current Learning Rate: 0.0039092838
2024-11-11 18:46:10,520 Train Loss: 0.0008636, Val Loss: 0.0009302
2024-11-11 18:46:10,520 Epoch 1087/2000
2024-11-11 18:46:26,233 Current Learning Rate: 0.0039860635
2024-11-11 18:46:26,233 Train Loss: 0.0009090, Val Loss: 0.0009573
2024-11-11 18:46:26,233 Epoch 1088/2000
2024-11-11 18:46:41,469 Current Learning Rate: 0.0040630934
2024-11-11 18:46:41,470 Train Loss: 0.0007566, Val Loss: 0.0009119
2024-11-11 18:46:41,470 Epoch 1089/2000
2024-11-11 18:46:56,869 Current Learning Rate: 0.0041403545
2024-11-11 18:46:56,870 Train Loss: 0.0008079, Val Loss: 0.0009209
2024-11-11 18:46:56,870 Epoch 1090/2000
2024-11-11 18:47:12,678 Current Learning Rate: 0.0042178277
2024-11-11 18:47:12,679 Train Loss: 0.0007778, Val Loss: 0.0009014
2024-11-11 18:47:12,679 Epoch 1091/2000
2024-11-11 18:47:28,858 Current Learning Rate: 0.0042954938
2024-11-11 18:47:28,859 Train Loss: 0.0007078, Val Loss: 0.0008732
2024-11-11 18:47:28,859 Epoch 1092/2000
2024-11-11 18:47:45,369 Current Learning Rate: 0.0043733338
2024-11-11 18:47:45,370 Train Loss: 0.0008591, Val Loss: 0.0009461
2024-11-11 18:47:45,370 Epoch 1093/2000
2024-11-11 18:48:01,773 Current Learning Rate: 0.0044513284
2024-11-11 18:48:01,773 Train Loss: 0.0007430, Val Loss: 0.0009078
2024-11-11 18:48:01,774 Epoch 1094/2000
2024-11-11 18:48:16,770 Current Learning Rate: 0.0045294584
2024-11-11 18:48:16,771 Train Loss: 0.0008409, Val Loss: 0.0009163
2024-11-11 18:48:16,771 Epoch 1095/2000
2024-11-11 18:48:32,376 Current Learning Rate: 0.0046077045
2024-11-11 18:48:32,377 Train Loss: 0.0007834, Val Loss: 0.0009657
2024-11-11 18:48:32,377 Epoch 1096/2000
2024-11-11 18:48:47,756 Current Learning Rate: 0.0046860474
2024-11-11 18:48:47,756 Train Loss: 0.0007785, Val Loss: 0.0009072
2024-11-11 18:48:47,756 Epoch 1097/2000
2024-11-11 18:49:03,674 Current Learning Rate: 0.0047644677
2024-11-11 18:49:03,675 Train Loss: 0.0008473, Val Loss: 0.0009193
2024-11-11 18:49:03,675 Epoch 1098/2000
2024-11-11 18:49:19,285 Current Learning Rate: 0.0048429462
2024-11-11 18:49:19,286 Train Loss: 0.0009198, Val Loss: 0.0009047
2024-11-11 18:49:19,286 Epoch 1099/2000
2024-11-11 18:49:35,804 Current Learning Rate: 0.0049214634
2024-11-11 18:49:35,805 Train Loss: 0.0007513, Val Loss: 0.0008817
2024-11-11 18:49:35,805 Epoch 1100/2000
2024-11-11 18:49:51,202 Current Learning Rate: 0.0050000000
2024-11-11 18:49:51,203 Train Loss: 0.0006984, Val Loss: 0.0008786
2024-11-11 18:49:51,203 Epoch 1101/2000
2024-11-11 18:50:06,773 Current Learning Rate: 0.0050785366
2024-11-11 18:50:06,774 Train Loss: 0.0007898, Val Loss: 0.0009168
2024-11-11 18:50:06,774 Epoch 1102/2000
2024-11-11 18:50:23,111 Current Learning Rate: 0.0051570538
2024-11-11 18:50:23,112 Train Loss: 0.0007377, Val Loss: 0.0009072
2024-11-11 18:50:23,112 Epoch 1103/2000
2024-11-11 18:50:38,981 Current Learning Rate: 0.0052355323
2024-11-11 18:50:38,981 Train Loss: 0.0007992, Val Loss: 0.0009590
2024-11-11 18:50:38,982 Epoch 1104/2000
2024-11-11 18:50:54,636 Current Learning Rate: 0.0053139526
2024-11-11 18:50:54,637 Train Loss: 0.0009170, Val Loss: 0.0009772
2024-11-11 18:50:54,637 Epoch 1105/2000
2024-11-11 18:51:11,338 Current Learning Rate: 0.0053922955
2024-11-11 18:51:11,338 Train Loss: 0.0008388, Val Loss: 0.0009675
2024-11-11 18:51:11,339 Epoch 1106/2000
2024-11-11 18:51:27,485 Current Learning Rate: 0.0054705416
2024-11-11 18:51:27,487 Train Loss: 0.0008355, Val Loss: 0.0010288
2024-11-11 18:51:27,487 Epoch 1107/2000
2024-11-11 18:51:43,126 Current Learning Rate: 0.0055486716
2024-11-11 18:51:43,126 Train Loss: 0.0007842, Val Loss: 0.0009114
2024-11-11 18:51:43,127 Epoch 1108/2000
2024-11-11 18:51:59,493 Current Learning Rate: 0.0056266662
2024-11-11 18:51:59,495 Train Loss: 0.0008256, Val Loss: 0.0009426
2024-11-11 18:51:59,495 Epoch 1109/2000
2024-11-11 18:52:15,288 Current Learning Rate: 0.0057045062
2024-11-11 18:52:15,289 Train Loss: 0.0009038, Val Loss: 0.0009367
2024-11-11 18:52:15,289 Epoch 1110/2000
2024-11-11 18:52:30,620 Current Learning Rate: 0.0057821723
2024-11-11 18:52:30,621 Train Loss: 0.0008269, Val Loss: 0.0009090
2024-11-11 18:52:30,621 Epoch 1111/2000
2024-11-11 18:52:46,896 Current Learning Rate: 0.0058596455
2024-11-11 18:52:46,897 Train Loss: 0.0009088, Val Loss: 0.0009546
2024-11-11 18:52:46,897 Epoch 1112/2000
2024-11-11 18:53:02,681 Current Learning Rate: 0.0059369066
2024-11-11 18:53:02,681 Train Loss: 0.0008433, Val Loss: 0.0009075
2024-11-11 18:53:02,682 Epoch 1113/2000
2024-11-11 18:53:18,567 Current Learning Rate: 0.0060139365
2024-11-11 18:53:18,567 Train Loss: 0.0009822, Val Loss: 0.0009441
2024-11-11 18:53:18,567 Epoch 1114/2000
2024-11-11 18:53:33,757 Current Learning Rate: 0.0060907162
2024-11-11 18:53:33,758 Train Loss: 0.0009162, Val Loss: 0.0009002
2024-11-11 18:53:33,758 Epoch 1115/2000
2024-11-11 18:53:49,097 Current Learning Rate: 0.0061672268
2024-11-11 18:53:49,098 Train Loss: 0.0007641, Val Loss: 0.0009208
2024-11-11 18:53:49,098 Epoch 1116/2000
2024-11-11 18:54:04,371 Current Learning Rate: 0.0062434494
2024-11-11 18:54:04,372 Train Loss: 0.0008616, Val Loss: 0.0009157
2024-11-11 18:54:04,372 Epoch 1117/2000
2024-11-11 18:54:20,901 Current Learning Rate: 0.0063193652
2024-11-11 18:54:20,902 Train Loss: 0.0007720, Val Loss: 0.0009131
2024-11-11 18:54:20,902 Epoch 1118/2000
2024-11-11 18:54:36,520 Current Learning Rate: 0.0063949555
2024-11-11 18:54:36,521 Train Loss: 0.0008748, Val Loss: 0.0009098
2024-11-11 18:54:36,522 Epoch 1119/2000
2024-11-11 18:54:52,529 Current Learning Rate: 0.0064702016
2024-11-11 18:54:52,529 Train Loss: 0.0008747, Val Loss: 0.0009104
2024-11-11 18:54:52,529 Epoch 1120/2000
2024-11-11 18:55:08,008 Current Learning Rate: 0.0065450850
2024-11-11 18:55:08,009 Train Loss: 0.0007746, Val Loss: 0.0009134
2024-11-11 18:55:08,009 Epoch 1121/2000
2024-11-11 18:55:24,190 Current Learning Rate: 0.0066195871
2024-11-11 18:55:24,191 Train Loss: 0.0008900, Val Loss: 0.0009446
2024-11-11 18:55:24,191 Epoch 1122/2000
2024-11-11 18:55:41,093 Current Learning Rate: 0.0066936896
2024-11-11 18:55:41,093 Train Loss: 0.0008111, Val Loss: 0.0009460
2024-11-11 18:55:41,094 Epoch 1123/2000
2024-11-11 18:55:58,069 Current Learning Rate: 0.0067673742
2024-11-11 18:55:58,069 Train Loss: 0.0010924, Val Loss: 0.0012449
2024-11-11 18:55:58,070 Epoch 1124/2000
2024-11-11 18:56:14,415 Current Learning Rate: 0.0068406228
2024-11-11 18:56:14,416 Train Loss: 0.0009590, Val Loss: 0.0009485
2024-11-11 18:56:14,416 Epoch 1125/2000
2024-11-11 18:56:30,273 Current Learning Rate: 0.0069134172
2024-11-11 18:56:30,274 Train Loss: 0.0009170, Val Loss: 0.0009529
2024-11-11 18:56:30,274 Epoch 1126/2000
2024-11-11 18:56:46,177 Current Learning Rate: 0.0069857395
2024-11-11 18:56:46,178 Train Loss: 0.0008428, Val Loss: 0.0009891
2024-11-11 18:56:46,178 Epoch 1127/2000
2024-11-11 18:57:02,205 Current Learning Rate: 0.0070575718
2024-11-11 18:57:02,205 Train Loss: 0.0008743, Val Loss: 0.0009457
2024-11-11 18:57:02,206 Epoch 1128/2000
2024-11-11 18:57:18,351 Current Learning Rate: 0.0071288965
2024-11-11 18:57:18,352 Train Loss: 0.0007929, Val Loss: 0.0009199
2024-11-11 18:57:18,352 Epoch 1129/2000
2024-11-11 18:57:34,626 Current Learning Rate: 0.0071996958
2024-11-11 18:57:34,627 Train Loss: 0.0008838, Val Loss: 0.0009955
2024-11-11 18:57:34,627 Epoch 1130/2000
2024-11-11 18:57:50,691 Current Learning Rate: 0.0072699525
2024-11-11 18:57:50,691 Train Loss: 0.0008128, Val Loss: 0.0009871
2024-11-11 18:57:50,692 Epoch 1131/2000
2024-11-11 18:58:07,237 Current Learning Rate: 0.0073396491
2024-11-11 18:58:07,237 Train Loss: 0.0008704, Val Loss: 0.0010355
2024-11-11 18:58:07,238 Epoch 1132/2000
2024-11-11 18:58:22,551 Current Learning Rate: 0.0074087684
2024-11-11 18:58:22,552 Train Loss: 0.0008751, Val Loss: 0.0009685
2024-11-11 18:58:22,552 Epoch 1133/2000
2024-11-11 18:58:37,894 Current Learning Rate: 0.0074772933
2024-11-11 18:58:37,894 Train Loss: 0.0008425, Val Loss: 0.0009452
2024-11-11 18:58:37,895 Epoch 1134/2000
2024-11-11 18:58:53,987 Current Learning Rate: 0.0075452071
2024-11-11 18:58:53,988 Train Loss: 0.0007711, Val Loss: 0.0009159
2024-11-11 18:58:53,988 Epoch 1135/2000
2024-11-11 18:59:09,818 Current Learning Rate: 0.0076124928
2024-11-11 18:59:09,818 Train Loss: 0.0007821, Val Loss: 0.0009073
2024-11-11 18:59:09,818 Epoch 1136/2000
2024-11-11 18:59:26,595 Current Learning Rate: 0.0076791340
2024-11-11 18:59:26,595 Train Loss: 0.0009624, Val Loss: 0.0010466
2024-11-11 18:59:26,596 Epoch 1137/2000
2024-11-11 18:59:42,702 Current Learning Rate: 0.0077451141
2024-11-11 18:59:42,703 Train Loss: 0.0008611, Val Loss: 0.0009538
2024-11-11 18:59:42,703 Epoch 1138/2000
2024-11-11 18:59:58,795 Current Learning Rate: 0.0078104169
2024-11-11 18:59:58,795 Train Loss: 0.0009359, Val Loss: 0.0010198
2024-11-11 18:59:58,796 Epoch 1139/2000
2024-11-11 19:00:13,498 Current Learning Rate: 0.0078750263
2024-11-11 19:00:13,498 Train Loss: 0.0008688, Val Loss: 0.0009391
2024-11-11 19:00:13,498 Epoch 1140/2000
2024-11-11 19:00:28,891 Current Learning Rate: 0.0079389263
2024-11-11 19:00:28,892 Train Loss: 0.0008664, Val Loss: 0.0009756
2024-11-11 19:00:28,892 Epoch 1141/2000
2024-11-11 19:00:44,292 Current Learning Rate: 0.0080021011
2024-11-11 19:00:44,293 Train Loss: 0.0008583, Val Loss: 0.0009709
2024-11-11 19:00:44,293 Epoch 1142/2000
2024-11-11 19:01:00,106 Current Learning Rate: 0.0080645353
2024-11-11 19:01:00,106 Train Loss: 0.0008195, Val Loss: 0.0009350
2024-11-11 19:01:00,106 Epoch 1143/2000
2024-11-11 19:01:15,206 Current Learning Rate: 0.0081262133
2024-11-11 19:01:15,206 Train Loss: 0.0009231, Val Loss: 0.0010300
2024-11-11 19:01:15,206 Epoch 1144/2000
2024-11-11 19:01:30,241 Current Learning Rate: 0.0081871199
2024-11-11 19:01:30,242 Train Loss: 0.0009033, Val Loss: 0.0010105
2024-11-11 19:01:30,242 Epoch 1145/2000
2024-11-11 19:01:46,298 Current Learning Rate: 0.0082472402
2024-11-11 19:01:46,319 Train Loss: 0.0009219, Val Loss: 0.0009805
2024-11-11 19:01:46,319 Epoch 1146/2000
2024-11-11 19:02:02,591 Current Learning Rate: 0.0083065593
2024-11-11 19:02:02,592 Train Loss: 0.0010280, Val Loss: 0.0010458
2024-11-11 19:02:02,592 Epoch 1147/2000
2024-11-11 19:02:18,516 Current Learning Rate: 0.0083650626
2024-11-11 19:02:18,517 Train Loss: 0.0008359, Val Loss: 0.0009765
2024-11-11 19:02:18,517 Epoch 1148/2000
2024-11-11 19:02:34,040 Current Learning Rate: 0.0084227355
2024-11-11 19:02:34,040 Train Loss: 0.0008897, Val Loss: 0.0010902
2024-11-11 19:02:34,041 Epoch 1149/2000
2024-11-11 19:02:48,957 Current Learning Rate: 0.0084795640
2024-11-11 19:02:48,957 Train Loss: 0.0009626, Val Loss: 0.0010651
2024-11-11 19:02:48,957 Epoch 1150/2000
2024-11-11 19:03:04,906 Current Learning Rate: 0.0085355339
2024-11-11 19:03:04,906 Train Loss: 0.0009077, Val Loss: 0.0009267
2024-11-11 19:03:04,907 Epoch 1151/2000
2024-11-11 19:03:18,450 Added key: store_based_barrier_key:1 to store for rank: 0
2024-11-11 19:03:20,329 Current Learning Rate: 0.0085906315
2024-11-11 19:03:20,331 Train Loss: 0.0008699, Val Loss: 0.0009190
2024-11-11 19:03:20,332 Epoch 1152/2000
2024-11-11 19:03:36,464 Current Learning Rate: 0.0086448431
2024-11-11 19:03:36,465 Train Loss: 0.0008264, Val Loss: 0.0009337
2024-11-11 19:03:36,466 Epoch 1153/2000
2024-11-11 19:03:42,658 Loading best model from checkpoint.
2024-11-11 19:03:59,935 Testing completed and best model saved.
-11-11 19:03:51,275 Train Loss: 0.0007436, Val Loss: 0.0009308
2024-11-11 19:03:51,275 Epoch 1154/2000
2024-11-11 19:04:06,794 Current Learning Rate: 0.0087505553
2024-11-11 19:04:06,794 Train Loss: 0.0006984, Val Loss: 0.0008624
2024-11-11 19:04:06,794 Epoch 1155/2000
2024-11-11 19:04:22,123 Current Learning Rate: 0.0088020298
2024-11-11 19:04:22,123 Train Loss: 0.0007567, Val Loss: 0.0008781
2024-11-11 19:04:22,124 Epoch 1156/2000
2024-11-11 19:04:37,362 Current Learning Rate: 0.0088525662
2024-11-11 19:04:37,363 Train Loss: 0.0008130, Val Loss: 0.0008850
2024-11-11 19:04:37,363 Epoch 1157/2000
2024-11-11 19:04:53,117 Current Learning Rate: 0.0089021520
2024-11-11 19:04:53,118 Train Loss: 0.0007347, Val Loss: 0.0009162
2024-11-11 19:04:53,118 Epoch 1158/2000
2024-11-11 19:05:09,548 Current Learning Rate: 0.0089507751
2024-11-11 19:05:09,548 Train Loss: 0.0008315, Val Loss: 0.0009582
2024-11-11 19:05:09,549 Epoch 1159/2000
2024-11-11 19:05:26,131 Current Learning Rate: 0.0089984233
2024-11-11 19:05:26,131 Train Loss: 0.0008935, Val Loss: 0.0009455
2024-11-11 19:05:26,132 Epoch 1160/2000
2024-11-11 19:05:42,270 Current Learning Rate: 0.0090450850
2024-11-11 19:05:42,271 Train Loss: 0.0007443, Val Loss: 0.0009121
2024-11-11 19:05:42,271 Epoch 1161/2000
2024-11-11 19:05:57,768 Current Learning Rate: 0.0090907486
2024-11-11 19:05:57,769 Train Loss: 0.0007306, Val Loss: 0.0009244
2024-11-11 19:05:57,769 Epoch 1162/2000
2024-11-11 19:06:13,568 Current Learning Rate: 0.0091354029
2024-11-11 19:06:13,569 Train Loss: 0.0009027, Val Loss: 0.0009766
2024-11-11 19:06:13,569 Epoch 1163/2000
2024-11-11 19:06:28,837 Current Learning Rate: 0.0091790368
2024-11-11 19:06:28,837 Train Loss: 0.0008806, Val Loss: 0.0009236
2024-11-11 19:06:28,837 Epoch 1164/2000
2024-11-11 19:06:44,143 Current Learning Rate: 0.0092216396
2024-11-11 19:06:44,144 Train Loss: 0.0008355, Val Loss: 0.0009116
2024-11-11 19:06:44,144 Epoch 1165/2000
2024-11-11 19:07:01,271 Current Learning Rate: 0.0092632008
2024-11-11 19:07:01,272 Train Loss: 0.0008376, Val Loss: 0.0009840
2024-11-11 19:07:01,272 Epoch 1166/2000
2024-11-11 19:07:17,404 Current Learning Rate: 0.0093037101
2024-11-11 19:07:17,404 Train Loss: 0.0009626, Val Loss: 0.0010315
2024-11-11 19:07:17,404 Epoch 1167/2000
2024-11-11 19:07:33,394 Current Learning Rate: 0.0093431576
2024-11-11 19:07:33,395 Train Loss: 0.0009344, Val Loss: 0.0009433
2024-11-11 19:07:33,395 Epoch 1168/2000
2024-11-11 19:07:49,114 Current Learning Rate: 0.0093815334
2024-11-11 19:07:49,115 Train Loss: 0.0009765, Val Loss: 0.0009319
2024-11-11 19:07:49,115 Epoch 1169/2000
2024-11-11 19:08:04,723 Current Learning Rate: 0.0094188282
2024-11-11 19:08:04,723 Train Loss: 0.0008037, Val Loss: 0.0008954
2024-11-11 19:08:04,723 Epoch 1170/2000
2024-11-11 19:08:20,121 Current Learning Rate: 0.0094550326
2024-11-11 19:08:20,121 Train Loss: 0.0007491, Val Loss: 0.0008791
2024-11-11 19:08:20,121 Epoch 1171/2000
2024-11-11 19:08:35,509 Current Learning Rate: 0.0094901379
2024-11-11 19:08:35,510 Train Loss: 0.0007986, Val Loss: 0.0009121
2024-11-11 19:08:35,510 Epoch 1172/2000
2024-11-11 19:08:50,960 Current Learning Rate: 0.0095241353
2024-11-11 19:08:50,961 Train Loss: 0.0007640, Val Loss: 0.0008937
2024-11-11 19:08:50,961 Epoch 1173/2000
2024-11-11 19:09:07,198 Current Learning Rate: 0.0095570164
2024-11-11 19:09:07,198 Train Loss: 0.0008310, Val Loss: 0.0008824
2024-11-11 19:09:07,198 Epoch 1174/2000
2024-11-11 19:09:23,042 Current Learning Rate: 0.0095887731
2024-11-11 19:09:23,042 Train Loss: 0.0008596, Val Loss: 0.0009071
2024-11-11 19:09:23,042 Epoch 1175/2000
2024-11-11 19:09:38,905 Current Learning Rate: 0.0096193977
2024-11-11 19:09:38,905 Train Loss: 0.0007866, Val Loss: 0.0009351
2024-11-11 19:09:38,906 Epoch 1176/2000
2024-11-11 19:09:55,190 Current Learning Rate: 0.0096488824
2024-11-11 19:09:55,190 Train Loss: 0.0007205, Val Loss: 0.0009014
2024-11-11 19:09:55,190 Epoch 1177/2000
2024-11-11 19:10:11,353 Current Learning Rate: 0.0096772202
2024-11-11 19:10:11,353 Train Loss: 0.0007054, Val Loss: 0.0008608
2024-11-11 19:10:11,353 Epoch 1178/2000
2024-11-11 19:10:26,492 Current Learning Rate: 0.0097044038
2024-11-11 19:10:26,492 Train Loss: 0.0007777, Val Loss: 0.0009524
2024-11-11 19:10:26,493 Epoch 1179/2000
2024-11-11 19:10:42,047 Current Learning Rate: 0.0097304268
2024-11-11 19:10:42,048 Train Loss: 0.0009247, Val Loss: 0.0010017
2024-11-11 19:10:42,048 Epoch 1180/2000
2024-11-11 19:10:57,387 Current Learning Rate: 0.0097552826
2024-11-11 19:10:57,388 Train Loss: 0.0009114, Val Loss: 0.0010670
2024-11-11 19:10:57,388 Epoch 1181/2000
2024-11-11 19:11:13,262 Current Learning Rate: 0.0097789651
2024-11-11 19:11:13,262 Train Loss: 0.0009571, Val Loss: 0.0010167
2024-11-11 19:11:13,263 Epoch 1182/2000
2024-11-11 19:11:29,838 Current Learning Rate: 0.0098014684
2024-11-11 19:11:29,839 Train Loss: 0.0008427, Val Loss: 0.0009216
2024-11-11 19:11:29,839 Epoch 1183/2000
2024-11-11 19:11:45,016 Current Learning Rate: 0.0098227871
2024-11-11 19:11:45,017 Train Loss: 0.0007869, Val Loss: 0.0009104
2024-11-11 19:11:45,017 Epoch 1184/2000
2024-11-11 19:12:00,408 Current Learning Rate: 0.0098429158
2024-11-11 19:12:00,409 Train Loss: 0.0007923, Val Loss: 0.0009151
2024-11-11 19:12:00,409 Epoch 1185/2000
2024-11-11 19:12:15,984 Current Learning Rate: 0.0098618496
2024-11-11 19:12:15,984 Train Loss: 0.0007653, Val Loss: 0.0009307
2024-11-11 19:12:15,984 Epoch 1186/2000
2024-11-11 19:12:32,713 Current Learning Rate: 0.0098795838
2024-11-11 19:12:32,714 Train Loss: 0.0007941, Val Loss: 0.0009135
2024-11-11 19:12:32,714 Epoch 1187/2000
2024-11-11 19:12:49,129 Current Learning Rate: 0.0098961141
2024-11-11 19:12:49,130 Train Loss: 0.0007957, Val Loss: 0.0008860
2024-11-11 19:12:49,130 Epoch 1188/2000
2024-11-11 19:13:05,500 Current Learning Rate: 0.0099114363
2024-11-11 19:13:05,501 Train Loss: 0.0007098, Val Loss: 0.0008635
2024-11-11 19:13:05,501 Epoch 1189/2000
2024-11-11 19:13:21,816 Current Learning Rate: 0.0099255466
2024-11-11 19:13:21,816 Train Loss: 0.0008032, Val Loss: 0.0009301
2024-11-11 19:13:21,816 Epoch 1190/2000
2024-11-11 19:13:38,502 Current Learning Rate: 0.0099384417
2024-11-11 19:13:38,502 Train Loss: 0.0008252, Val Loss: 0.0009136
2024-11-11 19:13:38,503 Epoch 1191/2000
2024-11-11 19:13:53,770 Current Learning Rate: 0.0099501183
2024-11-11 19:13:53,771 Train Loss: 0.0007597, Val Loss: 0.0009133
2024-11-11 19:13:53,771 Epoch 1192/2000
2024-11-11 19:14:09,597 Current Learning Rate: 0.0099605735
2024-11-11 19:14:09,598 Train Loss: 0.0008328, Val Loss: 0.0008811
2024-11-11 19:14:09,598 Epoch 1193/2000
2024-11-11 19:14:25,733 Current Learning Rate: 0.0099698048
2024-11-11 19:14:25,734 Train Loss: 0.0007931, Val Loss: 0.0009434
2024-11-11 19:14:25,734 Epoch 1194/2000
2024-11-11 19:14:40,484 Current Learning Rate: 0.0099778098
2024-11-11 19:14:40,485 Train Loss: 0.0009715, Val Loss: 0.0009576
2024-11-11 19:14:40,485 Epoch 1195/2000
2024-11-11 19:14:55,865 Current Learning Rate: 0.0099845867
2024-11-11 19:14:55,865 Train Loss: 0.0007851, Val Loss: 0.0009919
2024-11-11 19:14:55,865 Epoch 1196/2000
2024-11-11 19:15:12,166 Current Learning Rate: 0.0099901336
2024-11-11 19:15:12,167 Train Loss: 0.0008322, Val Loss: 0.0009438
2024-11-11 19:15:12,167 Epoch 1197/2000
2024-11-11 19:15:27,591 Current Learning Rate: 0.0099944494
2024-11-11 19:15:27,591 Train Loss: 0.0008722, Val Loss: 0.0009891
2024-11-11 19:15:27,591 Epoch 1198/2000
2024-11-11 19:15:43,066 Current Learning Rate: 0.0099975328
2024-11-11 19:15:43,066 Train Loss: 0.0007786, Val Loss: 0.0008811
2024-11-11 19:15:43,067 Epoch 1199/2000
2024-11-11 19:15:58,443 Current Learning Rate: 0.0099993832
2024-11-11 19:15:58,443 Train Loss: 0.0008825, Val Loss: 0.0009694
2024-11-11 19:15:58,443 Epoch 1200/2000
2024-11-11 19:16:15,059 Current Learning Rate: 0.0100000000
2024-11-11 19:16:15,061 Train Loss: 0.0009325, Val Loss: 0.0009888
2024-11-11 19:16:15,061 Epoch 1201/2000
2024-11-11 19:16:31,458 Current Learning Rate: 0.0099993832
2024-11-11 19:16:31,459 Train Loss: 0.0009915, Val Loss: 0.0009818
2024-11-11 19:16:31,459 Epoch 1202/2000
2024-11-11 19:16:47,431 Current Learning Rate: 0.0099975328
2024-11-11 19:16:47,432 Train Loss: 0.0007678, Val Loss: 0.0008826
2024-11-11 19:16:47,432 Epoch 1203/2000
2024-11-11 19:17:02,996 Current Learning Rate: 0.0099944494
2024-11-11 19:17:02,997 Train Loss: 0.0009000, Val Loss: 0.0008945
2024-11-11 19:17:02,997 Epoch 1204/2000
2024-11-11 19:17:19,426 Current Learning Rate: 0.0099901336
2024-11-11 19:17:19,426 Train Loss: 0.0008233, Val Loss: 0.0008658
2024-11-11 19:17:19,426 Epoch 1205/2000
2024-11-11 19:17:35,972 Current Learning Rate: 0.0099845867
2024-11-11 19:17:35,972 Train Loss: 0.0007527, Val Loss: 0.0008549
2024-11-11 19:17:35,972 Epoch 1206/2000
2024-11-11 19:17:50,680 Current Learning Rate: 0.0099778098
2024-11-11 19:17:51,480 Train Loss: 0.0007904, Val Loss: 0.0008346
2024-11-11 19:17:51,481 Epoch 1207/2000
2024-11-11 19:18:06,040 Current Learning Rate: 0.0099698048
2024-11-11 19:18:06,801 Train Loss: 0.0007418, Val Loss: 0.0008254
2024-11-11 19:18:06,801 Epoch 1208/2000
2024-11-11 19:18:21,420 Current Learning Rate: 0.0099605735
2024-11-11 19:18:21,421 Train Loss: 0.0007113, Val Loss: 0.0009579
2024-11-11 19:18:21,421 Epoch 1209/2000
2024-11-11 19:18:36,840 Current Learning Rate: 0.0099501183
2024-11-11 19:18:36,841 Train Loss: 0.0008529, Val Loss: 0.0009929
2024-11-11 19:18:36,841 Epoch 1210/2000
2024-11-11 19:18:53,220 Current Learning Rate: 0.0099384417
2024-11-11 19:18:53,221 Train Loss: 0.0007548, Val Loss: 0.0008843
2024-11-11 19:18:53,221 Epoch 1211/2000
2024-11-11 19:19:09,386 Current Learning Rate: 0.0099255466
2024-11-11 19:19:09,387 Train Loss: 0.0008613, Val Loss: 0.0009586
2024-11-11 19:19:09,387 Epoch 1212/2000
2024-11-11 19:19:25,803 Current Learning Rate: 0.0099114363
2024-11-11 19:19:25,803 Train Loss: 0.0008580, Val Loss: 0.0010224
2024-11-11 19:19:25,804 Epoch 1213/2000
2024-11-11 19:19:42,245 Current Learning Rate: 0.0098961141
2024-11-11 19:19:42,245 Train Loss: 0.0007980, Val Loss: 0.0008683
2024-11-11 19:19:42,246 Epoch 1214/2000
2024-11-11 19:19:58,284 Current Learning Rate: 0.0098795838
2024-11-11 19:19:58,285 Train Loss: 0.0008451, Val Loss: 0.0009139
2024-11-11 19:19:58,285 Epoch 1215/2000
2024-11-11 19:20:14,202 Current Learning Rate: 0.0098618496
2024-11-11 19:20:14,202 Train Loss: 0.0007909, Val Loss: 0.0008778
2024-11-11 19:20:14,203 Epoch 1216/2000
2024-11-11 19:20:30,648 Current Learning Rate: 0.0098429158
2024-11-11 19:20:30,649 Train Loss: 0.0007579, Val Loss: 0.0008329
2024-11-11 19:20:30,649 Epoch 1217/2000
2024-11-11 19:20:46,337 Current Learning Rate: 0.0098227871
2024-11-11 19:20:46,338 Train Loss: 0.0007602, Val Loss: 0.0008391
2024-11-11 19:20:46,338 Epoch 1218/2000
2024-11-11 19:21:02,049 Current Learning Rate: 0.0098014684
2024-11-11 19:21:02,049 Train Loss: 0.0007357, Val Loss: 0.0008317
2024-11-11 19:21:02,049 Epoch 1219/2000
2024-11-11 19:21:17,302 Current Learning Rate: 0.0097789651
2024-11-11 19:21:17,303 Train Loss: 0.0006970, Val Loss: 0.0008263
2024-11-11 19:21:17,303 Epoch 1220/2000
2024-11-11 19:21:32,763 Current Learning Rate: 0.0097552826
2024-11-11 19:21:33,486 Train Loss: 0.0007238, Val Loss: 0.0008217
2024-11-11 19:21:33,486 Epoch 1221/2000
2024-11-11 19:21:48,531 Current Learning Rate: 0.0097304268
2024-11-11 19:21:48,532 Train Loss: 0.0008221, Val Loss: 0.0008921
2024-11-11 19:21:48,532 Epoch 1222/2000
2024-11-11 19:22:04,214 Current Learning Rate: 0.0097044038
2024-11-11 19:22:04,214 Train Loss: 0.0007832, Val Loss: 0.0008790
2024-11-11 19:22:04,214 Epoch 1223/2000
2024-11-11 19:22:19,294 Current Learning Rate: 0.0096772202
2024-11-11 19:22:19,294 Train Loss: 0.0007623, Val Loss: 0.0008899
2024-11-11 19:22:19,295 Epoch 1224/2000
2024-11-11 19:22:35,499 Current Learning Rate: 0.0096488824
2024-11-11 19:22:35,499 Train Loss: 0.0007388, Val Loss: 0.0008893
2024-11-11 19:22:35,499 Epoch 1225/2000
2024-11-11 19:22:50,695 Current Learning Rate: 0.0096193977
2024-11-11 19:22:50,695 Train Loss: 0.0007008, Val Loss: 0.0008764
2024-11-11 19:22:50,696 Epoch 1226/2000
2024-11-11 19:23:06,849 Current Learning Rate: 0.0095887731
2024-11-11 19:23:06,849 Train Loss: 0.0007780, Val Loss: 0.0009302
2024-11-11 19:23:06,849 Epoch 1227/2000
2024-11-11 19:23:22,766 Current Learning Rate: 0.0095570164
2024-11-11 19:23:22,767 Train Loss: 0.0007999, Val Loss: 0.0008586
2024-11-11 19:23:22,767 Epoch 1228/2000
2024-11-11 19:23:39,711 Current Learning Rate: 0.0095241353
2024-11-11 19:23:39,712 Train Loss: 0.0007903, Val Loss: 0.0009007
2024-11-11 19:23:39,712 Epoch 1229/2000
2024-11-11 19:23:56,029 Current Learning Rate: 0.0094901379
2024-11-11 19:23:56,029 Train Loss: 0.0008943, Val Loss: 0.0008876
2024-11-11 19:23:56,030 Epoch 1230/2000
2024-11-11 19:24:11,968 Current Learning Rate: 0.0094550326
2024-11-11 19:24:11,969 Train Loss: 0.0007999, Val Loss: 0.0008620
2024-11-11 19:24:11,970 Epoch 1231/2000
2024-11-11 19:24:29,057 Current Learning Rate: 0.0094188282
2024-11-11 19:24:29,058 Train Loss: 0.0007607, Val Loss: 0.0008616
2024-11-11 19:24:29,058 Epoch 1232/2000
2024-11-11 19:24:44,139 Current Learning Rate: 0.0093815334
2024-11-11 19:24:44,953 Train Loss: 0.0006732, Val Loss: 0.0007991
2024-11-11 19:24:44,953 Epoch 1233/2000
2024-11-11 19:25:00,751 Current Learning Rate: 0.0093431576
2024-11-11 19:25:00,753 Train Loss: 0.0007684, Val Loss: 0.0008466
2024-11-11 19:25:00,754 Epoch 1234/2000
2024-11-11 19:25:16,710 Current Learning Rate: 0.0093037101
2024-11-11 19:25:16,711 Train Loss: 0.0008696, Val Loss: 0.0008539
2024-11-11 19:25:16,712 Epoch 1235/2000
2024-11-11 19:25:33,031 Current Learning Rate: 0.0092632008
2024-11-11 19:25:33,033 Train Loss: 0.0006910, Val Loss: 0.0008334
2024-11-11 19:25:33,033 Epoch 1236/2000
2024-11-11 19:25:48,841 Current Learning Rate: 0.0092216396
2024-11-11 19:25:48,841 Train Loss: 0.0007120, Val Loss: 0.0008217
2024-11-11 19:25:48,841 Epoch 1237/2000
2024-11-11 19:26:04,302 Current Learning Rate: 0.0091790368
2024-11-11 19:26:04,303 Train Loss: 0.0007135, Val Loss: 0.0008230
2024-11-11 19:26:04,304 Epoch 1238/2000
2024-11-11 19:26:19,513 Current Learning Rate: 0.0091354029
2024-11-11 19:26:19,514 Train Loss: 0.0008573, Val Loss: 0.0008267
2024-11-11 19:26:19,514 Epoch 1239/2000
2024-11-11 19:26:36,588 Current Learning Rate: 0.0090907486
2024-11-11 19:26:36,589 Train Loss: 0.0006955, Val Loss: 0.0008639
2024-11-11 19:26:36,589 Epoch 1240/2000
2024-11-11 19:26:51,627 Current Learning Rate: 0.0090450850
2024-11-11 19:26:51,627 Train Loss: 0.0008323, Val Loss: 0.0009813
2024-11-11 19:26:51,628 Epoch 1241/2000
2024-11-11 19:27:07,013 Current Learning Rate: 0.0089984233
2024-11-11 19:27:07,013 Train Loss: 0.0008396, Val Loss: 0.0009004
2024-11-11 19:27:07,013 Epoch 1242/2000
2024-11-11 19:27:22,295 Current Learning Rate: 0.0089507751
2024-11-11 19:27:22,295 Train Loss: 0.0007598, Val Loss: 0.0008683
2024-11-11 19:27:22,295 Epoch 1243/2000
2024-11-11 19:27:37,602 Current Learning Rate: 0.0089021520
2024-11-11 19:27:37,603 Train Loss: 0.0007631, Val Loss: 0.0008305
2024-11-11 19:27:37,603 Epoch 1244/2000
2024-11-11 19:27:53,458 Current Learning Rate: 0.0088525662
2024-11-11 19:27:53,459 Train Loss: 0.0007240, Val Loss: 0.0008004
2024-11-11 19:27:53,459 Epoch 1245/2000
2024-11-11 19:28:09,388 Current Learning Rate: 0.0088020298
2024-11-11 19:28:09,389 Train Loss: 0.0007155, Val Loss: 0.0007998
2024-11-11 19:28:09,389 Epoch 1246/2000
2024-11-11 19:28:24,273 Current Learning Rate: 0.0087505553
2024-11-11 19:28:24,273 Train Loss: 0.0006628, Val Loss: 0.0008059
2024-11-11 19:28:24,273 Epoch 1247/2000
2024-11-11 19:28:39,724 Current Learning Rate: 0.0086981555
2024-11-11 19:28:39,725 Train Loss: 0.0007403, Val Loss: 0.0008217
2024-11-11 19:28:39,725 Epoch 1248/2000
2024-11-11 19:28:54,997 Current Learning Rate: 0.0086448431
2024-11-11 19:28:54,998 Train Loss: 0.0007295, Val Loss: 0.0008898
2024-11-11 19:28:54,998 Epoch 1249/2000
2024-11-11 19:29:11,772 Current Learning Rate: 0.0085906315
2024-11-11 19:29:11,772 Train Loss: 0.0007997, Val Loss: 0.0008358
2024-11-11 19:29:11,772 Epoch 1250/2000
2024-11-11 19:29:27,488 Current Learning Rate: 0.0085355339
2024-11-11 19:29:27,489 Train Loss: 0.0006966, Val Loss: 0.0008217
2024-11-11 19:29:27,489 Epoch 1251/2000
2024-11-11 19:29:43,157 Current Learning Rate: 0.0084795640
2024-11-11 19:29:44,157 Train Loss: 0.0006985, Val Loss: 0.0007988
2024-11-11 19:29:44,157 Epoch 1252/2000
2024-11-11 19:29:59,832 Current Learning Rate: 0.0084227355
2024-11-11 19:30:00,861 Train Loss: 0.0007554, Val Loss: 0.0007808
2024-11-11 19:30:00,861 Epoch 1253/2000
2024-11-11 19:30:17,425 Current Learning Rate: 0.0083650626
2024-11-11 19:30:18,514 Train Loss: 0.0006460, Val Loss: 0.0007802
2024-11-11 19:30:18,514 Epoch 1254/2000
2024-11-11 19:30:33,545 Current Learning Rate: 0.0083065593
2024-11-11 19:30:33,546 Train Loss: 0.0006969, Val Loss: 0.0008002
2024-11-11 19:30:33,546 Epoch 1255/2000
2024-11-11 19:30:50,268 Current Learning Rate: 0.0082472402
2024-11-11 19:30:50,269 Train Loss: 0.0007126, Val Loss: 0.0008224
2024-11-11 19:30:50,269 Epoch 1256/2000
2024-11-11 19:31:06,405 Current Learning Rate: 0.0081871199
2024-11-11 19:31:06,406 Train Loss: 0.0007588, Val Loss: 0.0008356
2024-11-11 19:31:06,406 Epoch 1257/2000
2024-11-11 19:31:22,763 Current Learning Rate: 0.0081262133
2024-11-11 19:31:22,763 Train Loss: 0.0006239, Val Loss: 0.0008300
2024-11-11 19:31:22,763 Epoch 1258/2000
2024-11-11 19:31:37,510 Current Learning Rate: 0.0080645353
2024-11-11 19:31:37,511 Train Loss: 0.0007517, Val Loss: 0.0008245
2024-11-11 19:31:37,511 Epoch 1259/2000
2024-11-11 19:31:53,515 Current Learning Rate: 0.0080021011
2024-11-11 19:31:54,295 Train Loss: 0.0006759, Val Loss: 0.0007783
2024-11-11 19:31:54,295 Epoch 1260/2000
2024-11-11 19:32:09,350 Current Learning Rate: 0.0079389263
2024-11-11 19:32:10,081 Train Loss: 0.0007044, Val Loss: 0.0007597
2024-11-11 19:32:10,082 Epoch 1261/2000
2024-11-11 19:32:25,353 Current Learning Rate: 0.0078750263
2024-11-11 19:32:25,353 Train Loss: 0.0007259, Val Loss: 0.0007679
2024-11-11 19:32:25,354 Epoch 1262/2000
2024-11-11 19:32:40,724 Current Learning Rate: 0.0078104169
2024-11-11 19:32:40,725 Train Loss: 0.0007029, Val Loss: 0.0007838
2024-11-11 19:32:40,725 Epoch 1263/2000
2024-11-11 19:32:57,374 Current Learning Rate: 0.0077451141
2024-11-11 19:32:57,375 Train Loss: 0.0006776, Val Loss: 0.0008143
2024-11-11 19:32:57,375 Epoch 1264/2000
2024-11-11 19:33:12,321 Current Learning Rate: 0.0076791340
2024-11-11 19:33:12,322 Train Loss: 0.0006801, Val Loss: 0.0008060
2024-11-11 19:33:12,322 Epoch 1265/2000
2024-11-11 19:33:27,753 Current Learning Rate: 0.0076124928
2024-11-11 19:33:27,754 Train Loss: 0.0007322, Val Loss: 0.0009160
2024-11-11 19:33:27,754 Epoch 1266/2000
2024-11-11 19:33:43,418 Current Learning Rate: 0.0075452071
2024-11-11 19:33:43,418 Train Loss: 0.0007673, Val Loss: 0.0008446
2024-11-11 19:33:43,418 Epoch 1267/2000
2024-11-11 19:33:58,983 Current Learning Rate: 0.0074772933
2024-11-11 19:33:58,984 Train Loss: 0.0007103, Val Loss: 0.0007735
2024-11-11 19:33:58,984 Epoch 1268/2000
2024-11-11 19:34:14,391 Current Learning Rate: 0.0074087684
2024-11-11 19:34:14,391 Train Loss: 0.0006179, Val Loss: 0.0007601
2024-11-11 19:34:14,391 Epoch 1269/2000
2024-11-11 19:34:30,470 Current Learning Rate: 0.0073396491
2024-11-11 19:34:30,471 Train Loss: 0.0006432, Val Loss: 0.0007638
2024-11-11 19:34:30,471 Epoch 1270/2000
2024-11-11 19:34:45,792 Current Learning Rate: 0.0072699525
2024-11-11 19:34:45,792 Train Loss: 0.0007119, Val Loss: 0.0007776
2024-11-11 19:34:45,793 Epoch 1271/2000
2024-11-11 19:35:01,007 Current Learning Rate: 0.0071996958
2024-11-11 19:35:01,008 Train Loss: 0.0005824, Val Loss: 0.0007780
2024-11-11 19:35:01,008 Epoch 1272/2000
2024-11-11 19:35:16,516 Current Learning Rate: 0.0071288965
2024-11-11 19:35:16,516 Train Loss: 0.0006603, Val Loss: 0.0007763
2024-11-11 19:35:16,516 Epoch 1273/2000
2024-11-11 19:35:32,010 Current Learning Rate: 0.0070575718
2024-11-11 19:35:32,817 Train Loss: 0.0006173, Val Loss: 0.0007556
2024-11-11 19:35:32,817 Epoch 1274/2000
2024-11-11 19:35:47,430 Current Learning Rate: 0.0069857395
2024-11-11 19:35:48,212 Train Loss: 0.0005924, Val Loss: 0.0007387
2024-11-11 19:35:48,212 Epoch 1275/2000
2024-11-11 19:36:03,384 Current Learning Rate: 0.0069134172
2024-11-11 19:36:03,384 Train Loss: 0.0006143, Val Loss: 0.0007536
2024-11-11 19:36:03,385 Epoch 1276/2000
2024-11-11 19:36:18,807 Current Learning Rate: 0.0068406228
2024-11-11 19:36:18,808 Train Loss: 0.0006034, Val Loss: 0.0007466
2024-11-11 19:36:18,809 Epoch 1277/2000
2024-11-11 19:36:34,356 Current Learning Rate: 0.0067673742
2024-11-11 19:36:34,356 Train Loss: 0.0006757, Val Loss: 0.0007473
2024-11-11 19:36:34,357 Epoch 1278/2000
2024-11-11 19:36:49,397 Current Learning Rate: 0.0066936896
2024-11-11 19:36:49,397 Train Loss: 0.0005976, Val Loss: 0.0007534
2024-11-11 19:36:49,398 Epoch 1279/2000
2024-11-11 19:37:04,483 Current Learning Rate: 0.0066195871
2024-11-11 19:37:04,484 Train Loss: 0.0006511, Val Loss: 0.0007671
2024-11-11 19:37:04,484 Epoch 1280/2000
2024-11-11 19:37:19,936 Current Learning Rate: 0.0065450850
2024-11-11 19:37:19,937 Train Loss: 0.0005711, Val Loss: 0.0007763
2024-11-11 19:37:19,937 Epoch 1281/2000
2024-11-11 19:37:35,185 Current Learning Rate: 0.0064702016
2024-11-11 19:37:35,185 Train Loss: 0.0006569, Val Loss: 0.0008083
2024-11-11 19:37:35,185 Epoch 1282/2000
2024-11-11 19:37:50,662 Current Learning Rate: 0.0063949555
2024-11-11 19:37:50,663 Train Loss: 0.0006829, Val Loss: 0.0008200
2024-11-11 19:37:50,664 Epoch 1283/2000
2024-11-11 19:38:05,685 Current Learning Rate: 0.0063193652
2024-11-11 19:38:05,686 Train Loss: 0.0006795, Val Loss: 0.0007918
2024-11-11 19:38:05,687 Epoch 1284/2000
2024-11-11 19:38:21,127 Current Learning Rate: 0.0062434494
2024-11-11 19:38:21,128 Train Loss: 0.0007056, Val Loss: 0.0008107
2024-11-11 19:38:21,128 Epoch 1285/2000
2024-11-11 19:38:36,436 Current Learning Rate: 0.0061672268
2024-11-11 19:38:36,437 Train Loss: 0.0007121, Val Loss: 0.0007828
2024-11-11 19:38:36,438 Epoch 1286/2000
2024-11-11 19:38:51,528 Current Learning Rate: 0.0060907162
2024-11-11 19:38:51,528 Train Loss: 0.0007112, Val Loss: 0.0007594
2024-11-11 19:38:51,529 Epoch 1287/2000
2024-11-11 19:39:07,152 Current Learning Rate: 0.0060139365
2024-11-11 19:39:07,153 Train Loss: 0.0006527, Val Loss: 0.0007456
2024-11-11 19:39:07,153 Epoch 1288/2000
2024-11-11 19:39:22,864 Current Learning Rate: 0.0059369066
2024-11-11 19:39:22,864 Train Loss: 0.0006117, Val Loss: 0.0007583
2024-11-11 19:39:22,865 Epoch 1289/2000
2024-11-11 19:39:38,441 Current Learning Rate: 0.0058596455
2024-11-11 19:39:38,441 Train Loss: 0.0007125, Val Loss: 0.0007986
2024-11-11 19:39:38,442 Epoch 1290/2000
2024-11-11 19:39:54,221 Current Learning Rate: 0.0057821723
2024-11-11 19:39:54,221 Train Loss: 0.0006777, Val Loss: 0.0007551
2024-11-11 19:39:54,222 Epoch 1291/2000
2024-11-11 19:40:10,229 Current Learning Rate: 0.0057045062
2024-11-11 19:40:10,230 Train Loss: 0.0006606, Val Loss: 0.0007435
2024-11-11 19:40:10,231 Epoch 1292/2000
2024-11-11 19:40:25,814 Current Learning Rate: 0.0056266662
2024-11-11 19:40:25,815 Train Loss: 0.0006028, Val Loss: 0.0007567
2024-11-11 19:40:25,815 Epoch 1293/2000
2024-11-11 19:40:42,175 Current Learning Rate: 0.0055486716
2024-11-11 19:40:42,927 Train Loss: 0.0006782, Val Loss: 0.0007165
2024-11-11 19:40:42,927 Epoch 1294/2000
2024-11-11 19:40:58,217 Current Learning Rate: 0.0054705416
2024-11-11 19:40:59,109 Train Loss: 0.0006235, Val Loss: 0.0007030
2024-11-11 19:40:59,109 Epoch 1295/2000
2024-11-11 19:41:14,177 Current Learning Rate: 0.0053922955
2024-11-11 19:41:14,178 Train Loss: 0.0006007, Val Loss: 0.0007068
2024-11-11 19:41:14,179 Epoch 1296/2000
2024-11-11 19:41:29,127 Current Learning Rate: 0.0053139526
2024-11-11 19:41:29,127 Train Loss: 0.0006074, Val Loss: 0.0007046
2024-11-11 19:41:29,128 Epoch 1297/2000
2024-11-11 19:41:44,751 Current Learning Rate: 0.0052355323
2024-11-11 19:41:45,497 Train Loss: 0.0005509, Val Loss: 0.0006939
2024-11-11 19:41:45,497 Epoch 1298/2000
2024-11-11 19:42:00,626 Current Learning Rate: 0.0051570538
2024-11-11 19:42:01,625 Train Loss: 0.0006324, Val Loss: 0.0006921
2024-11-11 19:42:01,625 Epoch 1299/2000
2024-11-11 19:42:17,627 Current Learning Rate: 0.0050785366
2024-11-11 19:42:17,628 Train Loss: 0.0006506, Val Loss: 0.0006980
2024-11-11 19:42:17,628 Epoch 1300/2000
2024-11-11 19:42:33,534 Current Learning Rate: 0.0050000000
2024-11-11 19:42:33,535 Train Loss: 0.0007355, Val Loss: 0.0007156
2024-11-11 19:42:33,535 Epoch 1301/2000
2024-11-11 19:42:48,820 Current Learning Rate: 0.0049214634
2024-11-11 19:42:49,835 Train Loss: 0.0006352, Val Loss: 0.0006887
2024-11-11 19:42:49,836 Epoch 1302/2000
2024-11-11 19:43:05,428 Current Learning Rate: 0.0048429462
2024-11-11 19:43:06,263 Train Loss: 0.0006389, Val Loss: 0.0006805
2024-11-11 19:43:06,264 Epoch 1303/2000
2024-11-11 19:43:21,545 Current Learning Rate: 0.0047644677
2024-11-11 19:43:22,491 Train Loss: 0.0005725, Val Loss: 0.0006790
2024-11-11 19:43:22,491 Epoch 1304/2000
2024-11-11 19:43:37,963 Current Learning Rate: 0.0046860474
2024-11-11 19:43:37,964 Train Loss: 0.0005542, Val Loss: 0.0006816
2024-11-11 19:43:37,964 Epoch 1305/2000
2024-11-11 19:43:53,766 Current Learning Rate: 0.0046077045
2024-11-11 19:43:53,767 Train Loss: 0.0006077, Val Loss: 0.0006828
2024-11-11 19:43:53,767 Epoch 1306/2000
2024-11-11 19:44:09,461 Current Learning Rate: 0.0045294584
2024-11-11 19:44:09,462 Train Loss: 0.0005358, Val Loss: 0.0006855
2024-11-11 19:44:09,462 Epoch 1307/2000
2024-11-11 19:44:24,723 Current Learning Rate: 0.0044513284
2024-11-11 19:44:24,723 Train Loss: 0.0005936, Val Loss: 0.0006877
2024-11-11 19:44:24,723 Epoch 1308/2000
2024-11-11 19:44:40,655 Current Learning Rate: 0.0043733338
2024-11-11 19:44:40,656 Train Loss: 0.0005518, Val Loss: 0.0006815
2024-11-11 19:44:40,656 Epoch 1309/2000
2024-11-11 19:44:56,368 Current Learning Rate: 0.0042954938
2024-11-11 19:44:57,079 Train Loss: 0.0005299, Val Loss: 0.0006740
2024-11-11 19:44:57,079 Epoch 1310/2000
2024-11-11 19:45:12,025 Current Learning Rate: 0.0042178277
2024-11-11 19:45:12,026 Train Loss: 0.0006295, Val Loss: 0.0006814
2024-11-11 19:45:12,026 Epoch 1311/2000
2024-11-11 19:45:27,415 Current Learning Rate: 0.0041403545
2024-11-11 19:45:27,416 Train Loss: 0.0005916, Val Loss: 0.0006889
2024-11-11 19:45:27,416 Epoch 1312/2000
2024-11-11 19:45:43,210 Current Learning Rate: 0.0040630934
2024-11-11 19:45:44,034 Train Loss: 0.0006301, Val Loss: 0.0006719
2024-11-11 19:45:44,035 Epoch 1313/2000
2024-11-11 19:45:59,092 Current Learning Rate: 0.0039860635
2024-11-11 19:45:59,093 Train Loss: 0.0005829, Val Loss: 0.0006777
2024-11-11 19:45:59,093 Epoch 1314/2000
2024-11-11 19:46:15,211 Current Learning Rate: 0.0039092838
2024-11-11 19:46:15,212 Train Loss: 0.0005170, Val Loss: 0.0007158
2024-11-11 19:46:15,212 Epoch 1315/2000
2024-11-11 19:46:30,250 Current Learning Rate: 0.0038327732
2024-11-11 19:46:30,251 Train Loss: 0.0005662, Val Loss: 0.0007079
2024-11-11 19:46:30,251 Epoch 1316/2000
2024-11-11 19:46:45,529 Current Learning Rate: 0.0037565506
2024-11-11 19:46:45,529 Train Loss: 0.0005735, Val Loss: 0.0006768
2024-11-11 19:46:45,529 Epoch 1317/2000
2024-11-11 19:47:01,289 Current Learning Rate: 0.0036806348
2024-11-11 19:47:01,290 Train Loss: 0.0005399, Val Loss: 0.0007005
2024-11-11 19:47:01,291 Epoch 1318/2000
2024-11-11 19:47:16,945 Current Learning Rate: 0.0036050445
2024-11-11 19:47:16,946 Train Loss: 0.0005213, Val Loss: 0.0007417
2024-11-11 19:47:16,946 Epoch 1319/2000
2024-11-11 19:47:33,087 Current Learning Rate: 0.0035297984
2024-11-11 19:47:33,088 Train Loss: 0.0006110, Val Loss: 0.0006880
2024-11-11 19:47:33,088 Epoch 1320/2000
2024-11-11 19:47:49,882 Current Learning Rate: 0.0034549150
2024-11-11 19:47:49,882 Train Loss: 0.0005038, Val Loss: 0.0006751
2024-11-11 19:47:49,882 Epoch 1321/2000
2024-11-11 19:48:05,855 Current Learning Rate: 0.0033804129
2024-11-11 19:48:05,856 Train Loss: 0.0005429, Val Loss: 0.0006789
2024-11-11 19:48:05,856 Epoch 1322/2000
2024-11-11 19:48:21,447 Current Learning Rate: 0.0033063104
2024-11-11 19:48:21,447 Train Loss: 0.0005773, Val Loss: 0.0006816
2024-11-11 19:48:21,447 Epoch 1323/2000
2024-11-11 19:48:37,225 Current Learning Rate: 0.0032326258
2024-11-11 19:48:37,226 Train Loss: 0.0005571, Val Loss: 0.0006779
2024-11-11 19:48:37,226 Epoch 1324/2000
2024-11-11 19:48:52,382 Current Learning Rate: 0.0031593772
2024-11-11 19:48:53,203 Train Loss: 0.0005491, Val Loss: 0.0006707
2024-11-11 19:48:53,203 Epoch 1325/2000
2024-11-11 19:49:07,763 Current Learning Rate: 0.0030865828
2024-11-11 19:49:08,458 Train Loss: 0.0005470, Val Loss: 0.0006639
2024-11-11 19:49:08,458 Epoch 1326/2000
2024-11-11 19:49:23,171 Current Learning Rate: 0.0030142605
2024-11-11 19:49:24,006 Train Loss: 0.0005413, Val Loss: 0.0006619
2024-11-11 19:49:24,006 Epoch 1327/2000
2024-11-11 19:49:38,500 Current Learning Rate: 0.0029424282
2024-11-11 19:49:39,297 Train Loss: 0.0005771, Val Loss: 0.0006574
2024-11-11 19:49:39,297 Epoch 1328/2000
2024-11-11 19:49:53,818 Current Learning Rate: 0.0028711035
2024-11-11 19:49:54,593 Train Loss: 0.0005383, Val Loss: 0.0006545
2024-11-11 19:49:54,593 Epoch 1329/2000
2024-11-11 19:50:09,038 Current Learning Rate: 0.0028003042
2024-11-11 19:50:09,768 Train Loss: 0.0005376, Val Loss: 0.0006539
2024-11-11 19:50:09,768 Epoch 1330/2000
2024-11-11 19:50:24,475 Current Learning Rate: 0.0027300475
2024-11-11 19:50:25,189 Train Loss: 0.0005080, Val Loss: 0.0006516
2024-11-11 19:50:25,190 Epoch 1331/2000
2024-11-11 19:50:40,046 Current Learning Rate: 0.0026603509
2024-11-11 19:50:41,117 Train Loss: 0.0005167, Val Loss: 0.0006495
2024-11-11 19:50:41,117 Epoch 1332/2000
2024-11-11 19:50:56,357 Current Learning Rate: 0.0025912316
2024-11-11 19:50:56,358 Train Loss: 0.0005841, Val Loss: 0.0006509
2024-11-11 19:50:56,359 Epoch 1333/2000
2024-11-11 19:51:13,217 Current Learning Rate: 0.0025227067
2024-11-11 19:51:13,218 Train Loss: 0.0005029, Val Loss: 0.0006523
2024-11-11 19:51:13,218 Epoch 1334/2000
2024-11-11 19:51:28,576 Current Learning Rate: 0.0024547929
2024-11-11 19:51:28,576 Train Loss: 0.0005073, Val Loss: 0.0006524
2024-11-11 19:51:28,577 Epoch 1335/2000
2024-11-11 19:51:44,982 Current Learning Rate: 0.0023875072
2024-11-11 19:51:44,983 Train Loss: 0.0004979, Val Loss: 0.0006558
2024-11-11 19:51:44,983 Epoch 1336/2000
2024-11-11 19:52:01,889 Current Learning Rate: 0.0023208660
2024-11-11 19:52:04,546 Train Loss: 0.0005542, Val Loss: 0.0006464
2024-11-11 19:52:04,546 Epoch 1337/2000
2024-11-11 19:52:19,465 Current Learning Rate: 0.0022548859
2024-11-11 19:52:20,550 Train Loss: 0.0005587, Val Loss: 0.0006404
2024-11-11 19:52:20,550 Epoch 1338/2000
2024-11-11 19:52:36,007 Current Learning Rate: 0.0021895831
2024-11-11 19:52:36,770 Train Loss: 0.0005024, Val Loss: 0.0006403
2024-11-11 19:52:36,771 Epoch 1339/2000
2024-11-11 19:52:51,850 Current Learning Rate: 0.0021249737
2024-11-11 19:52:52,825 Train Loss: 0.0005034, Val Loss: 0.0006401
2024-11-11 19:52:52,826 Epoch 1340/2000
2024-11-11 19:53:08,283 Current Learning Rate: 0.0020610737
2024-11-11 19:53:09,365 Train Loss: 0.0004939, Val Loss: 0.0006363
2024-11-11 19:53:09,365 Epoch 1341/2000
2024-11-11 19:53:24,958 Current Learning Rate: 0.0019978989
2024-11-11 19:53:25,737 Train Loss: 0.0004918, Val Loss: 0.0006351
2024-11-11 19:53:25,738 Epoch 1342/2000
2024-11-11 19:53:40,705 Current Learning Rate: 0.0019354647
2024-11-11 19:53:41,502 Train Loss: 0.0005189, Val Loss: 0.0006343
2024-11-11 19:53:41,502 Epoch 1343/2000
2024-11-11 19:53:56,547 Current Learning Rate: 0.0018737867
2024-11-11 19:53:57,385 Train Loss: 0.0005233, Val Loss: 0.0006332
2024-11-11 19:53:57,385 Epoch 1344/2000
2024-11-11 19:54:12,439 Current Learning Rate: 0.0018128801
2024-11-11 19:54:13,530 Train Loss: 0.0005569, Val Loss: 0.0006325
2024-11-11 19:54:13,530 Epoch 1345/2000
2024-11-11 19:54:29,457 Current Learning Rate: 0.0017527598
2024-11-11 19:54:30,228 Train Loss: 0.0004619, Val Loss: 0.0006304
2024-11-11 19:54:30,229 Epoch 1346/2000
2024-11-11 19:54:44,678 Current Learning Rate: 0.0016934407
2024-11-11 19:54:45,428 Train Loss: 0.0004917, Val Loss: 0.0006289
2024-11-11 19:54:45,429 Epoch 1347/2000
2024-11-11 19:55:01,199 Current Learning Rate: 0.0016349374
2024-11-11 19:55:01,200 Train Loss: 0.0005784, Val Loss: 0.0006300
2024-11-11 19:55:01,200 Epoch 1348/2000
2024-11-11 19:55:16,580 Current Learning Rate: 0.0015772645
2024-11-11 19:55:17,277 Train Loss: 0.0005184, Val Loss: 0.0006288
2024-11-11 19:55:17,277 Epoch 1349/2000
2024-11-11 19:55:32,510 Current Learning Rate: 0.0015204360
2024-11-11 19:55:33,224 Train Loss: 0.0005124, Val Loss: 0.0006275
2024-11-11 19:55:33,224 Epoch 1350/2000
2024-11-11 19:55:48,242 Current Learning Rate: 0.0014644661
2024-11-11 19:55:48,968 Train Loss: 0.0005034, Val Loss: 0.0006267
2024-11-11 19:55:48,968 Epoch 1351/2000
2024-11-11 19:56:04,962 Current Learning Rate: 0.0014093685
2024-11-11 19:56:04,963 Train Loss: 0.0006049, Val Loss: 0.0006274
2024-11-11 19:56:04,963 Epoch 1352/2000
2024-11-11 19:56:21,231 Current Learning Rate: 0.0013551569
2024-11-11 19:56:22,015 Train Loss: 0.0005960, Val Loss: 0.0006250
2024-11-11 19:56:22,016 Epoch 1353/2000
2024-11-11 19:56:36,749 Current Learning Rate: 0.0013018445
2024-11-11 19:56:37,543 Train Loss: 0.0004573, Val Loss: 0.0006241
2024-11-11 19:56:37,544 Epoch 1354/2000
2024-11-11 19:56:52,284 Current Learning Rate: 0.0012494447
2024-11-11 19:56:53,035 Train Loss: 0.0004875, Val Loss: 0.0006239
2024-11-11 19:56:53,035 Epoch 1355/2000
2024-11-11 19:57:08,426 Current Learning Rate: 0.0011979702
2024-11-11 19:57:09,263 Train Loss: 0.0004818, Val Loss: 0.0006237
2024-11-11 19:57:09,264 Epoch 1356/2000
2024-11-11 19:57:23,711 Current Learning Rate: 0.0011474338
2024-11-11 19:57:24,519 Train Loss: 0.0004881, Val Loss: 0.0006232
2024-11-11 19:57:24,520 Epoch 1357/2000
2024-11-11 19:57:39,097 Current Learning Rate: 0.0010978480
2024-11-11 19:57:39,965 Train Loss: 0.0004620, Val Loss: 0.0006225
2024-11-11 19:57:39,966 Epoch 1358/2000
2024-11-11 19:57:54,537 Current Learning Rate: 0.0010492249
2024-11-11 19:57:54,538 Train Loss: 0.0005744, Val Loss: 0.0006229
2024-11-11 19:57:54,538 Epoch 1359/2000
2024-11-11 19:58:10,844 Current Learning Rate: 0.0010015767
2024-11-11 19:58:11,585 Train Loss: 0.0004765, Val Loss: 0.0006223
2024-11-11 19:58:11,585 Epoch 1360/2000
2024-11-11 19:58:26,382 Current Learning Rate: 0.0009549150
2024-11-11 19:58:27,213 Train Loss: 0.0005791, Val Loss: 0.0006222
2024-11-11 19:58:27,213 Epoch 1361/2000
2024-11-11 19:58:43,103 Current Learning Rate: 0.0009092514
2024-11-11 19:58:43,998 Train Loss: 0.0004861, Val Loss: 0.0006207
2024-11-11 19:58:43,998 Epoch 1362/2000
2024-11-11 19:58:59,476 Current Learning Rate: 0.0008645971
2024-11-11 19:59:00,494 Train Loss: 0.0005046, Val Loss: 0.0006202
2024-11-11 19:59:00,495 Epoch 1363/2000
2024-11-11 19:59:16,049 Current Learning Rate: 0.0008209632
2024-11-11 19:59:16,786 Train Loss: 0.0005233, Val Loss: 0.0006189
2024-11-11 19:59:16,786 Epoch 1364/2000
2024-11-11 19:59:31,459 Current Learning Rate: 0.0007783604
2024-11-11 19:59:31,460 Train Loss: 0.0005546, Val Loss: 0.0006190
2024-11-11 19:59:31,461 Epoch 1365/2000
2024-11-11 19:59:47,761 Current Learning Rate: 0.0007367992
2024-11-11 19:59:48,511 Train Loss: 0.0004817, Val Loss: 0.0006182
2024-11-11 19:59:48,512 Epoch 1366/2000
2024-11-11 20:00:04,295 Current Learning Rate: 0.0006962899
2024-11-11 20:00:05,153 Train Loss: 0.0005376, Val Loss: 0.0006179
2024-11-11 20:00:05,153 Epoch 1367/2000
2024-11-11 20:00:20,427 Current Learning Rate: 0.0006568424
2024-11-11 20:00:21,418 Train Loss: 0.0005212, Val Loss: 0.0006174
2024-11-11 20:00:21,418 Epoch 1368/2000
2024-11-11 20:00:37,084 Current Learning Rate: 0.0006184666
2024-11-11 20:00:38,080 Train Loss: 0.0005217, Val Loss: 0.0006170
2024-11-11 20:00:38,080 Epoch 1369/2000
2024-11-11 20:00:54,110 Current Learning Rate: 0.0005811718
2024-11-11 20:00:55,156 Train Loss: 0.0004690, Val Loss: 0.0006166
2024-11-11 20:00:55,157 Epoch 1370/2000
2024-11-11 20:01:10,897 Current Learning Rate: 0.0005449674
2024-11-11 20:01:11,730 Train Loss: 0.0004932, Val Loss: 0.0006164
2024-11-11 20:01:11,730 Epoch 1371/2000
2024-11-11 20:01:27,625 Current Learning Rate: 0.0005098621
2024-11-11 20:01:28,422 Train Loss: 0.0005213, Val Loss: 0.0006162
2024-11-11 20:01:28,423 Epoch 1372/2000
2024-11-11 20:01:43,386 Current Learning Rate: 0.0004758647
2024-11-11 20:01:44,315 Train Loss: 0.0004857, Val Loss: 0.0006157
2024-11-11 20:01:44,315 Epoch 1373/2000
2024-11-11 20:02:00,508 Current Learning Rate: 0.0004429836
2024-11-11 20:02:00,509 Train Loss: 0.0005455, Val Loss: 0.0006157
2024-11-11 20:02:00,509 Epoch 1374/2000
2024-11-11 20:02:16,044 Current Learning Rate: 0.0004112269
2024-11-11 20:02:16,879 Train Loss: 0.0004801, Val Loss: 0.0006153
2024-11-11 20:02:16,880 Epoch 1375/2000
2024-11-11 20:02:32,406 Current Learning Rate: 0.0003806023
2024-11-11 20:02:32,407 Train Loss: 0.0005578, Val Loss: 0.0006154
2024-11-11 20:02:32,408 Epoch 1376/2000
2024-11-11 20:02:48,159 Current Learning Rate: 0.0003511176
2024-11-11 20:02:48,957 Train Loss: 0.0004987, Val Loss: 0.0006150
2024-11-11 20:02:48,957 Epoch 1377/2000
2024-11-11 20:03:04,027 Current Learning Rate: 0.0003227798
2024-11-11 20:03:04,849 Train Loss: 0.0004501, Val Loss: 0.0006145
2024-11-11 20:03:04,849 Epoch 1378/2000
2024-11-11 20:03:19,727 Current Learning Rate: 0.0002955962
2024-11-11 20:03:20,514 Train Loss: 0.0005302, Val Loss: 0.0006143
2024-11-11 20:03:20,515 Epoch 1379/2000
2024-11-11 20:03:35,539 Current Learning Rate: 0.0002695732
2024-11-11 20:03:35,540 Train Loss: 0.0005470, Val Loss: 0.0006144
2024-11-11 20:03:35,540 Epoch 1380/2000
2024-11-11 20:03:52,709 Current Learning Rate: 0.0002447174
2024-11-11 20:03:53,559 Train Loss: 0.0004807, Val Loss: 0.0006140
2024-11-11 20:03:53,559 Epoch 1381/2000
2024-11-11 20:04:08,275 Current Learning Rate: 0.0002210349
2024-11-11 20:04:09,009 Train Loss: 0.0004747, Val Loss: 0.0006140
2024-11-11 20:04:09,010 Epoch 1382/2000
2024-11-11 20:04:25,007 Current Learning Rate: 0.0001985316
2024-11-11 20:04:25,809 Train Loss: 0.0004527, Val Loss: 0.0006137
2024-11-11 20:04:25,810 Epoch 1383/2000
2024-11-11 20:04:41,164 Current Learning Rate: 0.0001772129
2024-11-11 20:04:42,001 Train Loss: 0.0004655, Val Loss: 0.0006135
2024-11-11 20:04:42,001 Epoch 1384/2000
2024-11-11 20:04:57,280 Current Learning Rate: 0.0001570842
2024-11-11 20:04:58,004 Train Loss: 0.0005277, Val Loss: 0.0006132
2024-11-11 20:04:58,004 Epoch 1385/2000
2024-11-11 20:05:13,753 Current Learning Rate: 0.0001381504
2024-11-11 20:05:14,449 Train Loss: 0.0005367, Val Loss: 0.0006131
2024-11-11 20:05:14,450 Epoch 1386/2000
2024-11-11 20:05:30,359 Current Learning Rate: 0.0001204162
2024-11-11 20:05:31,109 Train Loss: 0.0004735, Val Loss: 0.0006130
2024-11-11 20:05:31,109 Epoch 1387/2000
2024-11-11 20:05:46,185 Current Learning Rate: 0.0001038859
2024-11-11 20:05:46,905 Train Loss: 0.0005416, Val Loss: 0.0006130
2024-11-11 20:05:46,905 Epoch 1388/2000
2024-11-11 20:06:01,720 Current Learning Rate: 0.0000885637
2024-11-11 20:06:04,291 Train Loss: 0.0005546, Val Loss: 0.0006130
2024-11-11 20:06:04,291 Epoch 1389/2000
2024-11-11 20:06:18,474 Current Learning Rate: 0.0000744534
2024-11-11 20:06:19,201 Train Loss: 0.0004698, Val Loss: 0.0006130
2024-11-11 20:06:19,201 Epoch 1390/2000
2024-11-11 20:06:34,181 Current Learning Rate: 0.0000615583
2024-11-11 20:06:34,780 Train Loss: 0.0005111, Val Loss: 0.0006129
2024-11-11 20:06:34,780 Epoch 1391/2000
2024-11-11 20:06:49,845 Current Learning Rate: 0.0000498817
2024-11-11 20:06:49,846 Train Loss: 0.0005220, Val Loss: 0.0006129
2024-11-11 20:06:49,846 Epoch 1392/2000
2024-11-11 20:07:05,676 Current Learning Rate: 0.0000394265
2024-11-11 20:07:06,525 Train Loss: 0.0005255, Val Loss: 0.0006129
2024-11-11 20:07:06,526 Epoch 1393/2000
2024-11-11 20:07:21,837 Current Learning Rate: 0.0000301952
2024-11-11 20:07:21,838 Train Loss: 0.0004672, Val Loss: 0.0006129
2024-11-11 20:07:21,839 Epoch 1394/2000
2024-11-11 20:07:37,788 Current Learning Rate: 0.0000221902
2024-11-11 20:07:38,536 Train Loss: 0.0004949, Val Loss: 0.0006128
2024-11-11 20:07:38,537 Epoch 1395/2000
2024-11-11 20:07:53,896 Current Learning Rate: 0.0000154133
2024-11-11 20:07:53,897 Train Loss: 0.0005009, Val Loss: 0.0006128
2024-11-11 20:07:53,897 Epoch 1396/2000
2024-11-11 20:08:10,114 Current Learning Rate: 0.0000098664
2024-11-11 20:08:10,896 Train Loss: 0.0004732, Val Loss: 0.0006128
2024-11-11 20:08:10,896 Epoch 1397/2000
2024-11-11 20:08:25,236 Current Learning Rate: 0.0000055506
2024-11-11 20:08:25,237 Train Loss: 0.0005249, Val Loss: 0.0006128
2024-11-11 20:08:25,237 Epoch 1398/2000
2024-11-11 20:08:41,681 Current Learning Rate: 0.0000024672
2024-11-11 20:08:42,420 Train Loss: 0.0004442, Val Loss: 0.0006128
2024-11-11 20:08:42,420 Epoch 1399/2000
2024-11-11 20:08:57,838 Current Learning Rate: 0.0000006168
2024-11-11 20:08:58,618 Train Loss: 0.0004666, Val Loss: 0.0006128
2024-11-11 20:08:58,618 Epoch 1400/2000
2024-11-11 20:09:14,324 Current Learning Rate: 0.0000000000
2024-11-11 20:09:14,325 Train Loss: 0.0005253, Val Loss: 0.0006129
2024-11-11 20:09:14,325 Epoch 1401/2000
2024-11-11 20:09:31,446 Current Learning Rate: 0.0000006168
2024-11-11 20:09:31,447 Train Loss: 0.0005096, Val Loss: 0.0006128
2024-11-11 20:09:31,447 Epoch 1402/2000
2024-11-11 20:09:48,001 Current Learning Rate: 0.0000024672
2024-11-11 20:09:48,002 Train Loss: 0.0004710, Val Loss: 0.0006128
2024-11-11 20:09:48,003 Epoch 1403/2000
2024-11-11 20:10:05,157 Current Learning Rate: 0.0000055506
2024-11-11 20:10:05,158 Train Loss: 0.0004903, Val Loss: 0.0006128
2024-11-11 20:10:05,158 Epoch 1404/2000
2024-11-11 20:10:21,151 Current Learning Rate: 0.0000098664
2024-11-11 20:10:21,152 Train Loss: 0.0004696, Val Loss: 0.0006128
2024-11-11 20:10:21,152 Epoch 1405/2000
2024-11-11 20:10:37,562 Current Learning Rate: 0.0000154133
2024-11-11 20:10:37,563 Train Loss: 0.0005107, Val Loss: 0.0006128
2024-11-11 20:10:37,563 Epoch 1406/2000
2024-11-11 20:10:53,670 Current Learning Rate: 0.0000221902
2024-11-11 20:10:54,735 Train Loss: 0.0004813, Val Loss: 0.0006128
2024-11-11 20:10:54,736 Epoch 1407/2000
2024-11-11 20:11:10,847 Current Learning Rate: 0.0000301952
2024-11-11 20:11:10,848 Train Loss: 0.0005241, Val Loss: 0.0006128
2024-11-11 20:11:10,848 Epoch 1408/2000
2024-11-11 20:11:27,126 Current Learning Rate: 0.0000394265
2024-11-11 20:11:27,127 Train Loss: 0.0004872, Val Loss: 0.0006128
2024-11-11 20:11:27,127 Epoch 1409/2000
2024-11-11 20:11:42,746 Current Learning Rate: 0.0000498817
2024-11-11 20:11:42,747 Train Loss: 0.0004862, Val Loss: 0.0006128
2024-11-11 20:11:42,747 Epoch 1410/2000
2024-11-11 20:11:58,066 Current Learning Rate: 0.0000615583
2024-11-11 20:11:58,066 Train Loss: 0.0004675, Val Loss: 0.0006128
2024-11-11 20:11:58,066 Epoch 1411/2000
2024-11-11 20:12:13,442 Current Learning Rate: 0.0000744534
2024-11-11 20:12:13,443 Train Loss: 0.0004975, Val Loss: 0.0006129
2024-11-11 20:12:13,443 Epoch 1412/2000
2024-11-11 20:12:28,781 Current Learning Rate: 0.0000885637
2024-11-11 20:12:29,566 Train Loss: 0.0004550, Val Loss: 0.0006127
2024-11-11 20:12:29,567 Epoch 1413/2000
2024-11-11 20:12:44,260 Current Learning Rate: 0.0001038859
2024-11-11 20:12:44,261 Train Loss: 0.0005011, Val Loss: 0.0006128
2024-11-11 20:12:44,261 Epoch 1414/2000
2024-11-11 20:12:59,904 Current Learning Rate: 0.0001204162
2024-11-11 20:12:59,904 Train Loss: 0.0005198, Val Loss: 0.0006129
2024-11-11 20:12:59,904 Epoch 1415/2000
2024-11-11 20:13:15,146 Current Learning Rate: 0.0001381504
2024-11-11 20:13:15,147 Train Loss: 0.0004447, Val Loss: 0.0006128
2024-11-11 20:13:15,147 Epoch 1416/2000
2024-11-11 20:13:30,686 Current Learning Rate: 0.0001570842
2024-11-11 20:13:30,687 Train Loss: 0.0004449, Val Loss: 0.0006128
2024-11-11 20:13:30,687 Epoch 1417/2000
2024-11-11 20:13:46,706 Current Learning Rate: 0.0001772129
2024-11-11 20:13:46,707 Train Loss: 0.0005148, Val Loss: 0.0006130
2024-11-11 20:13:46,707 Epoch 1418/2000
2024-11-11 20:14:03,613 Current Learning Rate: 0.0001985316
2024-11-11 20:14:03,613 Train Loss: 0.0004517, Val Loss: 0.0006130
2024-11-11 20:14:03,614 Epoch 1419/2000
2024-11-11 20:14:19,892 Current Learning Rate: 0.0002210349
2024-11-11 20:14:19,892 Train Loss: 0.0004902, Val Loss: 0.0006130
2024-11-11 20:14:19,893 Epoch 1420/2000
2024-11-11 20:14:36,221 Current Learning Rate: 0.0002447174
2024-11-11 20:14:36,222 Train Loss: 0.0004996, Val Loss: 0.0006131
2024-11-11 20:14:36,222 Epoch 1421/2000
2024-11-11 20:14:52,817 Current Learning Rate: 0.0002695732
2024-11-11 20:14:52,817 Train Loss: 0.0005237, Val Loss: 0.0006131
2024-11-11 20:14:52,818 Epoch 1422/2000
2024-11-11 20:15:08,077 Current Learning Rate: 0.0002955962
2024-11-11 20:15:08,077 Train Loss: 0.0004482, Val Loss: 0.0006132
2024-11-11 20:15:08,078 Epoch 1423/2000
2024-11-11 20:15:24,080 Current Learning Rate: 0.0003227798
2024-11-11 20:15:24,080 Train Loss: 0.0004680, Val Loss: 0.0006132
2024-11-11 20:15:24,080 Epoch 1424/2000
2024-11-11 20:15:40,211 Current Learning Rate: 0.0003511176
2024-11-11 20:15:40,211 Train Loss: 0.0005434, Val Loss: 0.0006135
2024-11-11 20:15:40,211 Epoch 1425/2000
2024-11-11 20:15:56,512 Current Learning Rate: 0.0003806023
2024-11-11 20:15:56,512 Train Loss: 0.0005129, Val Loss: 0.0006135
2024-11-11 20:15:56,513 Epoch 1426/2000
2024-11-11 20:16:12,728 Current Learning Rate: 0.0004112269
2024-11-11 20:16:12,728 Train Loss: 0.0005002, Val Loss: 0.0006136
2024-11-11 20:16:12,728 Epoch 1427/2000
2024-11-11 20:16:28,624 Current Learning Rate: 0.0004429836
2024-11-11 20:16:28,624 Train Loss: 0.0005272, Val Loss: 0.0006138
2024-11-11 20:16:28,624 Epoch 1428/2000
2024-11-11 20:16:44,393 Current Learning Rate: 0.0004758647
2024-11-11 20:16:44,394 Train Loss: 0.0004807, Val Loss: 0.0006142
2024-11-11 20:16:44,394 Epoch 1429/2000
2024-11-11 20:16:59,941 Current Learning Rate: 0.0005098621
2024-11-11 20:16:59,941 Train Loss: 0.0004729, Val Loss: 0.0006140
2024-11-11 20:16:59,941 Epoch 1430/2000
2024-11-11 20:17:15,698 Current Learning Rate: 0.0005449674
2024-11-11 20:17:15,699 Train Loss: 0.0004769, Val Loss: 0.0006139
2024-11-11 20:17:15,699 Epoch 1431/2000
2024-11-11 20:17:31,241 Current Learning Rate: 0.0005811718
2024-11-11 20:17:31,242 Train Loss: 0.0005230, Val Loss: 0.0006142
2024-11-11 20:17:31,242 Epoch 1432/2000
2024-11-11 20:17:46,743 Current Learning Rate: 0.0006184666
2024-11-11 20:17:46,743 Train Loss: 0.0005331, Val Loss: 0.0006143
2024-11-11 20:17:46,743 Epoch 1433/2000
2024-11-11 20:18:02,340 Current Learning Rate: 0.0006568424
2024-11-11 20:18:02,340 Train Loss: 0.0004917, Val Loss: 0.0006143
2024-11-11 20:18:02,340 Epoch 1434/2000
2024-11-11 20:18:17,752 Current Learning Rate: 0.0006962899
2024-11-11 20:18:17,753 Train Loss: 0.0004734, Val Loss: 0.0006141
2024-11-11 20:18:17,753 Epoch 1435/2000
2024-11-11 20:18:33,356 Current Learning Rate: 0.0007367992
2024-11-11 20:18:33,356 Train Loss: 0.0004983, Val Loss: 0.0006142
2024-11-11 20:18:33,357 Epoch 1436/2000
2024-11-11 20:18:49,180 Current Learning Rate: 0.0007783604
2024-11-11 20:18:49,181 Train Loss: 0.0005069, Val Loss: 0.0006153
2024-11-11 20:18:49,181 Epoch 1437/2000
2024-11-11 20:19:04,667 Current Learning Rate: 0.0008209632
2024-11-11 20:19:04,668 Train Loss: 0.0004598, Val Loss: 0.0006165
2024-11-11 20:19:04,668 Epoch 1438/2000
2024-11-11 20:19:21,372 Current Learning Rate: 0.0008645971
2024-11-11 20:19:21,373 Train Loss: 0.0005561, Val Loss: 0.0006181
2024-11-11 20:19:21,374 Epoch 1439/2000
2024-11-11 20:19:37,910 Current Learning Rate: 0.0009092514
2024-11-11 20:19:37,911 Train Loss: 0.0004470, Val Loss: 0.0006189
2024-11-11 20:19:37,911 Epoch 1440/2000
2024-11-11 20:19:53,435 Current Learning Rate: 0.0009549150
2024-11-11 20:19:53,436 Train Loss: 0.0004760, Val Loss: 0.0006184
2024-11-11 20:19:53,436 Epoch 1441/2000
2024-11-11 20:20:09,808 Current Learning Rate: 0.0010015767
2024-11-11 20:20:09,809 Train Loss: 0.0005692, Val Loss: 0.0006224
2024-11-11 20:20:09,809 Epoch 1442/2000
2024-11-11 20:20:25,028 Current Learning Rate: 0.0010492249
2024-11-11 20:20:25,029 Train Loss: 0.0004826, Val Loss: 0.0006208
2024-11-11 20:20:25,029 Epoch 1443/2000
2024-11-11 20:20:40,772 Current Learning Rate: 0.0010978480
2024-11-11 20:20:40,773 Train Loss: 0.0005237, Val Loss: 0.0006214
2024-11-11 20:20:40,773 Epoch 1444/2000
2024-11-11 20:20:56,856 Current Learning Rate: 0.0011474338
2024-11-11 20:20:56,856 Train Loss: 0.0005004, Val Loss: 0.0006202
2024-11-11 20:20:56,857 Epoch 1445/2000
2024-11-11 20:21:12,767 Current Learning Rate: 0.0011979702
2024-11-11 20:21:12,768 Train Loss: 0.0005248, Val Loss: 0.0006198
2024-11-11 20:21:12,769 Epoch 1446/2000
2024-11-11 20:21:28,844 Current Learning Rate: 0.0012494447
2024-11-11 20:21:28,844 Train Loss: 0.0005265, Val Loss: 0.0006207
2024-11-11 20:21:28,844 Epoch 1447/2000
2024-11-11 20:21:44,071 Current Learning Rate: 0.0013018445
2024-11-11 20:21:44,071 Train Loss: 0.0004981, Val Loss: 0.0006168
2024-11-11 20:21:44,071 Epoch 1448/2000
2024-11-11 20:21:59,511 Current Learning Rate: 0.0013551569
2024-11-11 20:21:59,512 Train Loss: 0.0004470, Val Loss: 0.0006150
2024-11-11 20:21:59,512 Epoch 1449/2000
2024-11-11 20:22:15,216 Current Learning Rate: 0.0014093685
2024-11-11 20:22:15,217 Train Loss: 0.0004914, Val Loss: 0.0006155
2024-11-11 20:22:15,217 Epoch 1450/2000
2024-11-11 20:22:30,529 Current Learning Rate: 0.0014644661
2024-11-11 20:22:30,529 Train Loss: 0.0005055, Val Loss: 0.0006161
2024-11-11 20:22:30,529 Epoch 1451/2000
2024-11-11 20:22:46,394 Current Learning Rate: 0.0015204360
2024-11-11 20:22:46,394 Train Loss: 0.0005414, Val Loss: 0.0006216
2024-11-11 20:22:46,395 Epoch 1452/2000
2024-11-11 20:23:02,337 Current Learning Rate: 0.0015772645
2024-11-11 20:23:02,338 Train Loss: 0.0004972, Val Loss: 0.0006162
2024-11-11 20:23:02,338 Epoch 1453/2000
2024-11-11 20:23:18,306 Current Learning Rate: 0.0016349374
2024-11-11 20:23:18,306 Train Loss: 0.0004740, Val Loss: 0.0006138
2024-11-11 20:23:18,306 Epoch 1454/2000
2024-11-11 20:23:33,983 Current Learning Rate: 0.0016934407
2024-11-11 20:23:33,983 Train Loss: 0.0004649, Val Loss: 0.0006135
2024-11-11 20:23:33,984 Epoch 1455/2000
2024-11-11 20:23:50,081 Current Learning Rate: 0.0017527598
2024-11-11 20:23:50,082 Train Loss: 0.0005251, Val Loss: 0.0006147
2024-11-11 20:23:50,082 Epoch 1456/2000
2024-11-11 20:24:05,610 Current Learning Rate: 0.0018128801
2024-11-11 20:24:05,611 Train Loss: 0.0004668, Val Loss: 0.0006150
2024-11-11 20:24:05,611 Epoch 1457/2000
2024-11-11 20:24:21,303 Current Learning Rate: 0.0018737867
2024-11-11 20:24:21,303 Train Loss: 0.0004783, Val Loss: 0.0006162
2024-11-11 20:24:21,304 Epoch 1458/2000
2024-11-11 20:24:37,834 Current Learning Rate: 0.0019354647
2024-11-11 20:24:37,834 Train Loss: 0.0004964, Val Loss: 0.0006145
2024-11-11 20:24:37,835 Epoch 1459/2000
2024-11-11 20:24:53,164 Current Learning Rate: 0.0019978989
2024-11-11 20:24:53,164 Train Loss: 0.0005357, Val Loss: 0.0006147
2024-11-11 20:24:53,165 Epoch 1460/2000
2024-11-11 20:25:09,191 Current Learning Rate: 0.0020610737
2024-11-11 20:25:09,192 Train Loss: 0.0004967, Val Loss: 0.0006198
2024-11-11 20:25:09,192 Epoch 1461/2000
2024-11-11 20:25:24,449 Current Learning Rate: 0.0021249737
2024-11-11 20:25:24,450 Train Loss: 0.0004676, Val Loss: 0.0006166
2024-11-11 20:25:24,450 Epoch 1462/2000
2024-11-11 20:25:40,590 Current Learning Rate: 0.0021895831
2024-11-11 20:25:40,590 Train Loss: 0.0005227, Val Loss: 0.0006166
2024-11-11 20:25:40,590 Epoch 1463/2000
2024-11-11 20:25:55,846 Current Learning Rate: 0.0022548859
2024-11-11 20:25:55,846 Train Loss: 0.0004782, Val Loss: 0.0006176
2024-11-11 20:25:55,847 Epoch 1464/2000
2024-11-11 20:26:11,954 Current Learning Rate: 0.0023208660
2024-11-11 20:26:11,955 Train Loss: 0.0004753, Val Loss: 0.0006173
2024-11-11 20:26:11,955 Epoch 1465/2000
2024-11-11 20:26:27,909 Current Learning Rate: 0.0023875072
2024-11-11 20:26:27,909 Train Loss: 0.0005405, Val Loss: 0.0006557
2024-11-11 20:26:27,909 Epoch 1466/2000
2024-11-11 20:26:43,062 Current Learning Rate: 0.0024547929
2024-11-11 20:26:43,062 Train Loss: 0.0004659, Val Loss: 0.0006196
2024-11-11 20:26:43,062 Epoch 1467/2000
2024-11-11 20:26:59,145 Current Learning Rate: 0.0025227067
2024-11-11 20:26:59,146 Train Loss: 0.0005794, Val Loss: 0.0006680
2024-11-11 20:26:59,146 Epoch 1468/2000
2024-11-11 20:27:15,435 Current Learning Rate: 0.0025912316
2024-11-11 20:27:15,435 Train Loss: 0.0005062, Val Loss: 0.0006347
2024-11-11 20:27:15,435 Epoch 1469/2000
2024-11-11 20:27:31,261 Current Learning Rate: 0.0026603509
2024-11-11 20:27:31,262 Train Loss: 0.0004669, Val Loss: 0.0006229
2024-11-11 20:27:31,262 Epoch 1470/2000
2024-11-11 20:27:47,063 Current Learning Rate: 0.0027300475
2024-11-11 20:27:47,064 Train Loss: 0.0005142, Val Loss: 0.0006265
2024-11-11 20:27:47,064 Epoch 1471/2000
2024-11-11 20:28:02,366 Current Learning Rate: 0.0028003042
2024-11-11 20:28:02,367 Train Loss: 0.0005021, Val Loss: 0.0006402
2024-11-11 20:28:02,367 Epoch 1472/2000
2024-11-11 20:28:18,150 Current Learning Rate: 0.0028711035
2024-11-11 20:28:18,150 Train Loss: 0.0004934, Val Loss: 0.0006229
2024-11-11 20:28:18,150 Epoch 1473/2000
2024-11-11 20:28:33,929 Current Learning Rate: 0.0029424282
2024-11-11 20:28:33,929 Train Loss: 0.0004801, Val Loss: 0.0006213
2024-11-11 20:28:33,930 Epoch 1474/2000
2024-11-11 20:28:49,658 Current Learning Rate: 0.0030142605
2024-11-11 20:28:49,659 Train Loss: 0.0005335, Val Loss: 0.0006207
2024-11-11 20:28:49,659 Epoch 1475/2000
2024-11-11 20:29:05,508 Current Learning Rate: 0.0030865828
2024-11-11 20:29:05,509 Train Loss: 0.0005787, Val Loss: 0.0007002
2024-11-11 20:29:05,509 Epoch 1476/2000
2024-11-11 20:29:21,614 Current Learning Rate: 0.0031593772
2024-11-11 20:29:21,614 Train Loss: 0.0005273, Val Loss: 0.0006420
2024-11-11 20:29:21,615 Epoch 1477/2000
2024-11-11 20:29:37,422 Current Learning Rate: 0.0032326258
2024-11-11 20:29:37,422 Train Loss: 0.0005064, Val Loss: 0.0006591
2024-11-11 20:29:37,422 Epoch 1478/2000
2024-11-11 20:29:54,498 Current Learning Rate: 0.0033063104
2024-11-11 20:29:54,498 Train Loss: 0.0005649, Val Loss: 0.0006485
2024-11-11 20:29:54,499 Epoch 1479/2000
2024-11-11 20:30:10,689 Current Learning Rate: 0.0033804129
2024-11-11 20:30:10,690 Train Loss: 0.0005134, Val Loss: 0.0006413
2024-11-11 20:30:10,690 Epoch 1480/2000
2024-11-11 20:30:26,778 Current Learning Rate: 0.0034549150
2024-11-11 20:30:26,778 Train Loss: 0.0006216, Val Loss: 0.0006586
2024-11-11 20:30:26,778 Epoch 1481/2000
2024-11-11 20:30:42,425 Current Learning Rate: 0.0035297984
2024-11-11 20:30:42,425 Train Loss: 0.0005311, Val Loss: 0.0006958
2024-11-11 20:30:42,425 Epoch 1482/2000
2024-11-11 20:30:57,934 Current Learning Rate: 0.0036050445
2024-11-11 20:30:57,935 Train Loss: 0.0005394, Val Loss: 0.0006506
2024-11-11 20:30:57,935 Epoch 1483/2000
2024-11-11 20:31:14,981 Current Learning Rate: 0.0036806348
2024-11-11 20:31:14,981 Train Loss: 0.0005059, Val Loss: 0.0006382
2024-11-11 20:31:14,982 Epoch 1484/2000
2024-11-11 20:31:30,773 Current Learning Rate: 0.0037565506
2024-11-11 20:31:30,773 Train Loss: 0.0005005, Val Loss: 0.0006381
2024-11-11 20:31:30,773 Epoch 1485/2000
2024-11-11 20:31:46,635 Current Learning Rate: 0.0038327732
2024-11-11 20:31:46,635 Train Loss: 0.0005609, Val Loss: 0.0006412
2024-11-11 20:31:46,636 Epoch 1486/2000
2024-11-11 20:32:03,296 Current Learning Rate: 0.0039092838
2024-11-11 20:32:03,297 Train Loss: 0.0006931, Val Loss: 0.0006978
2024-11-11 20:32:03,297 Epoch 1487/2000
2024-11-11 20:32:20,023 Current Learning Rate: 0.0039860635
2024-11-11 20:32:20,023 Train Loss: 0.0005532, Val Loss: 0.0006877
2024-11-11 20:32:20,023 Epoch 1488/2000
2024-11-11 20:32:35,457 Current Learning Rate: 0.0040630934
2024-11-11 20:32:35,457 Train Loss: 0.0005985, Val Loss: 0.0006451
2024-11-11 20:32:35,458 Epoch 1489/2000
2024-11-11 20:32:51,293 Current Learning Rate: 0.0041403545
2024-11-11 20:32:51,293 Train Loss: 0.0005420, Val Loss: 0.0006564
2024-11-11 20:32:51,293 Epoch 1490/2000
2024-11-11 20:33:08,048 Current Learning Rate: 0.0042178277
2024-11-11 20:33:08,048 Train Loss: 0.0006146, Val Loss: 0.0006611
2024-11-11 20:33:08,048 Epoch 1491/2000
2024-11-11 20:33:24,755 Current Learning Rate: 0.0042954938
2024-11-11 20:33:24,756 Train Loss: 0.0005250, Val Loss: 0.0006514
2024-11-11 20:33:24,756 Epoch 1492/2000
2024-11-11 20:33:41,304 Current Learning Rate: 0.0043733338
2024-11-11 20:33:41,305 Train Loss: 0.0004744, Val Loss: 0.0006579
2024-11-11 20:33:41,305 Epoch 1493/2000
2024-11-11 20:33:58,195 Current Learning Rate: 0.0044513284
2024-11-11 20:33:58,196 Train Loss: 0.0005280, Val Loss: 0.0006560
2024-11-11 20:33:58,196 Epoch 1494/2000
2024-11-11 20:34:13,641 Current Learning Rate: 0.0045294584
2024-11-11 20:34:13,641 Train Loss: 0.0004900, Val Loss: 0.0006516
2024-11-11 20:34:13,641 Epoch 1495/2000
2024-11-11 20:34:30,156 Current Learning Rate: 0.0046077045
2024-11-11 20:34:30,157 Train Loss: 0.0007456, Val Loss: 0.0007136
2024-11-11 20:34:30,157 Epoch 1496/2000
2024-11-11 20:34:46,606 Current Learning Rate: 0.0046860474
2024-11-11 20:34:46,606 Train Loss: 0.0005671, Val Loss: 0.0006747
2024-11-11 20:34:46,606 Epoch 1497/2000
2024-11-11 20:35:02,765 Current Learning Rate: 0.0047644677
2024-11-11 20:35:02,765 Train Loss: 0.0006108, Val Loss: 0.0007261
2024-11-11 20:35:02,765 Epoch 1498/2000
2024-11-11 20:35:19,650 Current Learning Rate: 0.0048429462
2024-11-11 20:35:19,650 Train Loss: 0.0006097, Val Loss: 0.0006788
2024-11-11 20:35:19,651 Epoch 1499/2000
2024-11-11 20:35:36,023 Current Learning Rate: 0.0049214634
2024-11-11 20:35:36,023 Train Loss: 0.0005545, Val Loss: 0.0006714
2024-11-11 20:35:36,023 Epoch 1500/2000
2024-11-11 20:35:52,077 Current Learning Rate: 0.0050000000
2024-11-11 20:35:52,077 Train Loss: 0.0005661, Val Loss: 0.0006809
2024-11-11 20:35:52,077 Epoch 1501/2000
2024-11-11 20:36:08,282 Current Learning Rate: 0.0050785366
2024-11-11 20:36:08,282 Train Loss: 0.0005054, Val Loss: 0.0006400
2024-11-11 20:36:08,282 Epoch 1502/2000
2024-11-11 20:36:24,742 Current Learning Rate: 0.0051570538
2024-11-11 20:36:24,742 Train Loss: 0.0005751, Val Loss: 0.0006711
2024-11-11 20:36:24,742 Epoch 1503/2000
2024-11-11 20:36:39,922 Current Learning Rate: 0.0052355323
2024-11-11 20:36:39,923 Train Loss: 0.0005303, Val Loss: 0.0006609
2024-11-11 20:36:39,923 Epoch 1504/2000
2024-11-11 20:36:55,760 Current Learning Rate: 0.0053139526
2024-11-11 20:36:55,760 Train Loss: 0.0005221, Val Loss: 0.0006698
2024-11-11 20:36:55,761 Epoch 1505/2000
2024-11-11 20:37:11,953 Current Learning Rate: 0.0053922955
2024-11-11 20:37:11,954 Train Loss: 0.0005451, Val Loss: 0.0007014
2024-11-11 20:37:11,954 Epoch 1506/2000
2024-11-11 20:37:27,969 Current Learning Rate: 0.0054705416
2024-11-11 20:37:27,970 Train Loss: 0.0006498, Val Loss: 0.0007386
2024-11-11 20:37:27,970 Epoch 1507/2000
2024-11-11 20:37:43,495 Current Learning Rate: 0.0055486716
2024-11-11 20:37:43,495 Train Loss: 0.0005905, Val Loss: 0.0006920
2024-11-11 20:37:43,495 Epoch 1508/2000
2024-11-11 20:38:00,272 Current Learning Rate: 0.0056266662
2024-11-11 20:38:00,272 Train Loss: 0.0005553, Val Loss: 0.0006715
2024-11-11 20:38:00,272 Epoch 1509/2000
2024-11-11 20:38:16,144 Current Learning Rate: 0.0057045062
2024-11-11 20:38:16,144 Train Loss: 0.0005249, Val Loss: 0.0006635
2024-11-11 20:38:16,144 Epoch 1510/2000
2024-11-11 20:38:32,382 Current Learning Rate: 0.0057821723
2024-11-11 20:38:32,382 Train Loss: 0.0006379, Val Loss: 0.0007383
2024-11-11 20:38:32,382 Epoch 1511/2000
2024-11-11 20:38:47,553 Current Learning Rate: 0.0058596455
2024-11-11 20:38:47,553 Train Loss: 0.0006241, Val Loss: 0.0007068
2024-11-11 20:38:47,554 Epoch 1512/2000
2024-11-11 20:39:03,014 Current Learning Rate: 0.0059369066
2024-11-11 20:39:03,015 Train Loss: 0.0005958, Val Loss: 0.0006902
2024-11-11 20:39:03,015 Epoch 1513/2000
2024-11-11 20:39:19,176 Current Learning Rate: 0.0060139365
2024-11-11 20:39:19,176 Train Loss: 0.0005927, Val Loss: 0.0006851
2024-11-11 20:39:19,176 Epoch 1514/2000
2024-11-11 20:39:35,681 Current Learning Rate: 0.0060907162
2024-11-11 20:39:35,681 Train Loss: 0.0005849, Val Loss: 0.0006862
2024-11-11 20:39:35,682 Epoch 1515/2000
2024-11-11 20:39:51,533 Current Learning Rate: 0.0061672268
2024-11-11 20:39:51,534 Train Loss: 0.0005991, Val Loss: 0.0007278
2024-11-11 20:39:51,534 Epoch 1516/2000
2024-11-11 20:40:07,384 Current Learning Rate: 0.0062434494
2024-11-11 20:40:07,385 Train Loss: 0.0006466, Val Loss: 0.0007101
2024-11-11 20:40:07,385 Epoch 1517/2000
2024-11-11 20:40:24,452 Current Learning Rate: 0.0063193652
2024-11-11 20:40:24,452 Train Loss: 0.0005909, Val Loss: 0.0007273
2024-11-11 20:40:24,453 Epoch 1518/2000
2024-11-11 20:40:40,274 Current Learning Rate: 0.0063949555
2024-11-11 20:40:40,274 Train Loss: 0.0006418, Val Loss: 0.0007695
2024-11-11 20:40:40,274 Epoch 1519/2000
2024-11-11 20:40:55,638 Current Learning Rate: 0.0064702016
2024-11-11 20:40:55,638 Train Loss: 0.0006079, Val Loss: 0.0007664
2024-11-11 20:40:55,638 Epoch 1520/2000
2024-11-11 20:41:10,975 Current Learning Rate: 0.0065450850
2024-11-11 20:41:10,975 Train Loss: 0.0006324, Val Loss: 0.0007451
2024-11-11 20:41:10,976 Epoch 1521/2000
2024-11-11 20:41:26,362 Current Learning Rate: 0.0066195871
2024-11-11 20:41:26,362 Train Loss: 0.0005829, Val Loss: 0.0007283
2024-11-11 20:41:26,362 Epoch 1522/2000
2024-11-11 20:41:41,693 Current Learning Rate: 0.0066936896
2024-11-11 20:41:41,694 Train Loss: 0.0005885, Val Loss: 0.0007237
2024-11-11 20:41:41,694 Epoch 1523/2000
2024-11-11 20:41:57,294 Current Learning Rate: 0.0067673742
2024-11-11 20:41:57,295 Train Loss: 0.0006465, Val Loss: 0.0007127
2024-11-11 20:41:57,295 Epoch 1524/2000
2024-11-11 20:42:13,255 Current Learning Rate: 0.0068406228
2024-11-11 20:42:13,256 Train Loss: 0.0005241, Val Loss: 0.0007018
2024-11-11 20:42:13,256 Epoch 1525/2000
2024-11-11 20:42:29,226 Current Learning Rate: 0.0069134172
2024-11-11 20:42:29,226 Train Loss: 0.0005400, Val Loss: 0.0006822
2024-11-11 20:42:29,227 Epoch 1526/2000
2024-11-11 20:42:45,445 Current Learning Rate: 0.0069857395
2024-11-11 20:42:45,445 Train Loss: 0.0006432, Val Loss: 0.0006552
2024-11-11 20:42:45,445 Epoch 1527/2000
2024-11-11 20:43:02,102 Current Learning Rate: 0.0070575718
2024-11-11 20:43:02,102 Train Loss: 0.0005796, Val Loss: 0.0007324
2024-11-11 20:43:02,102 Epoch 1528/2000
2024-11-11 20:43:17,896 Current Learning Rate: 0.0071288965
2024-11-11 20:43:17,896 Train Loss: 0.0005811, Val Loss: 0.0007003
2024-11-11 20:43:17,897 Epoch 1529/2000
2024-11-11 20:43:33,769 Current Learning Rate: 0.0071996958
2024-11-11 20:43:33,770 Train Loss: 0.0005254, Val Loss: 0.0006856
2024-11-11 20:43:33,771 Epoch 1530/2000
2024-11-11 20:43:50,678 Current Learning Rate: 0.0072699525
2024-11-11 20:43:50,679 Train Loss: 0.0007053, Val Loss: 0.0007410
2024-11-11 20:43:50,680 Epoch 1531/2000
2024-11-11 20:44:06,548 Current Learning Rate: 0.0073396491
2024-11-11 20:44:06,549 Train Loss: 0.0007030, Val Loss: 0.0006901
2024-11-11 20:44:06,549 Epoch 1532/2000
2024-11-11 20:44:22,515 Current Learning Rate: 0.0074087684
2024-11-11 20:44:22,516 Train Loss: 0.0005726, Val Loss: 0.0006696
2024-11-11 20:44:22,516 Epoch 1533/2000
2024-11-11 20:44:38,820 Current Learning Rate: 0.0074772933
2024-11-11 20:44:38,821 Train Loss: 0.0005782, Val Loss: 0.0006637
2024-11-11 20:44:38,821 Epoch 1534/2000
2024-11-11 20:44:54,761 Current Learning Rate: 0.0075452071
2024-11-11 20:44:54,761 Train Loss: 0.0005654, Val Loss: 0.0006796
2024-11-11 20:44:54,762 Epoch 1535/2000
2024-11-11 20:45:10,162 Current Learning Rate: 0.0076124928
2024-11-11 20:45:10,163 Train Loss: 0.0005819, Val Loss: 0.0007103
2024-11-11 20:45:10,163 Epoch 1536/2000
2024-11-11 20:45:25,934 Current Learning Rate: 0.0076791340
2024-11-11 20:45:25,934 Train Loss: 0.0006324, Val Loss: 0.0007561
2024-11-11 20:45:25,934 Epoch 1537/2000
2024-11-11 20:45:41,402 Current Learning Rate: 0.0077451141
2024-11-11 20:45:41,402 Train Loss: 0.0005841, Val Loss: 0.0007257
2024-11-11 20:45:41,403 Epoch 1538/2000
2024-11-11 20:45:56,820 Current Learning Rate: 0.0078104169
2024-11-11 20:45:56,821 Train Loss: 0.0006216, Val Loss: 0.0007181
2024-11-11 20:45:56,821 Epoch 1539/2000
2024-11-11 20:46:12,328 Current Learning Rate: 0.0078750263
2024-11-11 20:46:12,328 Train Loss: 0.0006122, Val Loss: 0.0006882
2024-11-11 20:46:12,329 Epoch 1540/2000
2024-11-11 20:46:28,022 Current Learning Rate: 0.0079389263
2024-11-11 20:46:28,022 Train Loss: 0.0006561, Val Loss: 0.0007055
2024-11-11 20:46:28,023 Epoch 1541/2000
2024-11-11 20:46:44,403 Current Learning Rate: 0.0080021011
2024-11-11 20:46:44,404 Train Loss: 0.0006346, Val Loss: 0.0007708
2024-11-11 20:46:44,404 Epoch 1542/2000
2024-11-11 20:47:00,499 Current Learning Rate: 0.0080645353
2024-11-11 20:47:00,499 Train Loss: 0.0007184, Val Loss: 0.0007942
2024-11-11 20:47:00,500 Epoch 1543/2000
2024-11-11 20:47:16,735 Current Learning Rate: 0.0081262133
2024-11-11 20:47:16,736 Train Loss: 0.0006839, Val Loss: 0.0008166
2024-11-11 20:47:16,736 Epoch 1544/2000
2024-11-11 20:47:32,587 Current Learning Rate: 0.0081871199
2024-11-11 20:47:32,588 Train Loss: 0.0006820, Val Loss: 0.0007634
2024-11-11 20:47:32,588 Epoch 1545/2000
2024-11-11 20:47:48,165 Current Learning Rate: 0.0082472402
2024-11-11 20:47:48,165 Train Loss: 0.0006341, Val Loss: 0.0007432
2024-11-11 20:47:48,165 Epoch 1546/2000
2024-11-11 20:48:03,330 Current Learning Rate: 0.0083065593
2024-11-11 20:48:03,330 Train Loss: 0.0006256, Val Loss: 0.0007161
2024-11-11 20:48:03,331 Epoch 1547/2000
2024-11-11 20:48:18,658 Current Learning Rate: 0.0083650626
2024-11-11 20:48:18,659 Train Loss: 0.0006356, Val Loss: 0.0006923
2024-11-11 20:48:18,659 Epoch 1548/2000
2024-11-11 20:48:34,173 Current Learning Rate: 0.0084227355
2024-11-11 20:48:34,174 Train Loss: 0.0005454, Val Loss: 0.0006823
2024-11-11 20:48:34,174 Epoch 1549/2000
2024-11-11 20:48:49,519 Current Learning Rate: 0.0084795640
2024-11-11 20:48:49,520 Train Loss: 0.0006010, Val Loss: 0.0006703
2024-11-11 20:48:49,520 Epoch 1550/2000
2024-11-11 20:49:05,189 Current Learning Rate: 0.0085355339
2024-11-11 20:49:05,190 Train Loss: 0.0006627, Val Loss: 0.0006890
2024-11-11 20:49:05,190 Epoch 1551/2000
2024-11-11 20:49:20,460 Current Learning Rate: 0.0085906315
2024-11-11 20:49:20,460 Train Loss: 0.0007621, Val Loss: 0.0007254
2024-11-11 20:49:20,460 Epoch 1552/2000
2024-11-11 20:49:35,724 Current Learning Rate: 0.0086448431
2024-11-11 20:49:35,724 Train Loss: 0.0005560, Val Loss: 0.0007352
2024-11-11 20:49:35,725 Epoch 1553/2000
2024-11-11 20:49:51,051 Current Learning Rate: 0.0086981555
2024-11-11 20:49:51,052 Train Loss: 0.0006073, Val Loss: 0.0007517
2024-11-11 20:49:51,052 Epoch 1554/2000
2024-11-11 20:50:07,115 Current Learning Rate: 0.0087505553
2024-11-11 20:50:07,115 Train Loss: 0.0006429, Val Loss: 0.0007569
2024-11-11 20:50:07,115 Epoch 1555/2000
2024-11-11 20:50:22,644 Current Learning Rate: 0.0088020298
2024-11-11 20:50:22,645 Train Loss: 0.0007582, Val Loss: 0.0007841
2024-11-11 20:50:22,645 Epoch 1556/2000
2024-11-11 20:50:38,129 Current Learning Rate: 0.0088525662
2024-11-11 20:50:38,130 Train Loss: 0.0006544, Val Loss: 0.0007788
2024-11-11 20:50:38,130 Epoch 1557/2000
2024-11-11 20:50:53,981 Current Learning Rate: 0.0089021520
2024-11-11 20:50:53,982 Train Loss: 0.0006456, Val Loss: 0.0007563
2024-11-11 20:50:53,982 Epoch 1558/2000
2024-11-11 20:51:10,423 Current Learning Rate: 0.0089507751
2024-11-11 20:51:10,423 Train Loss: 0.0006523, Val Loss: 0.0007980
2024-11-11 20:51:10,424 Epoch 1559/2000
2024-11-11 20:51:26,694 Current Learning Rate: 0.0089984233
2024-11-11 20:51:26,695 Train Loss: 0.0006832, Val Loss: 0.0007425
2024-11-11 20:51:26,695 Epoch 1560/2000
2024-11-11 20:51:42,747 Current Learning Rate: 0.0090450850
2024-11-11 20:51:42,748 Train Loss: 0.0005677, Val Loss: 0.0007043
2024-11-11 20:51:42,748 Epoch 1561/2000
2024-11-11 20:51:59,257 Current Learning Rate: 0.0090907486
2024-11-11 20:51:59,257 Train Loss: 0.0006249, Val Loss: 0.0007395
2024-11-11 20:51:59,258 Epoch 1562/2000
2024-11-11 20:52:15,419 Current Learning Rate: 0.0091354029
2024-11-11 20:52:15,420 Train Loss: 0.0005915, Val Loss: 0.0007268
2024-11-11 20:52:15,420 Epoch 1563/2000
2024-11-11 20:52:31,647 Current Learning Rate: 0.0091790368
2024-11-11 20:52:31,647 Train Loss: 0.0006009, Val Loss: 0.0007376
2024-11-11 20:52:31,648 Epoch 1564/2000
2024-11-11 20:52:47,545 Current Learning Rate: 0.0092216396
2024-11-11 20:52:47,545 Train Loss: 0.0006693, Val Loss: 0.0007563
2024-11-11 20:52:47,546 Epoch 1565/2000
2024-11-11 20:53:02,887 Current Learning Rate: 0.0092632008
2024-11-11 20:53:02,888 Train Loss: 0.0005953, Val Loss: 0.0007969
2024-11-11 20:53:02,889 Epoch 1566/2000
2024-11-11 20:53:18,419 Current Learning Rate: 0.0093037101
2024-11-11 20:53:18,419 Train Loss: 0.0006203, Val Loss: 0.0008209
2024-11-11 20:53:18,419 Epoch 1567/2000
2024-11-11 20:53:34,229 Current Learning Rate: 0.0093431576
2024-11-11 20:53:34,230 Train Loss: 0.0006143, Val Loss: 0.0007918
2024-11-11 20:53:34,230 Epoch 1568/2000
2024-11-11 20:53:50,097 Current Learning Rate: 0.0093815334
2024-11-11 20:53:50,098 Train Loss: 0.0006855, Val Loss: 0.0007661
2024-11-11 20:53:50,098 Epoch 1569/2000
2024-11-11 20:54:07,549 Current Learning Rate: 0.0094188282
2024-11-11 20:54:07,550 Train Loss: 0.0006081, Val Loss: 0.0006879
2024-11-11 20:54:07,550 Epoch 1570/2000
2024-11-11 20:54:23,029 Current Learning Rate: 0.0094550326
2024-11-11 20:54:23,030 Train Loss: 0.0005917, Val Loss: 0.0007156
2024-11-11 20:54:23,030 Epoch 1571/2000
2024-11-11 20:54:38,360 Current Learning Rate: 0.0094901379
2024-11-11 20:54:38,360 Train Loss: 0.0005541, Val Loss: 0.0006825
2024-11-11 20:54:38,360 Epoch 1572/2000
2024-11-11 20:54:53,711 Current Learning Rate: 0.0095241353
2024-11-11 20:54:53,711 Train Loss: 0.0005301, Val Loss: 0.0006911
2024-11-11 20:54:53,711 Epoch 1573/2000
2024-11-11 20:55:09,628 Current Learning Rate: 0.0095570164
2024-11-11 20:55:09,628 Train Loss: 0.0005563, Val Loss: 0.0006888
2024-11-11 20:55:09,628 Epoch 1574/2000
2024-11-11 20:55:24,931 Current Learning Rate: 0.0095887731
2024-11-11 20:55:24,931 Train Loss: 0.0006267, Val Loss: 0.0007232
2024-11-11 20:55:24,932 Epoch 1575/2000
2024-11-11 20:55:40,676 Current Learning Rate: 0.0096193977
2024-11-11 20:55:40,677 Train Loss: 0.0005328, Val Loss: 0.0006615
2024-11-11 20:55:40,677 Epoch 1576/2000
2024-11-11 20:55:56,121 Current Learning Rate: 0.0096488824
2024-11-11 20:55:56,121 Train Loss: 0.0005385, Val Loss: 0.0007085
2024-11-11 20:55:56,122 Epoch 1577/2000
2024-11-11 20:56:11,419 Current Learning Rate: 0.0096772202
2024-11-11 20:56:11,420 Train Loss: 0.0006376, Val Loss: 0.0007590
2024-11-11 20:56:11,420 Epoch 1578/2000
2024-11-11 20:56:26,934 Current Learning Rate: 0.0097044038
2024-11-11 20:56:26,934 Train Loss: 0.0006362, Val Loss: 0.0007474
2024-11-11 20:56:26,934 Epoch 1579/2000
2024-11-11 20:56:42,217 Current Learning Rate: 0.0097304268
2024-11-11 20:56:42,218 Train Loss: 0.0006159, Val Loss: 0.0007392
2024-11-11 20:56:42,218 Epoch 1580/2000
2024-11-11 20:56:57,672 Current Learning Rate: 0.0097552826
2024-11-11 20:56:57,672 Train Loss: 0.0006625, Val Loss: 0.0007923
2024-11-11 20:56:57,673 Epoch 1581/2000
2024-11-11 20:57:13,147 Current Learning Rate: 0.0097789651
2024-11-11 20:57:13,147 Train Loss: 0.0005825, Val Loss: 0.0007274
2024-11-11 20:57:13,147 Epoch 1582/2000
2024-11-11 20:57:28,576 Current Learning Rate: 0.0098014684
2024-11-11 20:57:28,577 Train Loss: 0.0010995, Val Loss: 0.0010468
2024-11-11 20:57:28,577 Epoch 1583/2000
2024-11-11 20:57:44,072 Current Learning Rate: 0.0098227871
2024-11-11 20:57:44,073 Train Loss: 0.0007570, Val Loss: 0.0007988
2024-11-11 20:57:44,073 Epoch 1584/2000
2024-11-11 20:57:59,278 Current Learning Rate: 0.0098429158
2024-11-11 20:57:59,279 Train Loss: 0.0006140, Val Loss: 0.0007082
2024-11-11 20:57:59,279 Epoch 1585/2000
2024-11-11 20:58:14,221 Current Learning Rate: 0.0098618496
2024-11-11 20:58:14,221 Train Loss: 0.0006296, Val Loss: 0.0007013
2024-11-11 20:58:14,221 Epoch 1586/2000
2024-11-11 20:58:29,757 Current Learning Rate: 0.0098795838
2024-11-11 20:58:29,758 Train Loss: 0.0006007, Val Loss: 0.0007067
2024-11-11 20:58:29,758 Epoch 1587/2000
2024-11-11 20:58:45,046 Current Learning Rate: 0.0098961141
2024-11-11 20:58:45,047 Train Loss: 0.0006515, Val Loss: 0.0007251
2024-11-11 20:58:45,047 Epoch 1588/2000
2024-11-11 20:59:00,279 Current Learning Rate: 0.0099114363
2024-11-11 20:59:00,280 Train Loss: 0.0006884, Val Loss: 0.0007127
2024-11-11 20:59:00,280 Epoch 1589/2000
2024-11-11 20:59:15,591 Current Learning Rate: 0.0099255466
2024-11-11 20:59:15,591 Train Loss: 0.0006105, Val Loss: 0.0006956
2024-11-11 20:59:15,591 Epoch 1590/2000
2024-11-11 20:59:31,057 Current Learning Rate: 0.0099384417
2024-11-11 20:59:31,057 Train Loss: 0.0005697, Val Loss: 0.0006622
2024-11-11 20:59:31,058 Epoch 1591/2000
2024-11-11 20:59:46,225 Current Learning Rate: 0.0099501183
2024-11-11 20:59:46,226 Train Loss: 0.0005302, Val Loss: 0.0006668
2024-11-11 20:59:46,226 Epoch 1592/2000
2024-11-11 21:00:01,386 Current Learning Rate: 0.0099605735
2024-11-11 21:00:01,386 Train Loss: 0.0006113, Val Loss: 0.0007438
2024-11-11 21:00:01,386 Epoch 1593/2000
2024-11-11 21:00:17,057 Current Learning Rate: 0.0099698048
2024-11-11 21:00:17,058 Train Loss: 0.0005776, Val Loss: 0.0006927
2024-11-11 21:00:17,059 Epoch 1594/2000
2024-11-11 21:00:32,223 Current Learning Rate: 0.0099778098
2024-11-11 21:00:32,223 Train Loss: 0.0005939, Val Loss: 0.0007548
2024-11-11 21:00:32,223 Epoch 1595/2000
2024-11-11 21:00:47,543 Current Learning Rate: 0.0099845867
2024-11-11 21:00:47,543 Train Loss: 0.0006328, Val Loss: 0.0007164
2024-11-11 21:00:47,544 Epoch 1596/2000
2024-11-11 21:01:03,076 Current Learning Rate: 0.0099901336
2024-11-11 21:01:03,077 Train Loss: 0.0006429, Val Loss: 0.0007135
2024-11-11 21:01:03,077 Epoch 1597/2000
2024-11-11 21:01:19,029 Current Learning Rate: 0.0099944494
2024-11-11 21:01:19,029 Train Loss: 0.0005700, Val Loss: 0.0007071
2024-11-11 21:01:19,030 Epoch 1598/2000
2024-11-11 21:01:34,774 Current Learning Rate: 0.0099975328
2024-11-11 21:01:34,775 Train Loss: 0.0005241, Val Loss: 0.0006831
2024-11-11 21:01:34,775 Epoch 1599/2000
2024-11-11 21:01:51,215 Current Learning Rate: 0.0099993832
2024-11-11 21:01:51,215 Train Loss: 0.0005710, Val Loss: 0.0006871
2024-11-11 21:01:51,216 Epoch 1600/2000
2024-11-11 21:02:07,583 Current Learning Rate: 0.0100000000
2024-11-11 21:02:07,583 Train Loss: 0.0006290, Val Loss: 0.0007006
2024-11-11 21:02:07,584 Epoch 1601/2000
2024-11-11 21:02:24,025 Current Learning Rate: 0.0099993832
2024-11-11 21:02:24,026 Train Loss: 0.0005653, Val Loss: 0.0006867
2024-11-11 21:02:24,026 Epoch 1602/2000
2024-11-11 21:02:39,275 Current Learning Rate: 0.0099975328
2024-11-11 21:02:39,276 Train Loss: 0.0006054, Val Loss: 0.0007082
2024-11-11 21:02:39,276 Epoch 1603/2000
2024-11-11 21:02:55,958 Current Learning Rate: 0.0099944494
2024-11-11 21:02:55,959 Train Loss: 0.0005532, Val Loss: 0.0007211
2024-11-11 21:02:55,959 Epoch 1604/2000
2024-11-11 21:03:11,381 Current Learning Rate: 0.0099901336
2024-11-11 21:03:11,382 Train Loss: 0.0006236, Val Loss: 0.0007576
2024-11-11 21:03:11,382 Epoch 1605/2000
2024-11-11 21:03:27,585 Current Learning Rate: 0.0099845867
2024-11-11 21:03:27,586 Train Loss: 0.0006244, Val Loss: 0.0007181
2024-11-11 21:03:27,587 Epoch 1606/2000
2024-11-11 21:03:42,922 Current Learning Rate: 0.0099778098
2024-11-11 21:03:42,922 Train Loss: 0.0005689, Val Loss: 0.0006912
2024-11-11 21:03:42,922 Epoch 1607/2000
2024-11-11 21:03:57,985 Current Learning Rate: 0.0099698048
2024-11-11 21:03:57,986 Train Loss: 0.0006124, Val Loss: 0.0007400
2024-11-11 21:03:57,986 Epoch 1608/2000
2024-11-11 21:04:13,229 Current Learning Rate: 0.0099605735
2024-11-11 21:04:13,230 Train Loss: 0.0006312, Val Loss: 0.0007457
2024-11-11 21:04:13,230 Epoch 1609/2000
2024-11-11 21:04:29,189 Current Learning Rate: 0.0099501183
2024-11-11 21:04:29,189 Train Loss: 0.0006641, Val Loss: 0.0007831
2024-11-11 21:04:29,190 Epoch 1610/2000
2024-11-11 21:04:45,613 Current Learning Rate: 0.0099384417
2024-11-11 21:04:45,615 Train Loss: 0.0006055, Val Loss: 0.0007235
2024-11-11 21:04:45,615 Epoch 1611/2000
2024-11-11 21:05:01,489 Current Learning Rate: 0.0099255466
2024-11-11 21:05:01,490 Train Loss: 0.0006120, Val Loss: 0.0006749
2024-11-11 21:05:01,490 Epoch 1612/2000
2024-11-11 21:05:17,316 Current Learning Rate: 0.0099114363
2024-11-11 21:05:17,317 Train Loss: 0.0006593, Val Loss: 0.0006665
2024-11-11 21:05:17,317 Epoch 1613/2000
2024-11-11 21:05:34,608 Current Learning Rate: 0.0098961141
2024-11-11 21:05:34,609 Train Loss: 0.0005506, Val Loss: 0.0006879
2024-11-11 21:05:34,609 Epoch 1614/2000
2024-11-11 21:05:50,551 Current Learning Rate: 0.0098795838
2024-11-11 21:05:50,552 Train Loss: 0.0005703, Val Loss: 0.0006667
2024-11-11 21:05:50,552 Epoch 1615/2000
2024-11-11 21:06:07,113 Current Learning Rate: 0.0098618496
2024-11-11 21:06:07,113 Train Loss: 0.0005136, Val Loss: 0.0006420
2024-11-11 21:06:07,113 Epoch 1616/2000
2024-11-11 21:06:23,347 Current Learning Rate: 0.0098429158
2024-11-11 21:06:23,347 Train Loss: 0.0005996, Val Loss: 0.0006467
2024-11-11 21:06:23,348 Epoch 1617/2000
2024-11-11 21:06:38,694 Current Learning Rate: 0.0098227871
2024-11-11 21:06:38,696 Train Loss: 0.0005619, Val Loss: 0.0006564
2024-11-11 21:06:38,696 Epoch 1618/2000
2024-11-11 21:06:54,865 Current Learning Rate: 0.0098014684
2024-11-11 21:06:54,866 Train Loss: 0.0005644, Val Loss: 0.0006659
2024-11-11 21:06:54,866 Epoch 1619/2000
2024-11-11 21:07:10,971 Current Learning Rate: 0.0097789651
2024-11-11 21:07:10,973 Train Loss: 0.0006016, Val Loss: 0.0006553
2024-11-11 21:07:10,973 Epoch 1620/2000
2024-11-11 21:07:26,910 Current Learning Rate: 0.0097552826
2024-11-11 21:07:26,910 Train Loss: 0.0004954, Val Loss: 0.0006867
2024-11-11 21:07:26,911 Epoch 1621/2000
2024-11-11 21:07:42,858 Current Learning Rate: 0.0097304268
2024-11-11 21:07:42,858 Train Loss: 0.0005692, Val Loss: 0.0007243
2024-11-11 21:07:42,859 Epoch 1622/2000
2024-11-11 21:07:58,421 Current Learning Rate: 0.0097044038
2024-11-11 21:07:58,421 Train Loss: 0.0006182, Val Loss: 0.0006944
2024-11-11 21:07:58,422 Epoch 1623/2000
2024-11-11 21:08:14,521 Current Learning Rate: 0.0096772202
2024-11-11 21:08:14,522 Train Loss: 0.0005968, Val Loss: 0.0007559
2024-11-11 21:08:14,522 Epoch 1624/2000
2024-11-11 21:08:30,611 Current Learning Rate: 0.0096488824
2024-11-11 21:08:30,613 Train Loss: 0.0005986, Val Loss: 0.0006993
2024-11-11 21:08:30,613 Epoch 1625/2000
2024-11-11 21:08:46,495 Current Learning Rate: 0.0096193977
2024-11-11 21:08:46,496 Train Loss: 0.0005167, Val Loss: 0.0006882
2024-11-11 21:08:46,496 Epoch 1626/2000
2024-11-11 21:09:01,803 Current Learning Rate: 0.0095887731
2024-11-11 21:09:01,803 Train Loss: 0.0005384, Val Loss: 0.0006748
2024-11-11 21:09:01,804 Epoch 1627/2000
2024-11-11 21:09:17,477 Current Learning Rate: 0.0095570164
2024-11-11 21:09:17,477 Train Loss: 0.0006096, Val Loss: 0.0006921
2024-11-11 21:09:17,478 Epoch 1628/2000
2024-11-11 21:09:33,161 Current Learning Rate: 0.0095241353
2024-11-11 21:09:33,161 Train Loss: 0.0006100, Val Loss: 0.0007079
2024-11-11 21:09:33,162 Epoch 1629/2000
2024-11-11 21:09:48,659 Current Learning Rate: 0.0094901379
2024-11-11 21:09:48,659 Train Loss: 0.0006276, Val Loss: 0.0006840
2024-11-11 21:09:48,660 Epoch 1630/2000
2024-11-11 21:10:04,413 Current Learning Rate: 0.0094550326
2024-11-11 21:10:04,413 Train Loss: 0.0006200, Val Loss: 0.0006968
2024-11-11 21:10:04,413 Epoch 1631/2000
2024-11-11 21:10:20,284 Current Learning Rate: 0.0094188282
2024-11-11 21:10:20,285 Train Loss: 0.0005760, Val Loss: 0.0007054
2024-11-11 21:10:20,285 Epoch 1632/2000
2024-11-11 21:10:35,692 Current Learning Rate: 0.0093815334
2024-11-11 21:10:35,693 Train Loss: 0.0005433, Val Loss: 0.0006638
2024-11-11 21:10:35,693 Epoch 1633/2000
2024-11-11 21:10:51,424 Current Learning Rate: 0.0093431576
2024-11-11 21:10:51,425 Train Loss: 0.0005208, Val Loss: 0.0007129
2024-11-11 21:10:51,425 Epoch 1634/2000
2024-11-11 21:11:06,879 Current Learning Rate: 0.0093037101
2024-11-11 21:11:06,879 Train Loss: 0.0005983, Val Loss: 0.0007182
2024-11-11 21:11:06,880 Epoch 1635/2000
2024-11-11 21:11:23,115 Current Learning Rate: 0.0092632008
2024-11-11 21:11:23,116 Train Loss: 0.0006129, Val Loss: 0.0006546
2024-11-11 21:11:23,116 Epoch 1636/2000
2024-11-11 21:11:38,929 Current Learning Rate: 0.0092216396
2024-11-11 21:11:38,929 Train Loss: 0.0006309, Val Loss: 0.0007002
2024-11-11 21:11:38,930 Epoch 1637/2000
2024-11-11 21:11:54,777 Current Learning Rate: 0.0091790368
2024-11-11 21:11:54,778 Train Loss: 0.0005877, Val Loss: 0.0007419
2024-11-11 21:11:54,778 Epoch 1638/2000
2024-11-11 21:12:11,117 Current Learning Rate: 0.0091354029
2024-11-11 21:12:11,118 Train Loss: 0.0006475, Val Loss: 0.0006683
2024-11-11 21:12:11,118 Epoch 1639/2000
2024-11-11 21:12:27,165 Current Learning Rate: 0.0090907486
2024-11-11 21:12:27,166 Train Loss: 0.0005489, Val Loss: 0.0006673
2024-11-11 21:12:27,166 Epoch 1640/2000
2024-11-11 21:12:43,178 Current Learning Rate: 0.0090450850
2024-11-11 21:12:43,179 Train Loss: 0.0007216, Val Loss: 0.0008442
2024-11-11 21:12:43,179 Epoch 1641/2000
2024-11-11 21:12:59,330 Current Learning Rate: 0.0089984233
2024-11-11 21:12:59,332 Train Loss: 0.0006222, Val Loss: 0.0006745
2024-11-11 21:12:59,332 Epoch 1642/2000
2024-11-11 21:13:15,731 Current Learning Rate: 0.0089507751
2024-11-11 21:13:15,731 Train Loss: 0.0005324, Val Loss: 0.0007027
2024-11-11 21:13:15,732 Epoch 1643/2000
2024-11-11 21:13:32,312 Current Learning Rate: 0.0089021520
2024-11-11 21:13:32,312 Train Loss: 0.0006039, Val Loss: 0.0007143
2024-11-11 21:13:32,313 Epoch 1644/2000
2024-11-11 21:13:49,583 Current Learning Rate: 0.0088525662
2024-11-11 21:13:49,584 Train Loss: 0.0005603, Val Loss: 0.0006621
2024-11-11 21:13:49,584 Epoch 1645/2000
2024-11-11 21:14:05,625 Current Learning Rate: 0.0088020298
2024-11-11 21:14:05,625 Train Loss: 0.0006123, Val Loss: 0.0006480
2024-11-11 21:14:05,625 Epoch 1646/2000
2024-11-11 21:14:21,815 Current Learning Rate: 0.0087505553
2024-11-11 21:14:21,816 Train Loss: 0.0005118, Val Loss: 0.0006217
2024-11-11 21:14:21,816 Epoch 1647/2000
2024-11-11 21:14:38,370 Current Learning Rate: 0.0086981555
2024-11-11 21:14:38,371 Train Loss: 0.0005529, Val Loss: 0.0006320
2024-11-11 21:14:38,372 Epoch 1648/2000
2024-11-11 21:14:54,991 Current Learning Rate: 0.0086448431
2024-11-11 21:14:54,992 Train Loss: 0.0005973, Val Loss: 0.0006550
2024-11-11 21:14:54,992 Epoch 1649/2000
2024-11-11 21:15:10,277 Current Learning Rate: 0.0085906315
2024-11-11 21:15:10,277 Train Loss: 0.0005381, Val Loss: 0.0006692
2024-11-11 21:15:10,277 Epoch 1650/2000
2024-11-11 21:15:26,085 Current Learning Rate: 0.0085355339
2024-11-11 21:15:26,086 Train Loss: 0.0005312, Val Loss: 0.0006560
2024-11-11 21:15:26,086 Epoch 1651/2000
2024-11-11 21:15:41,190 Current Learning Rate: 0.0084795640
2024-11-11 21:15:41,190 Train Loss: 0.0005639, Val Loss: 0.0006777
2024-11-11 21:15:41,190 Epoch 1652/2000
2024-11-11 21:15:56,598 Current Learning Rate: 0.0084227355
2024-11-11 21:15:56,599 Train Loss: 0.0005813, Val Loss: 0.0007803
2024-11-11 21:15:56,599 Epoch 1653/2000
2024-11-11 21:16:12,226 Current Learning Rate: 0.0083650626
2024-11-11 21:16:12,226 Train Loss: 0.0005547, Val Loss: 0.0006887
2024-11-11 21:16:12,226 Epoch 1654/2000
2024-11-11 21:16:27,413 Current Learning Rate: 0.0083065593
2024-11-11 21:16:27,414 Train Loss: 0.0005443, Val Loss: 0.0006525
2024-11-11 21:16:27,414 Epoch 1655/2000
2024-11-11 21:16:43,153 Current Learning Rate: 0.0082472402
2024-11-11 21:16:43,153 Train Loss: 0.0005685, Val Loss: 0.0006408
2024-11-11 21:16:43,153 Epoch 1656/2000
2024-11-11 21:16:58,433 Current Learning Rate: 0.0081871199
2024-11-11 21:16:58,434 Train Loss: 0.0005863, Val Loss: 0.0006451
2024-11-11 21:16:58,434 Epoch 1657/2000
2024-11-11 21:17:14,151 Current Learning Rate: 0.0081262133
2024-11-11 21:17:14,151 Train Loss: 0.0005508, Val Loss: 0.0006583
2024-11-11 21:17:14,151 Epoch 1658/2000
2024-11-11 21:17:29,690 Current Learning Rate: 0.0080645353
2024-11-11 21:17:29,690 Train Loss: 0.0004973, Val Loss: 0.0006194
2024-11-11 21:17:29,690 Epoch 1659/2000
2024-11-11 21:17:45,728 Current Learning Rate: 0.0080021011
2024-11-11 21:17:46,492 Train Loss: 0.0004798, Val Loss: 0.0005947
2024-11-11 21:17:46,493 Epoch 1660/2000
2024-11-11 21:18:01,198 Current Learning Rate: 0.0079389263
2024-11-11 21:18:02,052 Train Loss: 0.0005035, Val Loss: 0.0005858
2024-11-11 21:18:02,053 Epoch 1661/2000
2024-11-11 21:18:16,728 Current Learning Rate: 0.0078750263
2024-11-11 21:18:16,729 Train Loss: 0.0005292, Val Loss: 0.0005920
2024-11-11 21:18:16,729 Epoch 1662/2000
2024-11-11 21:18:32,072 Current Learning Rate: 0.0078104169
2024-11-11 21:18:32,073 Train Loss: 0.0004721, Val Loss: 0.0005989
2024-11-11 21:18:32,073 Epoch 1663/2000
2024-11-11 21:18:47,577 Current Learning Rate: 0.0077451141
2024-11-11 21:18:47,577 Train Loss: 0.0004615, Val Loss: 0.0006117
2024-11-11 21:18:47,577 Epoch 1664/2000
2024-11-11 21:19:03,624 Current Learning Rate: 0.0076791340
2024-11-11 21:19:03,624 Train Loss: 0.0004617, Val Loss: 0.0006160
2024-11-11 21:19:03,624 Epoch 1665/2000
2024-11-11 21:19:19,559 Current Learning Rate: 0.0076124928
2024-11-11 21:19:19,560 Train Loss: 0.0004836, Val Loss: 0.0006143
2024-11-11 21:19:19,560 Epoch 1666/2000
2024-11-11 21:19:35,949 Current Learning Rate: 0.0075452071
2024-11-11 21:19:35,950 Train Loss: 0.0004806, Val Loss: 0.0006009
2024-11-11 21:19:35,950 Epoch 1667/2000
2024-11-11 21:19:50,396 Current Learning Rate: 0.0074772933
2024-11-11 21:19:50,397 Train Loss: 0.0004721, Val Loss: 0.0006422
2024-11-11 21:19:50,397 Epoch 1668/2000
2024-11-11 21:20:05,722 Current Learning Rate: 0.0074087684
2024-11-11 21:20:05,723 Train Loss: 0.0005263, Val Loss: 0.0006798
2024-11-11 21:20:05,733 Epoch 1669/2000
2024-11-11 21:20:21,494 Current Learning Rate: 0.0073396491
2024-11-11 21:20:21,494 Train Loss: 0.0006208, Val Loss: 0.0006382
2024-11-11 21:20:21,495 Epoch 1670/2000
2024-11-11 21:20:37,203 Current Learning Rate: 0.0072699525
2024-11-11 21:20:37,204 Train Loss: 0.0005495, Val Loss: 0.0006000
2024-11-11 21:20:37,204 Epoch 1671/2000
2024-11-11 21:20:52,738 Current Learning Rate: 0.0071996958
2024-11-11 21:20:52,738 Train Loss: 0.0004826, Val Loss: 0.0006239
2024-11-11 21:20:52,738 Epoch 1672/2000
2024-11-11 21:21:08,673 Current Learning Rate: 0.0071288965
2024-11-11 21:21:08,674 Train Loss: 0.0005262, Val Loss: 0.0006366
2024-11-11 21:21:08,674 Epoch 1673/2000
2024-11-11 21:21:24,328 Current Learning Rate: 0.0070575718
2024-11-11 21:21:24,329 Train Loss: 0.0005894, Val Loss: 0.0005975
2024-11-11 21:21:24,329 Epoch 1674/2000
2024-11-11 21:21:40,397 Current Learning Rate: 0.0069857395
2024-11-11 21:21:40,399 Train Loss: 0.0004935, Val Loss: 0.0005900
2024-11-11 21:21:40,399 Epoch 1675/2000
2024-11-11 21:21:56,131 Current Learning Rate: 0.0069134172
2024-11-11 21:21:57,193 Train Loss: 0.0004529, Val Loss: 0.0005800
2024-11-11 21:21:57,194 Epoch 1676/2000
2024-11-11 21:22:12,837 Current Learning Rate: 0.0068406228
2024-11-11 21:22:13,807 Train Loss: 0.0005233, Val Loss: 0.0005721
2024-11-11 21:22:13,808 Epoch 1677/2000
2024-11-11 21:22:29,606 Current Learning Rate: 0.0067673742
2024-11-11 21:22:30,593 Train Loss: 0.0004106, Val Loss: 0.0005661
2024-11-11 21:22:30,594 Epoch 1678/2000
2024-11-11 21:22:46,452 Current Learning Rate: 0.0066936896
2024-11-11 21:22:46,452 Train Loss: 0.0004143, Val Loss: 0.0005719
2024-11-11 21:22:46,452 Epoch 1679/2000
2024-11-11 21:23:02,868 Current Learning Rate: 0.0066195871
2024-11-11 21:23:02,868 Train Loss: 0.0004331, Val Loss: 0.0005670
2024-11-11 21:23:02,869 Epoch 1680/2000
2024-11-11 21:23:19,416 Current Learning Rate: 0.0065450850
2024-11-11 21:23:19,416 Train Loss: 0.0004751, Val Loss: 0.0005682
2024-11-11 21:23:19,416 Epoch 1681/2000
2024-11-11 21:23:34,856 Current Learning Rate: 0.0064702016
2024-11-11 21:23:35,622 Train Loss: 0.0004536, Val Loss: 0.0005595
2024-11-11 21:23:35,622 Epoch 1682/2000
2024-11-11 21:23:51,120 Current Learning Rate: 0.0063949555
2024-11-11 21:23:51,121 Train Loss: 0.0004756, Val Loss: 0.0005645
2024-11-11 21:23:51,122 Epoch 1683/2000
2024-11-11 21:24:07,872 Current Learning Rate: 0.0063193652
2024-11-11 21:24:07,873 Train Loss: 0.0003916, Val Loss: 0.0005648
2024-11-11 21:24:07,873 Epoch 1684/2000
2024-11-11 21:24:23,939 Current Learning Rate: 0.0062434494
2024-11-11 21:24:23,940 Train Loss: 0.0004938, Val Loss: 0.0005678
2024-11-11 21:24:23,940 Epoch 1685/2000
2024-11-11 21:24:39,544 Current Learning Rate: 0.0061672268
2024-11-11 21:24:39,544 Train Loss: 0.0004542, Val Loss: 0.0005658
2024-11-11 21:24:39,544 Epoch 1686/2000
2024-11-11 21:24:55,368 Current Learning Rate: 0.0060907162
2024-11-11 21:24:55,368 Train Loss: 0.0004392, Val Loss: 0.0005781
2024-11-11 21:24:55,369 Epoch 1687/2000
2024-11-11 21:25:10,854 Current Learning Rate: 0.0060139365
2024-11-11 21:25:10,854 Train Loss: 0.0004555, Val Loss: 0.0005663
2024-11-11 21:25:10,855 Epoch 1688/2000
2024-11-11 21:25:27,116 Current Learning Rate: 0.0059369066
2024-11-11 21:25:27,943 Train Loss: 0.0004500, Val Loss: 0.0005566
2024-11-11 21:25:27,944 Epoch 1689/2000
2024-11-11 21:25:42,949 Current Learning Rate: 0.0058596455
2024-11-11 21:25:44,031 Train Loss: 0.0004700, Val Loss: 0.0005511
2024-11-11 21:25:44,032 Epoch 1690/2000
2024-11-11 21:26:00,235 Current Learning Rate: 0.0057821723
2024-11-11 21:26:00,235 Train Loss: 0.0004602, Val Loss: 0.0005725
2024-11-11 21:26:00,236 Epoch 1691/2000
2024-11-11 21:26:15,990 Current Learning Rate: 0.0057045062
2024-11-11 21:26:15,990 Train Loss: 0.0004776, Val Loss: 0.0005732
2024-11-11 21:26:15,990 Epoch 1692/2000
2024-11-11 21:26:31,978 Current Learning Rate: 0.0056266662
2024-11-11 21:26:32,948 Train Loss: 0.0004919, Val Loss: 0.0005486
2024-11-11 21:26:32,948 Epoch 1693/2000
2024-11-11 21:26:48,656 Current Learning Rate: 0.0055486716
2024-11-11 21:26:48,657 Train Loss: 0.0004299, Val Loss: 0.0005550
2024-11-11 21:26:48,658 Epoch 1694/2000
2024-11-11 21:27:04,898 Current Learning Rate: 0.0054705416
2024-11-11 21:27:04,899 Train Loss: 0.0004087, Val Loss: 0.0005548
2024-11-11 21:27:04,899 Epoch 1695/2000
2024-11-11 21:27:20,389 Current Learning Rate: 0.0053922955
2024-11-11 21:27:21,409 Train Loss: 0.0003815, Val Loss: 0.0005389
2024-11-11 21:27:21,409 Epoch 1696/2000
2024-11-11 21:27:37,184 Current Learning Rate: 0.0053139526
2024-11-11 21:27:37,185 Train Loss: 0.0003787, Val Loss: 0.0005402
2024-11-11 21:27:37,185 Epoch 1697/2000
2024-11-11 21:27:53,586 Current Learning Rate: 0.0052355323
2024-11-11 21:27:53,587 Train Loss: 0.0004286, Val Loss: 0.0005417
2024-11-11 21:27:53,587 Epoch 1698/2000
2024-11-11 21:28:09,170 Current Learning Rate: 0.0051570538
2024-11-11 21:28:09,172 Train Loss: 0.0004803, Val Loss: 0.0005458
2024-11-11 21:28:09,173 Epoch 1699/2000
2024-11-11 21:28:25,196 Current Learning Rate: 0.0050785366
2024-11-11 21:28:25,197 Train Loss: 0.0004299, Val Loss: 0.0005413
2024-11-11 21:28:25,197 Epoch 1700/2000
2024-11-11 21:28:41,941 Current Learning Rate: 0.0050000000
2024-11-11 21:28:42,658 Train Loss: 0.0004231, Val Loss: 0.0005336
2024-11-11 21:28:42,658 Epoch 1701/2000
2024-11-11 21:28:57,427 Current Learning Rate: 0.0049214634
2024-11-11 21:28:57,427 Train Loss: 0.0004457, Val Loss: 0.0005372
2024-11-11 21:28:57,427 Epoch 1702/2000
2024-11-11 21:29:13,586 Current Learning Rate: 0.0048429462
2024-11-11 21:29:13,587 Train Loss: 0.0004917, Val Loss: 0.0005428
2024-11-11 21:29:13,587 Epoch 1703/2000
2024-11-11 21:29:29,746 Current Learning Rate: 0.0047644677
2024-11-11 21:29:29,747 Train Loss: 0.0004636, Val Loss: 0.0005574
2024-11-11 21:29:29,747 Epoch 1704/2000
2024-11-11 21:29:45,894 Current Learning Rate: 0.0046860474
2024-11-11 21:29:45,895 Train Loss: 0.0004433, Val Loss: 0.0005445
2024-11-11 21:29:45,895 Epoch 1705/2000
2024-11-11 21:30:01,533 Current Learning Rate: 0.0046077045
2024-11-11 21:30:03,920 Train Loss: 0.0004468, Val Loss: 0.0005330
2024-11-11 21:30:03,920 Epoch 1706/2000
2024-11-11 21:30:18,254 Current Learning Rate: 0.0045294584
2024-11-11 21:30:18,999 Train Loss: 0.0004184, Val Loss: 0.0005270
2024-11-11 21:30:18,999 Epoch 1707/2000
2024-11-11 21:30:34,360 Current Learning Rate: 0.0044513284
2024-11-11 21:30:35,097 Train Loss: 0.0004106, Val Loss: 0.0005222
2024-11-11 21:30:35,098 Epoch 1708/2000
2024-11-11 21:30:49,745 Current Learning Rate: 0.0043733338
2024-11-11 21:30:50,488 Train Loss: 0.0004124, Val Loss: 0.0005208
2024-11-11 21:30:50,488 Epoch 1709/2000
2024-11-11 21:31:05,230 Current Learning Rate: 0.0042954938
2024-11-11 21:31:05,231 Train Loss: 0.0004559, Val Loss: 0.0005259
2024-11-11 21:31:05,232 Epoch 1710/2000
2024-11-11 21:31:20,579 Current Learning Rate: 0.0042178277
2024-11-11 21:31:20,579 Train Loss: 0.0004848, Val Loss: 0.0005336
2024-11-11 21:31:20,579 Epoch 1711/2000
2024-11-11 21:31:36,045 Current Learning Rate: 0.0041403545
2024-11-11 21:31:36,045 Train Loss: 0.0004692, Val Loss: 0.0005393
2024-11-11 21:31:36,045 Epoch 1712/2000
2024-11-11 21:31:51,229 Current Learning Rate: 0.0040630934
2024-11-11 21:31:51,229 Train Loss: 0.0003951, Val Loss: 0.0005302
2024-11-11 21:31:51,229 Epoch 1713/2000
2024-11-11 21:32:06,839 Current Learning Rate: 0.0039860635
2024-11-11 21:32:06,839 Train Loss: 0.0003930, Val Loss: 0.0005283
2024-11-11 21:32:06,839 Epoch 1714/2000
2024-11-11 21:32:22,754 Current Learning Rate: 0.0039092838
2024-11-11 21:32:22,754 Train Loss: 0.0003887, Val Loss: 0.0005323
2024-11-11 21:32:22,754 Epoch 1715/2000
2024-11-11 21:32:38,545 Current Learning Rate: 0.0038327732
2024-11-11 21:32:38,546 Train Loss: 0.0004875, Val Loss: 0.0005406
2024-11-11 21:32:38,546 Epoch 1716/2000
2024-11-11 21:32:53,734 Current Learning Rate: 0.0037565506
2024-11-11 21:32:53,735 Train Loss: 0.0004535, Val Loss: 0.0005704
2024-11-11 21:32:53,735 Epoch 1717/2000
2024-11-11 21:33:09,333 Current Learning Rate: 0.0036806348
2024-11-11 21:33:09,334 Train Loss: 0.0004627, Val Loss: 0.0005671
2024-11-11 21:33:09,334 Epoch 1718/2000
2024-11-11 21:33:25,199 Current Learning Rate: 0.0036050445
2024-11-11 21:33:25,200 Train Loss: 0.0004753, Val Loss: 0.0005282
2024-11-11 21:33:25,200 Epoch 1719/2000
2024-11-11 21:33:40,572 Current Learning Rate: 0.0035297984
2024-11-11 21:33:41,378 Train Loss: 0.0003659, Val Loss: 0.0005147
2024-11-11 21:33:41,378 Epoch 1720/2000
2024-11-11 21:33:55,968 Current Learning Rate: 0.0034549150
2024-11-11 21:33:55,968 Train Loss: 0.0003476, Val Loss: 0.0005149
2024-11-11 21:33:55,969 Epoch 1721/2000
2024-11-11 21:34:11,759 Current Learning Rate: 0.0033804129
2024-11-11 21:34:12,517 Train Loss: 0.0003638, Val Loss: 0.0005122
2024-11-11 21:34:12,517 Epoch 1722/2000
2024-11-11 21:34:27,394 Current Learning Rate: 0.0033063104
2024-11-11 21:34:28,209 Train Loss: 0.0003518, Val Loss: 0.0005106
2024-11-11 21:34:28,209 Epoch 1723/2000
2024-11-11 21:34:42,940 Current Learning Rate: 0.0032326258
2024-11-11 21:34:43,808 Train Loss: 0.0003611, Val Loss: 0.0005086
2024-11-11 21:34:43,808 Epoch 1724/2000
2024-11-11 21:34:58,215 Current Learning Rate: 0.0031593772
2024-11-11 21:34:58,971 Train Loss: 0.0004347, Val Loss: 0.0005074
2024-11-11 21:34:58,972 Epoch 1725/2000
2024-11-11 21:35:13,471 Current Learning Rate: 0.0030865828
2024-11-11 21:35:14,299 Train Loss: 0.0003929, Val Loss: 0.0005071
2024-11-11 21:35:14,300 Epoch 1726/2000
2024-11-11 21:35:29,438 Current Learning Rate: 0.0030142605
2024-11-11 21:35:29,439 Train Loss: 0.0004435, Val Loss: 0.0005078
2024-11-11 21:35:29,440 Epoch 1727/2000
2024-11-11 21:35:45,711 Current Learning Rate: 0.0029424282
2024-11-11 21:35:45,712 Train Loss: 0.0004152, Val Loss: 0.0005074
2024-11-11 21:35:45,712 Epoch 1728/2000
2024-11-11 21:36:01,409 Current Learning Rate: 0.0028711035
2024-11-11 21:36:01,410 Train Loss: 0.0004438, Val Loss: 0.0005084
2024-11-11 21:36:01,410 Epoch 1729/2000
2024-11-11 21:36:16,408 Current Learning Rate: 0.0028003042
2024-11-11 21:36:16,409 Train Loss: 0.0004133, Val Loss: 0.0005072
2024-11-11 21:36:16,409 Epoch 1730/2000
2024-11-11 21:36:32,347 Current Learning Rate: 0.0027300475
2024-11-11 21:36:33,145 Train Loss: 0.0004135, Val Loss: 0.0005039
2024-11-11 21:36:33,145 Epoch 1731/2000
2024-11-11 21:36:48,090 Current Learning Rate: 0.0026603509
2024-11-11 21:36:48,771 Train Loss: 0.0003828, Val Loss: 0.0005030
2024-11-11 21:36:48,771 Epoch 1732/2000
2024-11-11 21:37:04,353 Current Learning Rate: 0.0025912316
2024-11-11 21:37:04,354 Train Loss: 0.0004278, Val Loss: 0.0005036
2024-11-11 21:37:04,354 Epoch 1733/2000
2024-11-11 21:37:20,914 Current Learning Rate: 0.0025227067
2024-11-11 21:37:20,915 Train Loss: 0.0003527, Val Loss: 0.0005042
2024-11-11 21:37:20,916 Epoch 1734/2000
2024-11-11 21:37:37,279 Current Learning Rate: 0.0024547929
2024-11-11 21:37:37,280 Train Loss: 0.0004140, Val Loss: 0.0005063
2024-11-11 21:37:37,280 Epoch 1735/2000
2024-11-11 21:37:53,847 Current Learning Rate: 0.0023875072
2024-11-11 21:37:54,817 Train Loss: 0.0004297, Val Loss: 0.0005015
2024-11-11 21:37:54,818 Epoch 1736/2000
2024-11-11 21:38:11,178 Current Learning Rate: 0.0023208660
2024-11-11 21:38:11,891 Train Loss: 0.0004593, Val Loss: 0.0004956
2024-11-11 21:38:11,891 Epoch 1737/2000
2024-11-11 21:38:26,214 Current Learning Rate: 0.0022548859
2024-11-11 21:38:27,181 Train Loss: 0.0004213, Val Loss: 0.0004920
2024-11-11 21:38:27,182 Epoch 1738/2000
2024-11-11 21:38:43,553 Current Learning Rate: 0.0021895831
2024-11-11 21:38:43,554 Train Loss: 0.0003847, Val Loss: 0.0004928
2024-11-11 21:38:43,554 Epoch 1739/2000
2024-11-11 21:38:59,299 Current Learning Rate: 0.0021249737
2024-11-11 21:39:00,058 Train Loss: 0.0003771, Val Loss: 0.0004876
2024-11-11 21:39:00,058 Epoch 1740/2000
2024-11-11 21:39:14,941 Current Learning Rate: 0.0020610737
2024-11-11 21:39:15,759 Train Loss: 0.0003475, Val Loss: 0.0004875
2024-11-11 21:39:15,759 Epoch 1741/2000
2024-11-11 21:39:30,817 Current Learning Rate: 0.0019978989
2024-11-11 21:39:31,683 Train Loss: 0.0003471, Val Loss: 0.0004858
2024-11-11 21:39:31,684 Epoch 1742/2000
2024-11-11 21:39:47,134 Current Learning Rate: 0.0019354647
2024-11-11 21:39:48,120 Train Loss: 0.0003708, Val Loss: 0.0004855
2024-11-11 21:39:48,121 Epoch 1743/2000
2024-11-11 21:40:04,333 Current Learning Rate: 0.0018737867
2024-11-11 21:40:04,334 Train Loss: 0.0003876, Val Loss: 0.0004855
2024-11-11 21:40:04,335 Epoch 1744/2000
2024-11-11 21:40:19,883 Current Learning Rate: 0.0018128801
2024-11-11 21:40:20,653 Train Loss: 0.0003427, Val Loss: 0.0004844
2024-11-11 21:40:20,654 Epoch 1745/2000
2024-11-11 21:40:35,474 Current Learning Rate: 0.0017527598
2024-11-11 21:40:36,222 Train Loss: 0.0004239, Val Loss: 0.0004835
2024-11-11 21:40:36,222 Epoch 1746/2000
2024-11-11 21:40:51,038 Current Learning Rate: 0.0016934407
2024-11-11 21:40:51,771 Train Loss: 0.0003626, Val Loss: 0.0004820
2024-11-11 21:40:51,772 Epoch 1747/2000
2024-11-11 21:41:06,709 Current Learning Rate: 0.0016349374
2024-11-11 21:41:07,441 Train Loss: 0.0003723, Val Loss: 0.0004810
2024-11-11 21:41:07,441 Epoch 1748/2000
2024-11-11 21:41:22,329 Current Learning Rate: 0.0015772645
2024-11-11 21:41:23,205 Train Loss: 0.0003645, Val Loss: 0.0004809
2024-11-11 21:41:23,206 Epoch 1749/2000
2024-11-11 21:41:38,018 Current Learning Rate: 0.0015204360
2024-11-11 21:41:38,019 Train Loss: 0.0003917, Val Loss: 0.0004810
2024-11-11 21:41:38,019 Epoch 1750/2000
2024-11-11 21:41:53,584 Current Learning Rate: 0.0014644661
2024-11-11 21:41:53,584 Train Loss: 0.0003951, Val Loss: 0.0004822
2024-11-11 21:41:53,585 Epoch 1751/2000
2024-11-11 21:42:09,970 Current Learning Rate: 0.0014093685
2024-11-11 21:42:11,020 Train Loss: 0.0003900, Val Loss: 0.0004800
2024-11-11 21:42:11,020 Epoch 1752/2000
2024-11-11 21:42:26,546 Current Learning Rate: 0.0013551569
2024-11-11 21:42:27,473 Train Loss: 0.0003723, Val Loss: 0.0004783
2024-11-11 21:42:27,474 Epoch 1753/2000
2024-11-11 21:42:43,426 Current Learning Rate: 0.0013018445
2024-11-11 21:42:44,194 Train Loss: 0.0003705, Val Loss: 0.0004778
2024-11-11 21:42:44,194 Epoch 1754/2000
2024-11-11 21:42:58,864 Current Learning Rate: 0.0012494447
2024-11-11 21:42:59,617 Train Loss: 0.0004015, Val Loss: 0.0004778
2024-11-11 21:42:59,617 Epoch 1755/2000
2024-11-11 21:43:15,679 Current Learning Rate: 0.0011979702
2024-11-11 21:43:16,399 Train Loss: 0.0003422, Val Loss: 0.0004773
2024-11-11 21:43:16,399 Epoch 1756/2000
2024-11-11 21:43:31,117 Current Learning Rate: 0.0011474338
2024-11-11 21:43:31,118 Train Loss: 0.0003694, Val Loss: 0.0004775
2024-11-11 21:43:31,119 Epoch 1757/2000
2024-11-11 21:43:46,465 Current Learning Rate: 0.0010978480
2024-11-11 21:43:47,262 Train Loss: 0.0003533, Val Loss: 0.0004769
2024-11-11 21:43:47,262 Epoch 1758/2000
2024-11-11 21:44:01,659 Current Learning Rate: 0.0010492249
2024-11-11 21:44:03,839 Train Loss: 0.0003803, Val Loss: 0.0004760
2024-11-11 21:44:03,839 Epoch 1759/2000
2024-11-11 21:44:18,102 Current Learning Rate: 0.0010015767
2024-11-11 21:44:18,876 Train Loss: 0.0003702, Val Loss: 0.0004746
2024-11-11 21:44:18,876 Epoch 1760/2000
2024-11-11 21:44:33,367 Current Learning Rate: 0.0009549150
2024-11-11 21:44:34,092 Train Loss: 0.0003760, Val Loss: 0.0004743
2024-11-11 21:44:34,092 Epoch 1761/2000
2024-11-11 21:44:48,542 Current Learning Rate: 0.0009092514
2024-11-11 21:44:48,543 Train Loss: 0.0005291, Val Loss: 0.0004754
2024-11-11 21:44:48,543 Epoch 1762/2000
2024-11-11 21:45:04,214 Current Learning Rate: 0.0008645971
2024-11-11 21:45:05,031 Train Loss: 0.0003119, Val Loss: 0.0004739
2024-11-11 21:45:05,032 Epoch 1763/2000
2024-11-11 21:45:20,482 Current Learning Rate: 0.0008209632
2024-11-11 21:45:21,257 Train Loss: 0.0003379, Val Loss: 0.0004735
2024-11-11 21:45:21,258 Epoch 1764/2000
2024-11-11 21:45:36,295 Current Learning Rate: 0.0007783604
2024-11-11 21:45:37,410 Train Loss: 0.0003394, Val Loss: 0.0004732
2024-11-11 21:45:37,410 Epoch 1765/2000
2024-11-11 21:45:53,458 Current Learning Rate: 0.0007367992
2024-11-11 21:45:54,230 Train Loss: 0.0003866, Val Loss: 0.0004730
2024-11-11 21:45:54,230 Epoch 1766/2000
2024-11-11 21:46:08,733 Current Learning Rate: 0.0006962899
2024-11-11 21:46:09,634 Train Loss: 0.0003961, Val Loss: 0.0004728
2024-11-11 21:46:09,635 Epoch 1767/2000
2024-11-11 21:46:25,404 Current Learning Rate: 0.0006568424
2024-11-11 21:46:26,165 Train Loss: 0.0003480, Val Loss: 0.0004724
2024-11-11 21:46:26,165 Epoch 1768/2000
2024-11-11 21:46:41,317 Current Learning Rate: 0.0006184666
2024-11-11 21:46:42,122 Train Loss: 0.0003090, Val Loss: 0.0004720
2024-11-11 21:46:42,122 Epoch 1769/2000
2024-11-11 21:46:57,320 Current Learning Rate: 0.0005811718
2024-11-11 21:46:57,321 Train Loss: 0.0004402, Val Loss: 0.0004724
2024-11-11 21:46:57,321 Epoch 1770/2000
2024-11-11 21:47:12,797 Current Learning Rate: 0.0005449674
2024-11-11 21:47:12,797 Train Loss: 0.0004016, Val Loss: 0.0004722
2024-11-11 21:47:12,798 Epoch 1771/2000
2024-11-11 21:47:29,463 Current Learning Rate: 0.0005098621
2024-11-11 21:47:30,420 Train Loss: 0.0003784, Val Loss: 0.0004716
2024-11-11 21:47:30,420 Epoch 1772/2000
2024-11-11 21:47:37,319 Added key: store_based_barrier_key:1 to store for rank: 0
2024-11-11 21:47:45,803 Current Learning Rate: 0.0004758647
2024-11-11 21:47:46,777 Train Loss: 0.0003402, Val Loss: 0.0004713
2024-11-11 21:47:46,782 Epoch 1773/2000
2024-11-11 21:47:59,503 Loading best model from checkpoint.
2024-11-11 21:48:01,961 Current Learning Rate: 0.0004429836
2024-11-11 21:48:04,354 Train Loss: 0.0003396, Val Loss: 0.0004709
2024-11-11 21:48:04,357 Epoch 1774/2000
2024-11-11 21:48:17,190 Testing completed and best model saved.
2024-11-11 21:48:19,262 Current Learning Rate: 0.0004112269
2024-11-11 21:48:20,444 Train Loss: 0.0003725, Val Loss: 0.0004708
2024-11-11 21:48:20,445 Epoch 1775/2000
2024-11-11 21:48:35,956 Current Learning Rate: 0.0003806023
2024-11-11 21:48:37,258 Train Loss: 0.0003322, Val Loss: 0.0004707
2024-11-11 21:48:37,259 Epoch 1776/2000
2024-11-11 21:48:53,900 Current Learning Rate: 0.0003511176
2024-11-11 21:48:54,819 Train Loss: 0.0003374, Val Loss: 0.0004704
2024-11-11 21:48:54,819 Epoch 1777/2000
2024-11-11 21:49:09,992 Current Learning Rate: 0.0003227798
2024-11-11 21:49:10,980 Train Loss: 0.0003443, Val Loss: 0.0004703
2024-11-11 21:49:10,980 Epoch 1778/2000
2024-11-11 21:49:26,425 Current Learning Rate: 0.0002955962
2024-11-11 21:49:27,579 Train Loss: 0.0003375, Val Loss: 0.0004701
2024-11-11 21:49:27,580 Epoch 1779/2000
2024-11-11 21:49:44,290 Current Learning Rate: 0.0002695732
2024-11-11 21:49:45,502 Train Loss: 0.0003066, Val Loss: 0.0004700
2024-11-11 21:49:45,502 Epoch 1780/2000
2024-11-11 21:50:01,038 Current Learning Rate: 0.0002447174
2024-11-11 21:50:03,590 Train Loss: 0.0003402, Val Loss: 0.0004699
2024-11-11 21:50:03,590 Epoch 1781/2000
2024-11-11 21:50:18,791 Current Learning Rate: 0.0002210349
2024-11-11 21:50:18,792 Train Loss: 0.0004048, Val Loss: 0.0004700
2024-11-11 21:50:18,792 Epoch 1782/2000
2024-11-11 21:50:34,836 Current Learning Rate: 0.0001985316
2024-11-11 21:50:35,663 Train Loss: 0.0004596, Val Loss: 0.0004697
2024-11-11 21:50:35,663 Epoch 1783/2000
2024-11-11 21:50:50,740 Current Learning Rate: 0.0001772129
2024-11-11 21:50:51,674 Train Loss: 0.0003570, Val Loss: 0.0004697
2024-11-11 21:50:51,675 Epoch 1784/2000
2024-11-11 21:51:07,094 Current Learning Rate: 0.0001570842
2024-11-11 21:51:08,197 Train Loss: 0.0003369, Val Loss: 0.0004694
2024-11-11 21:51:08,198 Epoch 1785/2000
2024-11-11 21:51:24,013 Current Learning Rate: 0.0001381504
2024-11-11 21:51:25,004 Train Loss: 0.0003301, Val Loss: 0.0004694
2024-11-11 21:51:25,004 Epoch 1786/2000
2024-11-11 21:51:41,129 Current Learning Rate: 0.0001204162
2024-11-11 21:51:42,124 Train Loss: 0.0003317, Val Loss: 0.0004694
2024-11-11 21:51:42,125 Epoch 1787/2000
2024-11-11 21:51:58,458 Current Learning Rate: 0.0001038859
2024-11-11 21:51:59,342 Train Loss: 0.0003683, Val Loss: 0.0004693
2024-11-11 21:51:59,343 Epoch 1788/2000
2024-11-11 21:52:14,208 Current Learning Rate: 0.0000885637
2024-11-11 21:52:14,209 Train Loss: 0.0003520, Val Loss: 0.0004693
2024-11-11 21:52:14,209 Epoch 1789/2000
2024-11-11 21:52:30,902 Current Learning Rate: 0.0000744534
2024-11-11 21:52:31,986 Train Loss: 0.0003830, Val Loss: 0.0004693
2024-11-11 21:52:31,986 Epoch 1790/2000
2024-11-11 21:52:47,176 Current Learning Rate: 0.0000615583
2024-11-11 21:52:48,119 Train Loss: 0.0003614, Val Loss: 0.0004692
2024-11-11 21:52:48,120 Epoch 1791/2000
2024-11-11 21:53:03,705 Current Learning Rate: 0.0000498817
2024-11-11 21:53:03,705 Train Loss: 0.0003523, Val Loss: 0.0004692
2024-11-11 21:53:03,706 Epoch 1792/2000
2024-11-11 21:53:20,146 Current Learning Rate: 0.0000394265
2024-11-11 21:53:21,130 Train Loss: 0.0003641, Val Loss: 0.0004692
2024-11-11 21:53:21,131 Epoch 1793/2000
2024-11-11 21:53:36,524 Current Learning Rate: 0.0000301952
2024-11-11 21:53:36,525 Train Loss: 0.0003091, Val Loss: 0.0004692
2024-11-11 21:53:36,525 Epoch 1794/2000
2024-11-11 21:53:52,783 Current Learning Rate: 0.0000221902
2024-11-11 21:53:52,784 Train Loss: 0.0003335, Val Loss: 0.0004692
2024-11-11 21:53:52,784 Epoch 1795/2000
2024-11-11 21:54:09,994 Current Learning Rate: 0.0000154133
2024-11-11 21:54:09,995 Train Loss: 0.0003923, Val Loss: 0.0004692
2024-11-11 21:54:09,996 Epoch 1796/2000
2024-11-11 21:54:26,341 Current Learning Rate: 0.0000098664
2024-11-11 21:54:27,269 Train Loss: 0.0003742, Val Loss: 0.0004692
2024-11-11 21:54:27,269 Epoch 1797/2000
2024-11-11 21:54:42,567 Current Learning Rate: 0.0000055506
2024-11-11 21:54:42,567 Train Loss: 0.0003479, Val Loss: 0.0004692
2024-11-11 21:54:42,568 Epoch 1798/2000
2024-11-11 21:54:58,721 Current Learning Rate: 0.0000024672
2024-11-11 21:54:59,630 Train Loss: 0.0003606, Val Loss: 0.0004691
2024-11-11 21:54:59,631 Epoch 1799/2000
2024-11-11 21:55:15,011 Current Learning Rate: 0.0000006168
2024-11-11 21:55:15,012 Train Loss: 0.0004004, Val Loss: 0.0004692
2024-11-11 21:55:15,013 Epoch 1800/2000
2024-11-11 21:55:31,238 Current Learning Rate: 0.0000000000
2024-11-11 21:55:31,239 Train Loss: 0.0003904, Val Loss: 0.0004692
2024-11-11 21:55:31,239 Epoch 1801/2000
2024-11-11 21:55:47,692 Current Learning Rate: 0.0000006168
2024-11-11 21:55:47,693 Train Loss: 0.0003716, Val Loss: 0.0004691
2024-11-11 21:55:47,693 Epoch 1802/2000
2024-11-11 21:56:03,571 Current Learning Rate: 0.0000024672
2024-11-11 21:56:03,572 Train Loss: 0.0004168, Val Loss: 0.0004692
2024-11-11 21:56:03,572 Epoch 1803/2000
2024-11-11 21:56:19,536 Current Learning Rate: 0.0000055506
2024-11-11 21:56:19,536 Train Loss: 0.0003723, Val Loss: 0.0004692
2024-11-11 21:56:19,537 Epoch 1804/2000
2024-11-11 21:56:36,065 Current Learning Rate: 0.0000098664
2024-11-11 21:56:36,066 Train Loss: 0.0003215, Val Loss: 0.0004691
2024-11-11 21:56:36,066 Epoch 1805/2000
2024-11-11 21:56:52,175 Current Learning Rate: 0.0000154133
2024-11-11 21:56:53,096 Train Loss: 0.0003321, Val Loss: 0.0004691
2024-11-11 21:56:53,097 Epoch 1806/2000
2024-11-11 21:57:08,682 Current Learning Rate: 0.0000221902
2024-11-11 21:57:09,561 Train Loss: 0.0003177, Val Loss: 0.0004691
2024-11-11 21:57:09,561 Epoch 1807/2000
2024-11-11 21:57:24,711 Current Learning Rate: 0.0000301952
2024-11-11 21:57:24,712 Train Loss: 0.0003665, Val Loss: 0.0004692
2024-11-11 21:57:24,712 Epoch 1808/2000
2024-11-11 21:57:41,111 Current Learning Rate: 0.0000394265
2024-11-11 21:57:41,111 Train Loss: 0.0003340, Val Loss: 0.0004692
2024-11-11 21:57:41,112 Epoch 1809/2000
2024-11-11 21:57:58,362 Current Learning Rate: 0.0000498817
2024-11-11 21:57:58,363 Train Loss: 0.0003851, Val Loss: 0.0004692
2024-11-11 21:57:58,363 Epoch 1810/2000
2024-11-11 21:58:13,926 Current Learning Rate: 0.0000615583
2024-11-11 21:58:14,822 Train Loss: 0.0003246, Val Loss: 0.0004691
2024-11-11 21:58:14,822 Epoch 1811/2000
2024-11-11 21:58:29,888 Current Learning Rate: 0.0000744534
2024-11-11 21:58:30,771 Train Loss: 0.0003821, Val Loss: 0.0004691
2024-11-11 21:58:30,771 Epoch 1812/2000
2024-11-11 21:58:46,192 Current Learning Rate: 0.0000885637
2024-11-11 21:58:46,193 Train Loss: 0.0004197, Val Loss: 0.0004692
2024-11-11 21:58:46,194 Epoch 1813/2000
2024-11-11 21:59:02,290 Current Learning Rate: 0.0001038859
2024-11-11 21:59:02,291 Train Loss: 0.0003423, Val Loss: 0.0004692
2024-11-11 21:59:02,291 Epoch 1814/2000
2024-11-11 21:59:19,378 Current Learning Rate: 0.0001204162
2024-11-11 21:59:19,379 Train Loss: 0.0003315, Val Loss: 0.0004692
2024-11-11 21:59:19,379 Epoch 1815/2000
2024-11-11 21:59:35,674 Current Learning Rate: 0.0001381504
2024-11-11 21:59:35,675 Train Loss: 0.0003277, Val Loss: 0.0004692
2024-11-11 21:59:35,675 Epoch 1816/2000
2024-11-11 21:59:51,596 Current Learning Rate: 0.0001570842
2024-11-11 21:59:51,597 Train Loss: 0.0004323, Val Loss: 0.0004693
2024-11-11 21:59:51,597 Epoch 1817/2000
2024-11-11 22:00:07,077 Current Learning Rate: 0.0001772129
2024-11-11 22:00:07,078 Train Loss: 0.0003093, Val Loss: 0.0004693
2024-11-11 22:00:07,078 Epoch 1818/2000
2024-11-11 22:00:23,129 Current Learning Rate: 0.0001985316
2024-11-11 22:00:23,129 Train Loss: 0.0003487, Val Loss: 0.0004693
2024-11-11 22:00:23,130 Epoch 1819/2000
2024-11-11 22:00:39,661 Current Learning Rate: 0.0002210349
2024-11-11 22:00:39,662 Train Loss: 0.0003095, Val Loss: 0.0004694
2024-11-11 22:00:39,663 Epoch 1820/2000
2024-11-11 22:00:55,428 Current Learning Rate: 0.0002447174
2024-11-11 22:00:55,428 Train Loss: 0.0003606, Val Loss: 0.0004694
2024-11-11 22:00:55,428 Epoch 1821/2000
2024-11-11 22:01:12,829 Current Learning Rate: 0.0002695732
2024-11-11 22:01:12,830 Train Loss: 0.0003425, Val Loss: 0.0004694
2024-11-11 22:01:12,830 Epoch 1822/2000
2024-11-11 22:01:28,743 Current Learning Rate: 0.0002955962
2024-11-11 22:01:28,744 Train Loss: 0.0003432, Val Loss: 0.0004694
2024-11-11 22:01:28,744 Epoch 1823/2000
2024-11-11 22:01:45,759 Current Learning Rate: 0.0003227798
2024-11-11 22:01:45,760 Train Loss: 0.0003854, Val Loss: 0.0004697
2024-11-11 22:01:45,760 Epoch 1824/2000
2024-11-11 22:02:01,192 Current Learning Rate: 0.0003511176
2024-11-11 22:02:01,194 Train Loss: 0.0003785, Val Loss: 0.0004697
2024-11-11 22:02:01,195 Epoch 1825/2000
2024-11-11 22:02:16,697 Current Learning Rate: 0.0003806023
2024-11-11 22:02:16,698 Train Loss: 0.0003138, Val Loss: 0.0004695
2024-11-11 22:02:16,698 Epoch 1826/2000
2024-11-11 22:02:33,139 Current Learning Rate: 0.0004112269
2024-11-11 22:02:33,140 Train Loss: 0.0003309, Val Loss: 0.0004696
2024-11-11 22:02:33,140 Epoch 1827/2000
2024-11-11 22:02:49,142 Current Learning Rate: 0.0004429836
2024-11-11 22:02:49,142 Train Loss: 0.0003257, Val Loss: 0.0004697
2024-11-11 22:02:49,142 Epoch 1828/2000
2024-11-11 22:03:05,205 Current Learning Rate: 0.0004758647
2024-11-11 22:03:05,205 Train Loss: 0.0003437, Val Loss: 0.0004696
2024-11-11 22:03:05,205 Epoch 1829/2000
2024-11-11 22:03:21,199 Current Learning Rate: 0.0005098621
2024-11-11 22:03:21,199 Train Loss: 0.0003767, Val Loss: 0.0004696
2024-11-11 22:03:21,200 Epoch 1830/2000
2024-11-11 22:03:37,582 Current Learning Rate: 0.0005449674
2024-11-11 22:03:37,583 Train Loss: 0.0003757, Val Loss: 0.0004699
2024-11-11 22:03:37,584 Epoch 1831/2000
2024-11-11 22:03:53,689 Current Learning Rate: 0.0005811718
2024-11-11 22:03:53,689 Train Loss: 0.0003557, Val Loss: 0.0004697
2024-11-11 22:03:53,689 Epoch 1832/2000
2024-11-11 22:04:10,567 Current Learning Rate: 0.0006184666
2024-11-11 22:04:10,569 Train Loss: 0.0003385, Val Loss: 0.0004698
2024-11-11 22:04:10,572 Epoch 1833/2000
2024-11-11 22:04:26,485 Current Learning Rate: 0.0006568424
2024-11-11 22:04:26,485 Train Loss: 0.0003319, Val Loss: 0.0004700
2024-11-11 22:04:26,486 Epoch 1834/2000
2024-11-11 22:04:42,683 Current Learning Rate: 0.0006962899
2024-11-11 22:04:42,683 Train Loss: 0.0003140, Val Loss: 0.0004696
2024-11-11 22:04:42,683 Epoch 1835/2000
2024-11-11 22:04:43,134 Added key: store_based_barrier_key:1 to store for rank: 0
2024-11-11 22:04:59,269 Current Learning Rate: 0.0007367992
2024-11-11 22:04:59,271 Train Loss: 0.0003429, Val Loss: 0.0004696
2024-11-11 22:04:59,272 Epoch 1836/2000
2024-11-11 22:05:17,200 Testing completed and best model saved.
-11-11 22:05:16,182 Train Loss: 0.0004232, Val Loss: 0.0004709
2024-11-11 22:05:16,183 Epoch 1837/2000
2024-11-11 22:05:32,656 Current Learning Rate: 0.0008209632
2024-11-11 22:05:32,656 Train Loss: 0.0003067, Val Loss: 0.0004707
2024-11-11 22:05:32,657 Epoch 1838/2000
2024-11-11 22:05:48,734 Current Learning Rate: 0.0008645971
2024-11-11 22:05:48,734 Train Loss: 0.0003915, Val Loss: 0.0004725
2024-11-11 22:05:48,734 Epoch 1839/2000
2024-11-11 22:06:04,855 Current Learning Rate: 0.0009092514
2024-11-11 22:06:04,856 Train Loss: 0.0003546, Val Loss: 0.0004736
2024-11-11 22:06:04,856 Epoch 1840/2000
2024-11-11 22:06:20,951 Current Learning Rate: 0.0009549150
2024-11-11 22:06:20,952 Train Loss: 0.0003297, Val Loss: 0.0004727
2024-11-11 22:06:20,952 Epoch 1841/2000
2024-11-11 22:06:38,754 Current Learning Rate: 0.0010015767
2024-11-11 22:06:38,755 Train Loss: 0.0003348, Val Loss: 0.0004729
2024-11-11 22:06:38,755 Epoch 1842/2000
2024-11-11 22:06:54,006 Current Learning Rate: 0.0010492249
2024-11-11 22:06:54,006 Train Loss: 0.0003834, Val Loss: 0.0004737
2024-11-11 22:06:54,006 Epoch 1843/2000
2024-11-11 22:07:10,579 Current Learning Rate: 0.0010978480
2024-11-11 22:07:10,580 Train Loss: 0.0004457, Val Loss: 0.0004777
2024-11-11 22:07:10,580 Epoch 1844/2000
2024-11-11 22:07:27,270 Current Learning Rate: 0.0011474338
2024-11-11 22:07:27,271 Train Loss: 0.0004340, Val Loss: 0.0004838
2024-11-11 22:07:27,271 Epoch 1845/2000
2024-11-11 22:07:43,160 Current Learning Rate: 0.0011979702
2024-11-11 22:07:43,164 Train Loss: 0.0003392, Val Loss: 0.0004817
2024-11-11 22:07:43,165 Epoch 1846/2000
2024-11-11 22:08:00,247 Current Learning Rate: 0.0012494447
2024-11-11 22:08:00,248 Train Loss: 0.0003844, Val Loss: 0.0004774
2024-11-11 22:08:00,248 Epoch 1847/2000
2024-11-11 22:08:15,871 Current Learning Rate: 0.0013018445
2024-11-11 22:08:15,871 Train Loss: 0.0003830, Val Loss: 0.0004773
2024-11-11 22:08:15,871 Epoch 1848/2000
2024-11-11 22:08:31,385 Current Learning Rate: 0.0013551569
2024-11-11 22:08:31,386 Train Loss: 0.0003410, Val Loss: 0.0004737
2024-11-11 22:08:31,386 Epoch 1849/2000
2024-11-11 22:08:47,358 Current Learning Rate: 0.0014093685
2024-11-11 22:08:47,359 Train Loss: 0.0003742, Val Loss: 0.0004718
2024-11-11 22:08:47,359 Epoch 1850/2000
2024-11-11 22:09:02,864 Current Learning Rate: 0.0014644661
2024-11-11 22:09:02,865 Train Loss: 0.0003540, Val Loss: 0.0004720
2024-11-11 22:09:02,865 Epoch 1851/2000
2024-11-11 22:09:18,445 Current Learning Rate: 0.0015204360
2024-11-11 22:09:18,446 Train Loss: 0.0003742, Val Loss: 0.0004732
2024-11-11 22:09:18,446 Epoch 1852/2000
2024-11-11 22:09:33,902 Current Learning Rate: 0.0015772645
2024-11-11 22:09:33,903 Train Loss: 0.0003416, Val Loss: 0.0004722
2024-11-11 22:09:33,903 Epoch 1853/2000
2024-11-11 22:09:50,443 Current Learning Rate: 0.0016349374
2024-11-11 22:09:50,444 Train Loss: 0.0003718, Val Loss: 0.0004713
2024-11-11 22:09:50,444 Epoch 1854/2000
2024-11-11 22:10:05,969 Current Learning Rate: 0.0016934407
2024-11-11 22:10:05,970 Train Loss: 0.0003528, Val Loss: 0.0004708
2024-11-11 22:10:05,970 Epoch 1855/2000
2024-11-11 22:10:21,652 Current Learning Rate: 0.0017527598
2024-11-11 22:10:21,653 Train Loss: 0.0004123, Val Loss: 0.0004723
2024-11-11 22:10:21,653 Epoch 1856/2000
2024-11-11 22:10:38,704 Current Learning Rate: 0.0018128801
2024-11-11 22:10:38,705 Train Loss: 0.0003573, Val Loss: 0.0004761
2024-11-11 22:10:38,705 Epoch 1857/2000
2024-11-11 22:10:54,696 Current Learning Rate: 0.0018737867
2024-11-11 22:10:54,698 Train Loss: 0.0003185, Val Loss: 0.0004699
2024-11-11 22:10:54,699 Epoch 1858/2000
2024-11-11 22:11:11,148 Current Learning Rate: 0.0019354647
2024-11-11 22:11:11,972 Train Loss: 0.0003121, Val Loss: 0.0004688
2024-11-11 22:11:11,973 Epoch 1859/2000
2024-11-11 22:11:27,466 Current Learning Rate: 0.0019978989
2024-11-11 22:11:27,467 Train Loss: 0.0003598, Val Loss: 0.0004757
2024-11-11 22:11:27,468 Epoch 1860/2000
2024-11-11 22:11:43,767 Current Learning Rate: 0.0020610737
2024-11-11 22:11:43,768 Train Loss: 0.0003094, Val Loss: 0.0004693
2024-11-11 22:11:43,768 Epoch 1861/2000
2024-11-11 22:11:59,732 Current Learning Rate: 0.0021249737
2024-11-11 22:11:59,732 Train Loss: 0.0004457, Val Loss: 0.0005119
2024-11-11 22:11:59,732 Epoch 1862/2000
2024-11-11 22:12:15,595 Current Learning Rate: 0.0021895831
2024-11-11 22:12:15,596 Train Loss: 0.0003953, Val Loss: 0.0004799
2024-11-11 22:12:15,596 Epoch 1863/2000
2024-11-11 22:12:32,638 Current Learning Rate: 0.0022548859
2024-11-11 22:12:32,639 Train Loss: 0.0003458, Val Loss: 0.0004753
2024-11-11 22:12:32,639 Epoch 1864/2000
2024-11-11 22:12:48,851 Current Learning Rate: 0.0023208660
2024-11-11 22:12:48,852 Train Loss: 0.0003620, Val Loss: 0.0004777
2024-11-11 22:12:48,852 Epoch 1865/2000
2024-11-11 22:13:04,936 Current Learning Rate: 0.0023875072
2024-11-11 22:13:04,936 Train Loss: 0.0003907, Val Loss: 0.0004786
2024-11-11 22:13:04,937 Epoch 1866/2000
2024-11-11 22:13:22,147 Current Learning Rate: 0.0024547929
2024-11-11 22:13:22,147 Train Loss: 0.0003682, Val Loss: 0.0004740
2024-11-11 22:13:22,148 Epoch 1867/2000
2024-11-11 22:13:37,714 Current Learning Rate: 0.0025227067
2024-11-11 22:13:37,715 Train Loss: 0.0004295, Val Loss: 0.0004764
2024-11-11 22:13:37,715 Epoch 1868/2000
2024-11-11 22:13:54,690 Current Learning Rate: 0.0025912316
2024-11-11 22:13:54,691 Train Loss: 0.0003818, Val Loss: 0.0004751
2024-11-11 22:13:54,691 Epoch 1869/2000
2024-11-11 22:14:11,115 Current Learning Rate: 0.0026603509
2024-11-11 22:14:11,115 Train Loss: 0.0003718, Val Loss: 0.0004788
2024-11-11 22:14:11,115 Epoch 1870/2000
2024-11-11 22:14:27,353 Current Learning Rate: 0.0027300475
2024-11-11 22:14:27,354 Train Loss: 0.0003444, Val Loss: 0.0004801
2024-11-11 22:14:27,354 Epoch 1871/2000
2024-11-11 22:14:43,008 Current Learning Rate: 0.0028003042
2024-11-11 22:14:43,009 Train Loss: 0.0004120, Val Loss: 0.0004983
2024-11-11 22:14:43,009 Epoch 1872/2000
2024-11-11 22:14:59,398 Current Learning Rate: 0.0028711035
2024-11-11 22:14:59,399 Train Loss: 0.0003376, Val Loss: 0.0004895
2024-11-11 22:14:59,399 Epoch 1873/2000
2024-11-11 22:15:15,467 Current Learning Rate: 0.0029424282
2024-11-11 22:15:15,468 Train Loss: 0.0004411, Val Loss: 0.0005521
2024-11-11 22:15:15,468 Epoch 1874/2000
2024-11-11 22:15:31,189 Current Learning Rate: 0.0030142605
2024-11-11 22:15:31,190 Train Loss: 0.0003611, Val Loss: 0.0004960
2024-11-11 22:15:31,190 Epoch 1875/2000
2024-11-11 22:15:47,235 Current Learning Rate: 0.0030865828
2024-11-11 22:15:47,236 Train Loss: 0.0003937, Val Loss: 0.0004952
2024-11-11 22:15:47,236 Epoch 1876/2000
2024-11-11 22:16:03,671 Current Learning Rate: 0.0031593772
2024-11-11 22:16:03,672 Train Loss: 0.0004007, Val Loss: 0.0005017
2024-11-11 22:16:03,672 Epoch 1877/2000
2024-11-11 22:16:19,734 Current Learning Rate: 0.0032326258
2024-11-11 22:16:19,735 Train Loss: 0.0003837, Val Loss: 0.0005004
2024-11-11 22:16:19,736 Epoch 1878/2000
2024-11-11 22:16:35,863 Current Learning Rate: 0.0033063104
2024-11-11 22:16:35,864 Train Loss: 0.0003563, Val Loss: 0.0004938
2024-11-11 22:16:35,864 Epoch 1879/2000
2024-11-11 22:16:51,733 Current Learning Rate: 0.0033804129
2024-11-11 22:16:51,734 Train Loss: 0.0003751, Val Loss: 0.0004927
2024-11-11 22:16:51,734 Epoch 1880/2000
2024-11-11 22:17:07,406 Current Learning Rate: 0.0034549150
2024-11-11 22:17:07,406 Train Loss: 0.0003335, Val Loss: 0.0005016
2024-11-11 22:17:07,406 Epoch 1881/2000
2024-11-11 22:17:23,536 Current Learning Rate: 0.0035297984
2024-11-11 22:17:23,537 Train Loss: 0.0003847, Val Loss: 0.0004856
2024-11-11 22:17:23,538 Epoch 1882/2000
2024-11-11 22:17:40,039 Current Learning Rate: 0.0036050445
2024-11-11 22:17:40,040 Train Loss: 0.0003872, Val Loss: 0.0005008
2024-11-11 22:17:40,040 Epoch 1883/2000
2024-11-11 22:17:56,607 Current Learning Rate: 0.0036806348
2024-11-11 22:17:56,608 Train Loss: 0.0004442, Val Loss: 0.0004979
2024-11-11 22:17:56,608 Epoch 1884/2000
2024-11-11 22:18:12,168 Current Learning Rate: 0.0037565506
2024-11-11 22:18:12,170 Train Loss: 0.0004071, Val Loss: 0.0004930
2024-11-11 22:18:12,170 Epoch 1885/2000
2024-11-11 22:18:28,927 Current Learning Rate: 0.0038327732
2024-11-11 22:18:28,927 Train Loss: 0.0003319, Val Loss: 0.0004986
2024-11-11 22:18:28,928 Epoch 1886/2000
2024-11-11 22:18:44,511 Current Learning Rate: 0.0039092838
2024-11-11 22:18:44,511 Train Loss: 0.0003312, Val Loss: 0.0005034
2024-11-11 22:18:44,511 Epoch 1887/2000
2024-11-11 22:19:00,428 Current Learning Rate: 0.0039860635
2024-11-11 22:19:00,429 Train Loss: 0.0004128, Val Loss: 0.0005134
2024-11-11 22:19:00,429 Epoch 1888/2000
2024-11-11 22:19:16,218 Current Learning Rate: 0.0040630934
2024-11-11 22:19:16,218 Train Loss: 0.0003791, Val Loss: 0.0005249
2024-11-11 22:19:16,219 Epoch 1889/2000
2024-11-11 22:19:32,392 Current Learning Rate: 0.0041403545
2024-11-11 22:19:32,393 Train Loss: 0.0003771, Val Loss: 0.0005103
2024-11-11 22:19:32,393 Epoch 1890/2000
2024-11-11 22:19:48,686 Current Learning Rate: 0.0042178277
2024-11-11 22:19:48,687 Train Loss: 0.0003920, Val Loss: 0.0005140
2024-11-11 22:19:48,687 Epoch 1891/2000
2024-11-11 22:20:05,551 Current Learning Rate: 0.0042954938
2024-11-11 22:20:05,552 Train Loss: 0.0004391, Val Loss: 0.0005231
2024-11-11 22:20:05,552 Epoch 1892/2000
2024-11-11 22:20:21,539 Current Learning Rate: 0.0043733338
2024-11-11 22:20:21,540 Train Loss: 0.0003930, Val Loss: 0.0005018
2024-11-11 22:20:21,540 Epoch 1893/2000
2024-11-11 22:20:38,421 Current Learning Rate: 0.0044513284
2024-11-11 22:20:38,421 Train Loss: 0.0003513, Val Loss: 0.0004932
2024-11-11 22:20:38,421 Epoch 1894/2000
2024-11-11 22:20:55,353 Current Learning Rate: 0.0045294584
2024-11-11 22:20:55,354 Train Loss: 0.0004034, Val Loss: 0.0005197
2024-11-11 22:20:55,355 Epoch 1895/2000
2024-11-11 22:21:11,844 Current Learning Rate: 0.0046077045
2024-11-11 22:21:11,844 Train Loss: 0.0004092, Val Loss: 0.0005259
2024-11-11 22:21:11,845 Epoch 1896/2000
2024-11-11 22:21:28,285 Current Learning Rate: 0.0046860474
2024-11-11 22:21:28,286 Train Loss: 0.0003873, Val Loss: 0.0005355
2024-11-11 22:21:28,305 Epoch 1897/2000
2024-11-11 22:21:44,025 Current Learning Rate: 0.0047644677
2024-11-11 22:21:44,025 Train Loss: 0.0005417, Val Loss: 0.0005593
2024-11-11 22:21:44,026 Epoch 1898/2000
2024-11-11 22:22:00,455 Current Learning Rate: 0.0048429462
2024-11-11 22:22:00,456 Train Loss: 0.0004578, Val Loss: 0.0005557
2024-11-11 22:22:00,456 Epoch 1899/2000
2024-11-11 22:22:16,620 Current Learning Rate: 0.0049214634
2024-11-11 22:22:16,621 Train Loss: 0.0004109, Val Loss: 0.0005379
2024-11-11 22:22:16,621 Epoch 1900/2000
2024-11-11 22:22:32,720 Current Learning Rate: 0.0050000000
2024-11-11 22:22:32,721 Train Loss: 0.0004444, Val Loss: 0.0005574
2024-11-11 22:22:32,721 Epoch 1901/2000
2024-11-11 22:22:48,745 Current Learning Rate: 0.0050785366
2024-11-11 22:22:48,745 Train Loss: 0.0004269, Val Loss: 0.0005297
2024-11-11 22:22:48,747 Epoch 1902/2000
2024-11-11 22:23:05,199 Current Learning Rate: 0.0051570538
2024-11-11 22:23:05,199 Train Loss: 0.0003823, Val Loss: 0.0005118
2024-11-11 22:23:05,199 Epoch 1903/2000
2024-11-11 22:23:21,125 Current Learning Rate: 0.0052355323
2024-11-11 22:23:21,125 Train Loss: 0.0004586, Val Loss: 0.0005273
2024-11-11 22:23:21,125 Epoch 1904/2000
2024-11-11 22:23:38,268 Current Learning Rate: 0.0053139526
2024-11-11 22:23:38,268 Train Loss: 0.0004171, Val Loss: 0.0005362
2024-11-11 22:23:38,269 Epoch 1905/2000
2024-11-11 22:23:55,405 Current Learning Rate: 0.0053922955
2024-11-11 22:23:55,406 Train Loss: 0.0004146, Val Loss: 0.0005299
2024-11-11 22:23:55,406 Epoch 1906/2000
2024-11-11 22:24:11,620 Current Learning Rate: 0.0054705416
2024-11-11 22:24:11,621 Train Loss: 0.0003814, Val Loss: 0.0005246
2024-11-11 22:24:11,621 Epoch 1907/2000
2024-11-11 22:24:28,148 Current Learning Rate: 0.0055486716
2024-11-11 22:24:28,149 Train Loss: 0.0003620, Val Loss: 0.0005353
2024-11-11 22:24:28,149 Epoch 1908/2000
2024-11-11 22:24:44,726 Current Learning Rate: 0.0056266662
2024-11-11 22:24:44,726 Train Loss: 0.0003887, Val Loss: 0.0005592
2024-11-11 22:24:44,727 Epoch 1909/2000
2024-11-11 22:25:00,627 Current Learning Rate: 0.0057045062
2024-11-11 22:25:00,628 Train Loss: 0.0004154, Val Loss: 0.0005705
2024-11-11 22:25:00,628 Epoch 1910/2000
2024-11-11 22:25:16,816 Current Learning Rate: 0.0057821723
2024-11-11 22:25:16,817 Train Loss: 0.0004081, Val Loss: 0.0005572
2024-11-11 22:25:16,817 Epoch 1911/2000
2024-11-11 22:25:32,575 Current Learning Rate: 0.0058596455
2024-11-11 22:25:32,576 Train Loss: 0.0005201, Val Loss: 0.0006403
2024-11-11 22:25:32,577 Epoch 1912/2000
2024-11-11 22:25:48,941 Current Learning Rate: 0.0059369066
2024-11-11 22:25:48,942 Train Loss: 0.0005510, Val Loss: 0.0005839
2024-11-11 22:25:48,942 Epoch 1913/2000
2024-11-11 22:26:04,790 Current Learning Rate: 0.0060139365
2024-11-11 22:26:04,791 Train Loss: 0.0004719, Val Loss: 0.0005980
2024-11-11 22:26:04,791 Epoch 1914/2000
2024-11-11 22:26:21,203 Current Learning Rate: 0.0060907162
2024-11-11 22:26:21,204 Train Loss: 0.0004550, Val Loss: 0.0005598
2024-11-11 22:26:21,204 Epoch 1915/2000
2024-11-11 22:26:36,281 Current Learning Rate: 0.0061672268
2024-11-11 22:26:36,281 Train Loss: 0.0004500, Val Loss: 0.0005986
2024-11-11 22:26:36,282 Epoch 1916/2000
2024-11-11 22:26:51,823 Current Learning Rate: 0.0062434494
2024-11-11 22:26:51,824 Train Loss: 0.0004075, Val Loss: 0.0005628
2024-11-11 22:26:51,824 Epoch 1917/2000
2024-11-11 22:27:07,607 Current Learning Rate: 0.0063193652
2024-11-11 22:27:07,608 Train Loss: 0.0004483, Val Loss: 0.0005511
2024-11-11 22:27:07,608 Epoch 1918/2000
2024-11-11 22:27:24,771 Current Learning Rate: 0.0063949555
2024-11-11 22:27:24,772 Train Loss: 0.0004663, Val Loss: 0.0005478
2024-11-11 22:27:24,772 Epoch 1919/2000
2024-11-11 22:27:39,734 Current Learning Rate: 0.0064702016
2024-11-11 22:27:39,735 Train Loss: 0.0003983, Val Loss: 0.0005294
2024-11-11 22:27:39,735 Epoch 1920/2000
2024-11-11 22:27:55,598 Current Learning Rate: 0.0065450850
2024-11-11 22:27:55,598 Train Loss: 0.0004561, Val Loss: 0.0005404
2024-11-11 22:27:55,599 Epoch 1921/2000
2024-11-11 22:28:11,431 Current Learning Rate: 0.0066195871
2024-11-11 22:28:11,432 Train Loss: 0.0003968, Val Loss: 0.0005200
2024-11-11 22:28:11,432 Epoch 1922/2000
2024-11-11 22:28:27,754 Current Learning Rate: 0.0066936896
2024-11-11 22:28:27,754 Train Loss: 0.0003605, Val Loss: 0.0005241
2024-11-11 22:28:27,755 Epoch 1923/2000
2024-11-11 22:28:44,247 Current Learning Rate: 0.0067673742
2024-11-11 22:28:44,247 Train Loss: 0.0004936, Val Loss: 0.0005612
2024-11-11 22:28:44,248 Epoch 1924/2000
2024-11-11 22:29:00,145 Current Learning Rate: 0.0068406228
2024-11-11 22:29:00,146 Train Loss: 0.0004738, Val Loss: 0.0005509
2024-11-11 22:29:00,146 Epoch 1925/2000
2024-11-11 22:29:15,838 Current Learning Rate: 0.0069134172
2024-11-11 22:29:15,839 Train Loss: 0.0004473, Val Loss: 0.0005633
2024-11-11 22:29:15,839 Epoch 1926/2000
2024-11-11 22:29:31,757 Current Learning Rate: 0.0069857395
2024-11-11 22:29:31,757 Train Loss: 0.0005060, Val Loss: 0.0005654
2024-11-11 22:29:31,757 Epoch 1927/2000
2024-11-11 22:29:47,388 Current Learning Rate: 0.0070575718
2024-11-11 22:29:47,389 Train Loss: 0.0003975, Val Loss: 0.0005398
2024-11-11 22:29:47,389 Epoch 1928/2000
2024-11-11 22:30:03,786 Current Learning Rate: 0.0071288965
2024-11-11 22:30:03,787 Train Loss: 0.0004850, Val Loss: 0.0005724
2024-11-11 22:30:03,787 Epoch 1929/2000
2024-11-11 22:30:19,730 Current Learning Rate: 0.0071996958
2024-11-11 22:30:19,731 Train Loss: 0.0004567, Val Loss: 0.0005773
2024-11-11 22:30:19,731 Epoch 1930/2000
2024-11-11 22:30:35,592 Current Learning Rate: 0.0072699525
2024-11-11 22:30:35,592 Train Loss: 0.0004131, Val Loss: 0.0005631
2024-11-11 22:30:35,592 Epoch 1931/2000
2024-11-11 22:30:51,100 Current Learning Rate: 0.0073396491
2024-11-11 22:30:51,101 Train Loss: 0.0004963, Val Loss: 0.0005700
2024-11-11 22:30:51,101 Epoch 1932/2000
2024-11-11 22:31:06,570 Current Learning Rate: 0.0074087684
2024-11-11 22:31:06,571 Train Loss: 0.0004965, Val Loss: 0.0006470
2024-11-11 22:31:06,571 Epoch 1933/2000
2024-11-11 22:31:22,792 Current Learning Rate: 0.0074772933
2024-11-11 22:31:22,792 Train Loss: 0.0005311, Val Loss: 0.0006102
2024-11-11 22:31:22,793 Epoch 1934/2000
2024-11-11 22:31:38,520 Current Learning Rate: 0.0075452071
2024-11-11 22:31:38,520 Train Loss: 0.0004443, Val Loss: 0.0005798
2024-11-11 22:31:38,521 Epoch 1935/2000
2024-11-11 22:31:53,616 Current Learning Rate: 0.0076124928
2024-11-11 22:31:53,616 Train Loss: 0.0004478, Val Loss: 0.0005784
2024-11-11 22:31:53,617 Epoch 1936/2000
2024-11-11 22:32:09,026 Current Learning Rate: 0.0076791340
2024-11-11 22:32:09,026 Train Loss: 0.0004659, Val Loss: 0.0006153
2024-11-11 22:32:09,027 Epoch 1937/2000
2024-11-11 22:32:24,609 Current Learning Rate: 0.0077451141
2024-11-11 22:32:24,610 Train Loss: 0.0004430, Val Loss: 0.0006007
2024-11-11 22:32:24,610 Epoch 1938/2000
2024-11-11 22:32:40,230 Current Learning Rate: 0.0078104169
2024-11-11 22:32:40,231 Train Loss: 0.0005379, Val Loss: 0.0006273
2024-11-11 22:32:40,232 Epoch 1939/2000
2024-11-11 22:32:55,734 Current Learning Rate: 0.0078750263
2024-11-11 22:32:55,734 Train Loss: 0.0005183, Val Loss: 0.0006318
2024-11-11 22:32:55,734 Epoch 1940/2000
2024-11-11 22:33:11,066 Current Learning Rate: 0.0079389263
2024-11-11 22:33:11,066 Train Loss: 0.0005867, Val Loss: 0.0006767
2024-11-11 22:33:11,066 Epoch 1941/2000
2024-11-11 22:33:26,922 Current Learning Rate: 0.0080021011
2024-11-11 22:33:26,923 Train Loss: 0.0004844, Val Loss: 0.0006373
2024-11-11 22:33:26,924 Epoch 1942/2000
2024-11-11 22:33:42,301 Current Learning Rate: 0.0080645353
2024-11-11 22:33:42,302 Train Loss: 0.0005559, Val Loss: 0.0005938
2024-11-11 22:33:42,302 Epoch 1943/2000
2024-11-11 22:33:57,589 Current Learning Rate: 0.0081262133
2024-11-11 22:33:57,589 Train Loss: 0.0005101, Val Loss: 0.0006782
2024-11-11 22:33:57,590 Epoch 1944/2000
2024-11-11 22:34:13,075 Current Learning Rate: 0.0081871199
2024-11-11 22:34:13,076 Train Loss: 0.0005106, Val Loss: 0.0005803
2024-11-11 22:34:13,076 Epoch 1945/2000
2024-11-11 22:34:29,641 Current Learning Rate: 0.0082472402
2024-11-11 22:34:29,642 Train Loss: 0.0004256, Val Loss: 0.0005619
2024-11-11 22:34:29,642 Epoch 1946/2000
2024-11-11 22:34:46,539 Current Learning Rate: 0.0083065593
2024-11-11 22:34:46,539 Train Loss: 0.0005152, Val Loss: 0.0005588
2024-11-11 22:34:46,539 Epoch 1947/2000
2024-11-11 22:35:03,225 Current Learning Rate: 0.0083650626
2024-11-11 22:35:03,226 Train Loss: 0.0003819, Val Loss: 0.0005585
2024-11-11 22:35:03,227 Epoch 1948/2000
2024-11-11 22:35:19,521 Current Learning Rate: 0.0084227355
2024-11-11 22:35:19,522 Train Loss: 0.0004611, Val Loss: 0.0005604
2024-11-11 22:35:19,522 Epoch 1949/2000
2024-11-11 22:35:35,988 Current Learning Rate: 0.0084795640
2024-11-11 22:35:35,989 Train Loss: 0.0005769, Val Loss: 0.0006435
2024-11-11 22:35:35,989 Epoch 1950/2000
2024-11-11 22:35:51,984 Current Learning Rate: 0.0085355339
2024-11-11 22:35:51,985 Train Loss: 0.0004643, Val Loss: 0.0005785
2024-11-11 22:35:51,985 Epoch 1951/2000
2024-11-11 22:36:07,018 Current Learning Rate: 0.0085906315
2024-11-11 22:36:07,019 Train Loss: 0.0004961, Val Loss: 0.0005766
2024-11-11 22:36:07,019 Epoch 1952/2000
2024-11-11 22:36:23,026 Current Learning Rate: 0.0086448431
2024-11-11 22:36:23,026 Train Loss: 0.0005167, Val Loss: 0.0005771
2024-11-11 22:36:23,026 Epoch 1953/2000
2024-11-11 22:36:38,473 Current Learning Rate: 0.0086981555
2024-11-11 22:36:38,473 Train Loss: 0.0004278, Val Loss: 0.0005576
2024-11-11 22:36:38,474 Epoch 1954/2000
2024-11-11 22:36:54,301 Current Learning Rate: 0.0087505553
2024-11-11 22:36:54,305 Train Loss: 0.0004320, Val Loss: 0.0005682
2024-11-11 22:36:54,306 Epoch 1955/2000
2024-11-11 22:37:10,387 Current Learning Rate: 0.0088020298
2024-11-11 22:37:10,387 Train Loss: 0.0003972, Val Loss: 0.0005400
2024-11-11 22:37:10,387 Epoch 1956/2000
2024-11-11 22:37:26,164 Current Learning Rate: 0.0088525662
2024-11-11 22:37:26,165 Train Loss: 0.0005276, Val Loss: 0.0005955
2024-11-11 22:37:26,165 Epoch 1957/2000
2024-11-11 22:37:42,188 Current Learning Rate: 0.0089021520
2024-11-11 22:37:42,189 Train Loss: 0.0004871, Val Loss: 0.0005588
2024-11-11 22:37:42,189 Epoch 1958/2000
2024-11-11 22:37:57,877 Current Learning Rate: 0.0089507751
2024-11-11 22:37:57,878 Train Loss: 0.0004784, Val Loss: 0.0006114
2024-11-11 22:37:57,878 Epoch 1959/2000
2024-11-11 22:38:14,224 Current Learning Rate: 0.0089984233
2024-11-11 22:38:14,225 Train Loss: 0.0004367, Val Loss: 0.0005510
2024-11-11 22:38:14,225 Epoch 1960/2000
2024-11-11 22:38:30,170 Current Learning Rate: 0.0090450850
2024-11-11 22:38:30,170 Train Loss: 0.0005190, Val Loss: 0.0005609
2024-11-11 22:38:30,171 Epoch 1961/2000
2024-11-11 22:38:46,174 Current Learning Rate: 0.0090907486
2024-11-11 22:38:46,174 Train Loss: 0.0004511, Val Loss: 0.0006288
2024-11-11 22:38:46,175 Epoch 1962/2000
2024-11-11 22:39:02,107 Current Learning Rate: 0.0091354029
2024-11-11 22:39:02,108 Train Loss: 0.0004677, Val Loss: 0.0005959
2024-11-11 22:39:02,109 Epoch 1963/2000
2024-11-11 22:39:18,754 Current Learning Rate: 0.0091790368
2024-11-11 22:39:18,755 Train Loss: 0.0005031, Val Loss: 0.0006509
2024-11-11 22:39:18,755 Epoch 1964/2000
2024-11-11 22:39:35,031 Current Learning Rate: 0.0092216396
2024-11-11 22:39:35,031 Train Loss: 0.0005291, Val Loss: 0.0006005
2024-11-11 22:39:35,031 Epoch 1965/2000
2024-11-11 22:39:50,174 Current Learning Rate: 0.0092632008
2024-11-11 22:39:50,175 Train Loss: 0.0004980, Val Loss: 0.0006615
2024-11-11 22:39:50,175 Epoch 1966/2000
2024-11-11 22:40:05,947 Current Learning Rate: 0.0093037101
2024-11-11 22:40:05,947 Train Loss: 0.0005937, Val Loss: 0.0006493
2024-11-11 22:40:05,948 Epoch 1967/2000
2024-11-11 22:40:21,471 Current Learning Rate: 0.0093431576
2024-11-11 22:40:21,472 Train Loss: 0.0004971, Val Loss: 0.0005757
2024-11-11 22:40:21,472 Epoch 1968/2000
2024-11-11 22:40:37,415 Current Learning Rate: 0.0093815334
2024-11-11 22:40:37,416 Train Loss: 0.0004579, Val Loss: 0.0005759
2024-11-11 22:40:37,416 Epoch 1969/2000
2024-11-11 22:40:54,488 Current Learning Rate: 0.0094188282
2024-11-11 22:40:54,489 Train Loss: 0.0005135, Val Loss: 0.0005550
2024-11-11 22:40:54,489 Epoch 1970/2000
2024-11-11 22:41:10,743 Current Learning Rate: 0.0094550326
2024-11-11 22:41:10,743 Train Loss: 0.0004032, Val Loss: 0.0005354
2024-11-11 22:41:10,744 Epoch 1971/2000
2024-11-11 22:41:27,207 Current Learning Rate: 0.0094901379
2024-11-11 22:41:27,209 Train Loss: 0.0004060, Val Loss: 0.0005371
2024-11-11 22:41:27,209 Epoch 1972/2000
2024-11-11 22:41:43,846 Current Learning Rate: 0.0095241353
2024-11-11 22:41:43,847 Train Loss: 0.0004552, Val Loss: 0.0005439
2024-11-11 22:41:43,848 Epoch 1973/2000
2024-11-11 22:41:59,302 Current Learning Rate: 0.0095570164
2024-11-11 22:41:59,302 Train Loss: 0.0004786, Val Loss: 0.0006519
2024-11-11 22:41:59,303 Epoch 1974/2000
2024-11-11 22:42:16,111 Current Learning Rate: 0.0095887731
2024-11-11 22:42:16,112 Train Loss: 0.0005079, Val Loss: 0.0005804
2024-11-11 22:42:16,112 Epoch 1975/2000
2024-11-11 22:42:32,181 Current Learning Rate: 0.0096193977
2024-11-11 22:42:32,182 Train Loss: 0.0004874, Val Loss: 0.0006093
2024-11-11 22:42:32,182 Epoch 1976/2000
2024-11-11 22:42:49,302 Current Learning Rate: 0.0096488824
2024-11-11 22:42:49,303 Train Loss: 0.0004719, Val Loss: 0.0005910
2024-11-11 22:42:49,304 Epoch 1977/2000
2024-11-11 22:43:06,203 Current Learning Rate: 0.0096772202
2024-11-11 22:43:06,203 Train Loss: 0.0005030, Val Loss: 0.0006389
2024-11-11 22:43:06,204 Epoch 1978/2000
2024-11-11 22:43:21,586 Current Learning Rate: 0.0097044038
2024-11-11 22:43:21,587 Train Loss: 0.0005297, Val Loss: 0.0006394
2024-11-11 22:43:21,587 Epoch 1979/2000
2024-11-11 22:43:37,738 Current Learning Rate: 0.0097304268
2024-11-11 22:43:37,739 Train Loss: 0.0005388, Val Loss: 0.0006585
2024-11-11 22:43:37,739 Epoch 1980/2000
2024-11-11 22:43:53,676 Current Learning Rate: 0.0097552826
2024-11-11 22:43:53,676 Train Loss: 0.0005008, Val Loss: 0.0006466
2024-11-11 22:43:53,677 Epoch 1981/2000
2024-11-11 22:44:09,510 Current Learning Rate: 0.0097789651
2024-11-11 22:44:09,510 Train Loss: 0.0005345, Val Loss: 0.0006037
2024-11-11 22:44:09,510 Epoch 1982/2000
2024-11-11 22:44:25,307 Current Learning Rate: 0.0098014684
2024-11-11 22:44:25,308 Train Loss: 0.0005008, Val Loss: 0.0006197
2024-11-11 22:44:25,308 Epoch 1983/2000
2024-11-11 22:44:41,161 Current Learning Rate: 0.0098227871
2024-11-11 22:44:41,162 Train Loss: 0.0005541, Val Loss: 0.0005858
2024-11-11 22:44:41,162 Epoch 1984/2000
2024-11-11 22:44:56,974 Current Learning Rate: 0.0098429158
2024-11-11 22:44:56,974 Train Loss: 0.0004540, Val Loss: 0.0005872
2024-11-11 22:44:56,975 Epoch 1985/2000
2024-11-11 22:45:12,528 Current Learning Rate: 0.0098618496
2024-11-11 22:45:12,528 Train Loss: 0.0005192, Val Loss: 0.0006465
2024-11-11 22:45:12,529 Epoch 1986/2000
2024-11-11 22:45:28,548 Current Learning Rate: 0.0098795838
2024-11-11 22:45:28,548 Train Loss: 0.0005380, Val Loss: 0.0006143
2024-11-11 22:45:28,549 Epoch 1987/2000
2024-11-11 22:45:45,004 Current Learning Rate: 0.0098961141
2024-11-11 22:45:45,005 Train Loss: 0.0004710, Val Loss: 0.0005924
2024-11-11 22:45:45,005 Epoch 1988/2000
2024-11-11 22:46:00,953 Current Learning Rate: 0.0099114363
2024-11-11 22:46:00,954 Train Loss: 0.0005370, Val Loss: 0.0006376
2024-11-11 22:46:00,954 Epoch 1989/2000
2024-11-11 22:46:16,911 Current Learning Rate: 0.0099255466
2024-11-11 22:46:16,911 Train Loss: 0.0004813, Val Loss: 0.0006095
2024-11-11 22:46:16,911 Epoch 1990/2000
2024-11-11 22:46:32,954 Current Learning Rate: 0.0099384417
2024-11-11 22:46:32,955 Train Loss: 0.0005072, Val Loss: 0.0006012
2024-11-11 22:46:32,955 Epoch 1991/2000
2024-11-11 22:46:48,743 Current Learning Rate: 0.0099501183
2024-11-11 22:46:48,744 Train Loss: 0.0004263, Val Loss: 0.0006099
2024-11-11 22:46:48,744 Epoch 1992/2000
2024-11-11 22:47:04,812 Current Learning Rate: 0.0099605735
2024-11-11 22:47:04,812 Train Loss: 0.0004198, Val Loss: 0.0006060
2024-11-11 22:47:04,813 Epoch 1993/2000
2024-11-11 22:47:21,632 Current Learning Rate: 0.0099698048
2024-11-11 22:47:21,633 Train Loss: 0.0005051, Val Loss: 0.0005728
2024-11-11 22:47:21,633 Epoch 1994/2000
2024-11-11 22:47:37,650 Current Learning Rate: 0.0099778098
2024-11-11 22:47:37,650 Train Loss: 0.0004463, Val Loss: 0.0005757
2024-11-11 22:47:37,651 Epoch 1995/2000
2024-11-11 22:47:53,167 Current Learning Rate: 0.0099845867
2024-11-11 22:47:53,167 Train Loss: 0.0004818, Val Loss: 0.0005728
2024-11-11 22:47:53,168 Epoch 1996/2000
2024-11-11 22:48:09,423 Current Learning Rate: 0.0099901336
2024-11-11 22:48:09,423 Train Loss: 0.0004305, Val Loss: 0.0005917
2024-11-11 22:48:09,424 Epoch 1997/2000
2024-11-11 22:48:26,012 Current Learning Rate: 0.0099944494
2024-11-11 22:48:26,013 Train Loss: 0.0004970, Val Loss: 0.0005895
2024-11-11 22:48:26,013 Epoch 1998/2000
2024-11-11 22:48:41,468 Current Learning Rate: 0.0099975328
2024-11-11 22:48:41,468 Train Loss: 0.0004721, Val Loss: 0.0005880
2024-11-11 22:48:41,469 Epoch 1999/2000
2024-11-11 22:48:57,740 Current Learning Rate: 0.0099993832
2024-11-11 22:48:57,742 Train Loss: 0.0004842, Val Loss: 0.0006773
2024-11-11 22:48:57,743 Epoch 2000/2000
2024-11-11 22:49:13,391 Current Learning Rate: 0.0100000000
2024-11-11 22:49:13,391 Train Loss: 0.0005090, Val Loss: 0.0006428
2024-11-11 22:49:16,988 Testing completed and best model saved.
2025-02-17 17:19:16,665 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-17 17:19:36,860 Loading best model from checkpoint.
2025-02-17 17:19:54,119 Testing completed and best model saved.
2025-02-17 18:21:25,906 Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-02-17 18:22:38,427 Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-02-17 18:23:09,626 Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-02-17 18:39:34,079 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-17 18:39:48,459 Loading best model from checkpoint.
2025-02-17 18:39:54,512 Error loading model checkpoint during testing: The size of tensor a (40) must match the size of tensor b (10) at non-singleton dimension 1
2025-02-17 18:42:21,244 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-17 18:42:34,481 Loading best model from checkpoint.
2025-02-17 18:43:27,442 Testing completed and best model saved.
2025-02-17 18:47:40,178 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-17 18:47:53,382 Loading best model from checkpoint.
2025-02-17 18:48:46,845 Testing completed and best model saved.
2025-02-17 18:52:52,187 Added key: store_based_barrier_key:1 to store for rank: 0
2025-02-17 18:53:05,667 Loading best model from checkpoint.
2025-02-17 18:53:59,552 Testing completed and best model saved.
2025-02-17 20:30:48,677 Loading best model from checkpoint.
2025-02-17 20:30:49,613 Error loading model checkpoint: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 20:31:20,940 Loading best model from checkpoint.
2025-02-17 20:31:21,192 Error loading model checkpoint: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 20:45:29,895 Loading best model from checkpoint.
2025-02-17 20:45:30,122 Error loading model checkpoint: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 20:48:07,523 Loading best model from checkpoint.
2025-02-17 20:48:07,797 Error loading model checkpoint: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 21:06:07,942 Loading best model from checkpoint.
2025-02-17 21:06:08,422 Error loading model checkpoint: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 21:10:17,504 Loading best model from checkpoint.
2025-02-17 21:10:17,753 Error loading model checkpoint: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 21:11:47,434 Loading best model from checkpoint.
2025-02-17 21:11:47,665 Error loading model checkpoint: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 21:14:02,706 Loading best model from checkpoint.
2025-02-17 21:14:02,939 Error loading model checkpoint directly: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 21:14:02,939 Attempting to fix the state_dict by removing "module." prefix.
2025-02-17 21:14:02,944 Model loaded successfully after fixing state_dict.
2025-02-17 21:17:11,275 Loading best model from checkpoint.
2025-02-17 21:17:11,560 Error loading model checkpoint directly: Error(s) in loading state_dict for Triton:
	Missing key(s) in state_dict: "atmospheric_encoder.enc.0.conv.conv.weight", "atmospheric_encoder.enc.0.conv.conv.bias", "atmospheric_encoder.enc.0.conv.norm.weight", "atmospheric_encoder.enc.0.conv.norm.bias", "atmospheric_encoder.enc.1.conv.conv.weight", "atmospheric_encoder.enc.1.conv.conv.bias", "atmospheric_encoder.enc.1.conv.norm.weight", "atmospheric_encoder.enc.1.conv.norm.bias", "atmospheric_encoder.enc.2.conv.conv.weight", "atmospheric_encoder.enc.2.conv.conv.bias", "atmospheric_encoder.enc.2.conv.norm.weight", "atmospheric_encoder.enc.2.conv.norm.bias", "atmospheric_encoder.enc.3.conv.conv.weight", "atmospheric_encoder.enc.3.conv.conv.bias", "atmospheric_encoder.enc.3.conv.norm.weight", "atmospheric_encoder.enc.3.conv.norm.bias", "temporal_evolution.enc.0.block.pos_embed.weight", "temporal_evolution.enc.0.block.pos_embed.bias", "temporal_evolution.enc.0.block.norm1.weight", "temporal_evolution.enc.0.block.norm1.bias", "temporal_evolution.enc.0.block.norm1.running_mean", "temporal_evolution.enc.0.block.norm1.running_var", "temporal_evolution.enc.0.block.conv1.weight", "temporal_evolution.enc.0.block.conv1.bias", "temporal_evolution.enc.0.block.conv2.weight", "temporal_evolution.enc.0.block.conv2.bias", "temporal_evolution.enc.0.block.attn.weight", "temporal_evolution.enc.0.block.attn.bias", "temporal_evolution.enc.0.block.norm2.weight", "temporal_evolution.enc.0.block.norm2.bias", "temporal_evolution.enc.0.block.norm2.running_mean", "temporal_evolution.enc.0.block.norm2.running_var", "temporal_evolution.enc.0.block.mlp.fc1.weight", "temporal_evolution.enc.0.block.mlp.fc1.bias", "temporal_evolution.enc.0.block.mlp.fc2.weight", "temporal_evolution.enc.0.block.mlp.fc2.bias", "temporal_evolution.enc.0.reduction.weight", "temporal_evolution.enc.0.reduction.bias", "temporal_evolution.enc.1.block.gamma_1", "temporal_evolution.enc.1.block.gamma_2", "temporal_evolution.enc.1.block.pos_embed.weight", "temporal_evolution.enc.1.block.pos_embed.bias", "temporal_evolution.enc.1.block.norm1.weight", "temporal_evolution.enc.1.block.norm1.bias", "temporal_evolution.enc.1.block.attn.qkv.weight", "temporal_evolution.enc.1.block.attn.qkv.bias", "temporal_evolution.enc.1.block.attn.proj.weight", "temporal_evolution.enc.1.block.attn.proj.bias", "temporal_evolution.enc.1.block.norm2.weight", "temporal_evolution.enc.1.block.norm2.bias", "temporal_evolution.enc.1.block.mlp.fc1.weight", "temporal_evolution.enc.1.block.mlp.fc1.bias", "temporal_evolution.enc.1.block.mlp.fc2.weight", "temporal_evolution.enc.1.block.mlp.fc2.bias", "temporal_evolution.enc.2.block.gamma_1", "temporal_evolution.enc.2.block.gamma_2", "temporal_evolution.enc.2.block.pos_embed.weight", "temporal_evolution.enc.2.block.pos_embed.bias", "temporal_evolution.enc.2.block.norm1.weight", "temporal_evolution.enc.2.block.norm1.bias", "temporal_evolution.enc.2.block.attn.qkv.weight", "temporal_evolution.enc.2.block.attn.qkv.bias", "temporal_evolution.enc.2.block.attn.proj.weight", "temporal_evolution.enc.2.block.attn.proj.bias", "temporal_evolution.enc.2.block.norm2.weight", "temporal_evolution.enc.2.block.norm2.bias", "temporal_evolution.enc.2.block.mlp.fc1.weight", "temporal_evolution.enc.2.block.mlp.fc1.bias", "temporal_evolution.enc.2.block.mlp.fc2.weight", "temporal_evolution.enc.2.block.mlp.fc2.bias", "temporal_evolution.enc.3.block.gamma_1", "temporal_evolution.enc.3.block.gamma_2", "temporal_evolution.enc.3.block.pos_embed.weight", "temporal_evolution.enc.3.block.pos_embed.bias", "temporal_evolution.enc.3.block.norm1.weight", "temporal_evolution.enc.3.block.norm1.bias", "temporal_evolution.enc.3.block.attn.qkv.weight", "temporal_evolution.enc.3.block.attn.qkv.bias", "temporal_evolution.enc.3.block.attn.proj.weight", "temporal_evolution.enc.3.block.attn.proj.bias", "temporal_evolution.enc.3.block.norm2.weight", "temporal_evolution.enc.3.block.norm2.bias", "temporal_evolution.enc.3.block.mlp.fc1.weight", "temporal_evolution.enc.3.block.mlp.fc1.bias", "temporal_evolution.enc.3.block.mlp.fc2.weight", "temporal_evolution.enc.3.block.mlp.fc2.bias", "temporal_evolution.enc.4.block.gamma_1", "temporal_evolution.enc.4.block.gamma_2", "temporal_evolution.enc.4.block.pos_embed.weight", "temporal_evolution.enc.4.block.pos_embed.bias", "temporal_evolution.enc.4.block.norm1.weight", "temporal_evolution.enc.4.block.norm1.bias", "temporal_evolution.enc.4.block.attn.qkv.weight", "temporal_evolution.enc.4.block.attn.qkv.bias", "temporal_evolution.enc.4.block.attn.proj.weight", "temporal_evolution.enc.4.block.attn.proj.bias", "temporal_evolution.enc.4.block.norm2.weight", "temporal_evolution.enc.4.block.norm2.bias", "temporal_evolution.enc.4.block.mlp.fc1.weight", "temporal_evolution.enc.4.block.mlp.fc1.bias", "temporal_evolution.enc.4.block.mlp.fc2.weight", "temporal_evolution.enc.4.block.mlp.fc2.bias", "temporal_evolution.enc.5.block.gamma_1", "temporal_evolution.enc.5.block.gamma_2", "temporal_evolution.enc.5.block.pos_embed.weight", "temporal_evolution.enc.5.block.pos_embed.bias", "temporal_evolution.enc.5.block.norm1.weight", "temporal_evolution.enc.5.block.norm1.bias", "temporal_evolution.enc.5.block.attn.qkv.weight", "temporal_evolution.enc.5.block.attn.qkv.bias", "temporal_evolution.enc.5.block.attn.proj.weight", "temporal_evolution.enc.5.block.attn.proj.bias", "temporal_evolution.enc.5.block.norm2.weight", "temporal_evolution.enc.5.block.norm2.bias", "temporal_evolution.enc.5.block.mlp.fc1.weight", "temporal_evolution.enc.5.block.mlp.fc1.bias", "temporal_evolution.enc.5.block.mlp.fc2.weight", "temporal_evolution.enc.5.block.mlp.fc2.bias", "temporal_evolution.enc.6.block.gamma_1", "temporal_evolution.enc.6.block.gamma_2", "temporal_evolution.enc.6.block.pos_embed.weight", "temporal_evolution.enc.6.block.pos_embed.bias", "temporal_evolution.enc.6.block.norm1.weight", "temporal_evolution.enc.6.block.norm1.bias", "temporal_evolution.enc.6.block.attn.qkv.weight", "temporal_evolution.enc.6.block.attn.qkv.bias", "temporal_evolution.enc.6.block.attn.proj.weight", "temporal_evolution.enc.6.block.attn.proj.bias", "temporal_evolution.enc.6.block.norm2.weight", "temporal_evolution.enc.6.block.norm2.bias", "temporal_evolution.enc.6.block.mlp.fc1.weight", "temporal_evolution.enc.6.block.mlp.fc1.bias", "temporal_evolution.enc.6.block.mlp.fc2.weight", "temporal_evolution.enc.6.block.mlp.fc2.bias", "temporal_evolution.enc.7.block.pos_embed.weight", "temporal_evolution.enc.7.block.pos_embed.bias", "temporal_evolution.enc.7.block.norm1.weight", "temporal_evolution.enc.7.block.norm1.bias", "temporal_evolution.enc.7.block.norm1.running_mean", "temporal_evolution.enc.7.block.norm1.running_var", "temporal_evolution.enc.7.block.conv1.weight", "temporal_evolution.enc.7.block.conv1.bias", "temporal_evolution.enc.7.block.conv2.weight", "temporal_evolution.enc.7.block.conv2.bias", "temporal_evolution.enc.7.block.attn.weight", "temporal_evolution.enc.7.block.attn.bias", "temporal_evolution.enc.7.block.norm2.weight", "temporal_evolution.enc.7.block.norm2.bias", "temporal_evolution.enc.7.block.norm2.running_mean", "temporal_evolution.enc.7.block.norm2.running_var", "temporal_evolution.enc.7.block.mlp.fc1.weight", "temporal_evolution.enc.7.block.mlp.fc1.bias", "temporal_evolution.enc.7.block.mlp.fc2.weight", "temporal_evolution.enc.7.block.mlp.fc2.bias", "temporal_evolution.enc.7.reduction.weight", "temporal_evolution.enc.7.reduction.bias", "atmospheric_decoder.dec.0.conv.conv.weight", "atmospheric_decoder.dec.0.conv.conv.bias", "atmospheric_decoder.dec.0.conv.norm.weight", "atmospheric_decoder.dec.0.conv.norm.bias", "atmospheric_decoder.dec.1.conv.conv.weight", "atmospheric_decoder.dec.1.conv.conv.bias", "atmospheric_decoder.dec.1.conv.norm.weight", "atmospheric_decoder.dec.1.conv.norm.bias", "atmospheric_decoder.dec.2.conv.conv.weight", "atmospheric_decoder.dec.2.conv.conv.bias", "atmospheric_decoder.dec.2.conv.norm.weight", "atmospheric_decoder.dec.2.conv.norm.bias", "atmospheric_decoder.dec.3.conv.conv.weight", "atmospheric_decoder.dec.3.conv.conv.bias", "atmospheric_decoder.dec.3.conv.norm.weight", "atmospheric_decoder.dec.3.conv.norm.bias", "atmospheric_decoder.readout.weight", "atmospheric_decoder.readout.bias". 
	Unexpected key(s) in state_dict: "module.atmospheric_encoder.enc.0.conv.conv.weight", "module.atmospheric_encoder.enc.0.conv.conv.bias", "module.atmospheric_encoder.enc.0.conv.norm.weight", "module.atmospheric_encoder.enc.0.conv.norm.bias", "module.atmospheric_encoder.enc.1.conv.conv.weight", "module.atmospheric_encoder.enc.1.conv.conv.bias", "module.atmospheric_encoder.enc.1.conv.norm.weight", "module.atmospheric_encoder.enc.1.conv.norm.bias", "module.atmospheric_encoder.enc.2.conv.conv.weight", "module.atmospheric_encoder.enc.2.conv.conv.bias", "module.atmospheric_encoder.enc.2.conv.norm.weight", "module.atmospheric_encoder.enc.2.conv.norm.bias", "module.atmospheric_encoder.enc.3.conv.conv.weight", "module.atmospheric_encoder.enc.3.conv.conv.bias", "module.atmospheric_encoder.enc.3.conv.norm.weight", "module.atmospheric_encoder.enc.3.conv.norm.bias", "module.temporal_evolution.enc.0.block.pos_embed.weight", "module.temporal_evolution.enc.0.block.pos_embed.bias", "module.temporal_evolution.enc.0.block.norm1.weight", "module.temporal_evolution.enc.0.block.norm1.bias", "module.temporal_evolution.enc.0.block.norm1.running_mean", "module.temporal_evolution.enc.0.block.norm1.running_var", "module.temporal_evolution.enc.0.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.0.block.conv1.weight", "module.temporal_evolution.enc.0.block.conv1.bias", "module.temporal_evolution.enc.0.block.conv2.weight", "module.temporal_evolution.enc.0.block.conv2.bias", "module.temporal_evolution.enc.0.block.attn.weight", "module.temporal_evolution.enc.0.block.attn.bias", "module.temporal_evolution.enc.0.block.norm2.weight", "module.temporal_evolution.enc.0.block.norm2.bias", "module.temporal_evolution.enc.0.block.norm2.running_mean", "module.temporal_evolution.enc.0.block.norm2.running_var", "module.temporal_evolution.enc.0.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.0.block.mlp.fc1.weight", "module.temporal_evolution.enc.0.block.mlp.fc1.bias", "module.temporal_evolution.enc.0.block.mlp.fc2.weight", "module.temporal_evolution.enc.0.block.mlp.fc2.bias", "module.temporal_evolution.enc.0.reduction.weight", "module.temporal_evolution.enc.0.reduction.bias", "module.temporal_evolution.enc.1.block.gamma_1", "module.temporal_evolution.enc.1.block.gamma_2", "module.temporal_evolution.enc.1.block.pos_embed.weight", "module.temporal_evolution.enc.1.block.pos_embed.bias", "module.temporal_evolution.enc.1.block.norm1.weight", "module.temporal_evolution.enc.1.block.norm1.bias", "module.temporal_evolution.enc.1.block.attn.qkv.weight", "module.temporal_evolution.enc.1.block.attn.qkv.bias", "module.temporal_evolution.enc.1.block.attn.proj.weight", "module.temporal_evolution.enc.1.block.attn.proj.bias", "module.temporal_evolution.enc.1.block.norm2.weight", "module.temporal_evolution.enc.1.block.norm2.bias", "module.temporal_evolution.enc.1.block.mlp.fc1.weight", "module.temporal_evolution.enc.1.block.mlp.fc1.bias", "module.temporal_evolution.enc.1.block.mlp.fc2.weight", "module.temporal_evolution.enc.1.block.mlp.fc2.bias", "module.temporal_evolution.enc.2.block.gamma_1", "module.temporal_evolution.enc.2.block.gamma_2", "module.temporal_evolution.enc.2.block.pos_embed.weight", "module.temporal_evolution.enc.2.block.pos_embed.bias", "module.temporal_evolution.enc.2.block.norm1.weight", "module.temporal_evolution.enc.2.block.norm1.bias", "module.temporal_evolution.enc.2.block.attn.qkv.weight", "module.temporal_evolution.enc.2.block.attn.qkv.bias", "module.temporal_evolution.enc.2.block.attn.proj.weight", "module.temporal_evolution.enc.2.block.attn.proj.bias", "module.temporal_evolution.enc.2.block.norm2.weight", "module.temporal_evolution.enc.2.block.norm2.bias", "module.temporal_evolution.enc.2.block.mlp.fc1.weight", "module.temporal_evolution.enc.2.block.mlp.fc1.bias", "module.temporal_evolution.enc.2.block.mlp.fc2.weight", "module.temporal_evolution.enc.2.block.mlp.fc2.bias", "module.temporal_evolution.enc.3.block.gamma_1", "module.temporal_evolution.enc.3.block.gamma_2", "module.temporal_evolution.enc.3.block.pos_embed.weight", "module.temporal_evolution.enc.3.block.pos_embed.bias", "module.temporal_evolution.enc.3.block.norm1.weight", "module.temporal_evolution.enc.3.block.norm1.bias", "module.temporal_evolution.enc.3.block.attn.qkv.weight", "module.temporal_evolution.enc.3.block.attn.qkv.bias", "module.temporal_evolution.enc.3.block.attn.proj.weight", "module.temporal_evolution.enc.3.block.attn.proj.bias", "module.temporal_evolution.enc.3.block.norm2.weight", "module.temporal_evolution.enc.3.block.norm2.bias", "module.temporal_evolution.enc.3.block.mlp.fc1.weight", "module.temporal_evolution.enc.3.block.mlp.fc1.bias", "module.temporal_evolution.enc.3.block.mlp.fc2.weight", "module.temporal_evolution.enc.3.block.mlp.fc2.bias", "module.temporal_evolution.enc.4.block.gamma_1", "module.temporal_evolution.enc.4.block.gamma_2", "module.temporal_evolution.enc.4.block.pos_embed.weight", "module.temporal_evolution.enc.4.block.pos_embed.bias", "module.temporal_evolution.enc.4.block.norm1.weight", "module.temporal_evolution.enc.4.block.norm1.bias", "module.temporal_evolution.enc.4.block.attn.qkv.weight", "module.temporal_evolution.enc.4.block.attn.qkv.bias", "module.temporal_evolution.enc.4.block.attn.proj.weight", "module.temporal_evolution.enc.4.block.attn.proj.bias", "module.temporal_evolution.enc.4.block.norm2.weight", "module.temporal_evolution.enc.4.block.norm2.bias", "module.temporal_evolution.enc.4.block.mlp.fc1.weight", "module.temporal_evolution.enc.4.block.mlp.fc1.bias", "module.temporal_evolution.enc.4.block.mlp.fc2.weight", "module.temporal_evolution.enc.4.block.mlp.fc2.bias", "module.temporal_evolution.enc.5.block.gamma_1", "module.temporal_evolution.enc.5.block.gamma_2", "module.temporal_evolution.enc.5.block.pos_embed.weight", "module.temporal_evolution.enc.5.block.pos_embed.bias", "module.temporal_evolution.enc.5.block.norm1.weight", "module.temporal_evolution.enc.5.block.norm1.bias", "module.temporal_evolution.enc.5.block.attn.qkv.weight", "module.temporal_evolution.enc.5.block.attn.qkv.bias", "module.temporal_evolution.enc.5.block.attn.proj.weight", "module.temporal_evolution.enc.5.block.attn.proj.bias", "module.temporal_evolution.enc.5.block.norm2.weight", "module.temporal_evolution.enc.5.block.norm2.bias", "module.temporal_evolution.enc.5.block.mlp.fc1.weight", "module.temporal_evolution.enc.5.block.mlp.fc1.bias", "module.temporal_evolution.enc.5.block.mlp.fc2.weight", "module.temporal_evolution.enc.5.block.mlp.fc2.bias", "module.temporal_evolution.enc.6.block.gamma_1", "module.temporal_evolution.enc.6.block.gamma_2", "module.temporal_evolution.enc.6.block.pos_embed.weight", "module.temporal_evolution.enc.6.block.pos_embed.bias", "module.temporal_evolution.enc.6.block.norm1.weight", "module.temporal_evolution.enc.6.block.norm1.bias", "module.temporal_evolution.enc.6.block.attn.qkv.weight", "module.temporal_evolution.enc.6.block.attn.qkv.bias", "module.temporal_evolution.enc.6.block.attn.proj.weight", "module.temporal_evolution.enc.6.block.attn.proj.bias", "module.temporal_evolution.enc.6.block.norm2.weight", "module.temporal_evolution.enc.6.block.norm2.bias", "module.temporal_evolution.enc.6.block.mlp.fc1.weight", "module.temporal_evolution.enc.6.block.mlp.fc1.bias", "module.temporal_evolution.enc.6.block.mlp.fc2.weight", "module.temporal_evolution.enc.6.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.block.pos_embed.weight", "module.temporal_evolution.enc.7.block.pos_embed.bias", "module.temporal_evolution.enc.7.block.norm1.weight", "module.temporal_evolution.enc.7.block.norm1.bias", "module.temporal_evolution.enc.7.block.norm1.running_mean", "module.temporal_evolution.enc.7.block.norm1.running_var", "module.temporal_evolution.enc.7.block.norm1.num_batches_tracked", "module.temporal_evolution.enc.7.block.conv1.weight", "module.temporal_evolution.enc.7.block.conv1.bias", "module.temporal_evolution.enc.7.block.conv2.weight", "module.temporal_evolution.enc.7.block.conv2.bias", "module.temporal_evolution.enc.7.block.attn.weight", "module.temporal_evolution.enc.7.block.attn.bias", "module.temporal_evolution.enc.7.block.norm2.weight", "module.temporal_evolution.enc.7.block.norm2.bias", "module.temporal_evolution.enc.7.block.norm2.running_mean", "module.temporal_evolution.enc.7.block.norm2.running_var", "module.temporal_evolution.enc.7.block.norm2.num_batches_tracked", "module.temporal_evolution.enc.7.block.mlp.fc1.weight", "module.temporal_evolution.enc.7.block.mlp.fc1.bias", "module.temporal_evolution.enc.7.block.mlp.fc2.weight", "module.temporal_evolution.enc.7.block.mlp.fc2.bias", "module.temporal_evolution.enc.7.reduction.weight", "module.temporal_evolution.enc.7.reduction.bias", "module.atmospheric_decoder.dec.0.conv.conv.weight", "module.atmospheric_decoder.dec.0.conv.conv.bias", "module.atmospheric_decoder.dec.0.conv.norm.weight", "module.atmospheric_decoder.dec.0.conv.norm.bias", "module.atmospheric_decoder.dec.1.conv.conv.weight", "module.atmospheric_decoder.dec.1.conv.conv.bias", "module.atmospheric_decoder.dec.1.conv.norm.weight", "module.atmospheric_decoder.dec.1.conv.norm.bias", "module.atmospheric_decoder.dec.2.conv.conv.weight", "module.atmospheric_decoder.dec.2.conv.conv.bias", "module.atmospheric_decoder.dec.2.conv.norm.weight", "module.atmospheric_decoder.dec.2.conv.norm.bias", "module.atmospheric_decoder.dec.3.conv.conv.weight", "module.atmospheric_decoder.dec.3.conv.conv.bias", "module.atmospheric_decoder.dec.3.conv.norm.weight", "module.atmospheric_decoder.dec.3.conv.norm.bias", "module.atmospheric_decoder.readout.weight", "module.atmospheric_decoder.readout.bias". 
2025-02-17 21:17:11,561 Attempting to fix the state_dict by removing "module." prefix.
2025-02-17 21:17:11,567 Model loaded successfully after fixing state_dict.
2025-02-17 21:34:14,323 Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-02-17 21:34:27,244 Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-02-17 21:35:40,218 Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-02-17 21:39:32,910 Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-02-17 21:40:20,243 Animation.save using <class 'matplotlib.animation.PillowWriter'>
